{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "masks_path = '/home/cristopher/Desktop/TASOR-S2/dataset_creation/dataset/masks/event/segmentation_masks'\n",
    "\n",
    "events_path = '/home/cristopher/Desktop/TASOR-S2/dataset_creation/dataset/images/event/NIR_SWIR'\n",
    "\n",
    "train_dataset = (os.path.join(os.getcwd(),'model_training','train_dataset'))\n",
    "os.makedirs(train_dataset,exist_ok=True)\n",
    "\n",
    "train_images = (os.path.join(os.getcwd(),'train_dataset','train','images'))\n",
    "train_masks = (os.path.join(os.getcwd(),'train_dataset','train','masks'))\n",
    "\n",
    "val_images = (os.path.join(os.getcwd(),'train_dataset','val','images'))\n",
    "val_masks = (os.path.join(os.getcwd(),'train_dataset','val','masks'))\n",
    "\n",
    "test_images = (os.path.join(os.getcwd(),'train_dataset','test','images'))\n",
    "test_masks = (os.path.join(os.getcwd(),'train_dataset','test','masks'))\n",
    "\n",
    "\n",
    "os.makedirs(train_images,exist_ok=True)\n",
    "os.makedirs(train_masks,exist_ok=True)\n",
    "os.makedirs(val_images,exist_ok=True)\n",
    "os.makedirs(val_masks,exist_ok=True)\n",
    "os.makedirs(test_images,exist_ok=True)\n",
    "os.makedirs(test_masks,exist_ok=True)\n",
    "\n",
    "train_max_idx = np.round(len(os.listdir(masks_path))*0.8).astype(int)\n",
    "\n",
    "val_max_idx = train_max_idx + np.round(len(os.listdir(masks_path))*0.1).astype(int)\n",
    "\n",
    "test_max_idx = len(os.listdir(masks_path))\n",
    "\n",
    "# train_test = []\n",
    "# val_test = []\n",
    "# test_test = []\n",
    "\n",
    "\n",
    "for idx,mask_name in enumerate(os.listdir(masks_path)):\n",
    "    patch_name = re.match(r'(.+)_mask',mask_name).group(1)\n",
    "    patch_name = mask_name.replace('mask_weakly','NIR_SWIR')\n",
    "    \n",
    "    with open(os.path.join(masks_path,mask_name),'rb') as mask_file:\n",
    "        mask = pickle.load(mask_file)\n",
    "\n",
    "    mask[mask == 122] = 0\n",
    "    # mask[mask == 255] = 1\n",
    "\n",
    "    mask_img = Image.fromarray(mask)\n",
    "\n",
    "    \n",
    "    # print(type(mask),mask.dtype,mask.shape)\n",
    "    # break\n",
    "\n",
    "    with open(os.path.join(events_path,patch_name),'rb') as image_file:\n",
    "        image = pickle.load(image_file)\n",
    "        image_uint8 = np.round(np.multiply(image,255)).astype(np.uint8)\n",
    "\n",
    "        image_img = Image.fromarray(image_uint8)\n",
    "\n",
    "    # plt.figure()\n",
    "    # plt.imshow(image)\n",
    "    # plt.figure()\n",
    "    # plt.imshow(image_uint8)\n",
    "    # image_img.show()\n",
    "\n",
    "    # break\n",
    "\n",
    "    if idx<= train_max_idx:\n",
    "        # train_test.append('a')\n",
    "\n",
    "        mask_img.save(os.path.join(train_masks,mask_name.replace('_mask_weakly.pkl','_mask.png')))\n",
    "        image_img.save(os.path.join(train_images,patch_name.replace('.pkl','.png')))\n",
    "        \n",
    "        \n",
    "        # with open(os.path.join(train_masks,mask_name.replace('_mask_weakly','_mask')),'wb') as mask_file:\n",
    "        #     pickle.dump(mask,mask_file)\n",
    "        # with open(os.path.join(train_images,patch_name),'wb') as image_file:\n",
    "        #     pickle.dump(image,image_file)\n",
    "\n",
    "    elif (idx> train_max_idx) and (idx<= val_max_idx) :\n",
    "        # val_test.append('a')\n",
    "\n",
    "        mask_img.save(os.path.join(val_masks,mask_name.replace('_mask_weakly.pkl','_mask.png')))\n",
    "        image_img.save(os.path.join(val_images,patch_name.replace('.pkl','.png')))\n",
    "\n",
    "        # with open(os.path.join(val_masks,mask_name.replace('_mask_weakly','_mask')),'wb') as mask_file:\n",
    "        #     pickle.dump(mask,mask_file)\n",
    "        # with open(os.path.join(val_images,patch_name),'wb') as image_file:\n",
    "        #     pickle.dump(image,image_file)\n",
    "    \n",
    "    elif (idx> val_max_idx) and (idx<= test_max_idx):\n",
    "        # test_test.append('a')\n",
    "        mask_img.save(os.path.join(test_masks,mask_name.replace('_mask_weakly.pkl','_mask.png')))\n",
    "        image_img.save(os.path.join(test_images,patch_name.replace('.pkl','.png')))\n",
    "\n",
    "        # with open(os.path.join(test_masks,mask_name.replace('_mask_weakly','_mask')),'wb') as mask_file:\n",
    "        #     pickle.dump(mask,mask_file)\n",
    "        # with open(os.path.join(test_images,patch_name),'wb') as image_file:\n",
    "        #     pickle.dump(image,image_file)\n",
    "\n",
    "# print(len(train_test),len(val_test),len(test_test))\n",
    "    # print(os.path.isfile(os.path.join(events_path,patch_name)))\n",
    "\n",
    "    # break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "masks_path = '/home/cristopher/Desktop/TASOR-S2/dataset_creation/dataset/masks/event/segmentation_masks'\n",
    "\n",
    "# events_path = '/home/cristopher/Desktop/TASOR-S2/dataset_creation/dataset/images/event/NIR_SWIR'\n",
    "notevents_path = '/home/cristopher/Desktop/TASOR-S2/dataset_creation/dataset/images/notevent/NIR_SWIR'\n",
    "\n",
    "\n",
    "train_images = (os.path.join(os.getcwd(),'train_dataset','train','images'))\n",
    "train_masks = (os.path.join(os.getcwd(),'train_dataset','train','masks'))\n",
    "\n",
    "val_images = (os.path.join(os.getcwd(),'train_dataset','val','images'))\n",
    "val_masks = (os.path.join(os.getcwd(),'train_dataset','val','masks'))\n",
    "\n",
    "test_images = (os.path.join(os.getcwd(),'train_dataset','test','images'))\n",
    "test_masks = (os.path.join(os.getcwd(),'train_dataset','test','masks'))\n",
    "\n",
    "test_max_cont_end_loop = len(os.listdir(masks_path)) # Last index of where the data will be gathered\n",
    "train_max_cont_end_loop = np.round(test_max_cont_end_loop*0.8).astype(int)\n",
    "\n",
    "val_max_cont_end_loop = train_max_cont_end_loop + np.round(test_max_cont_end_loop*0.1).astype(int)\n",
    "\n",
    "\n",
    "mask = Image.fromarray(np.zeros((256,256,3),dtype=np.uint8))\n",
    "\n",
    "\n",
    "with open('/home/cristopher/Desktop/TASOR-S2/dataset_creation/dataset/granules_completed.txt','r') as file:\n",
    "    n_granules = len(file.readlines())\n",
    "\n",
    "n_images_per_granule = np.round(test_max_cont_end_loop/n_granules)\n",
    "\n",
    "\n",
    "\n",
    "train_test = []\n",
    "val_test = []\n",
    "test_test = []\n",
    "\n",
    "cont = 0\n",
    "cont_end_loop = 1\n",
    "scene_names_list = []\n",
    "null_image = []\n",
    "for image_name in os.listdir(notevents_path):\n",
    "    \n",
    "    \n",
    "    scene_name = re.match(r'(.+)_G',image_name).group(1)\n",
    "\n",
    "    # if (scene_name not in scene_names_list):\n",
    "    if scene_names_list.count(scene_name)<6:\n",
    "        \n",
    "            \n",
    "        scene_names_list.append(scene_name)\n",
    "\n",
    "        # print(cont, cont_end_loop,scene_name)\n",
    "        # cont +=1\n",
    "        \n",
    "        with open(os.path.join(notevents_path,image_name),'rb') as image_file:\n",
    "            image = pickle.load(image_file)\n",
    "\n",
    "            image_uint8 = np.round(np.multiply(image,255)).astype(np.uint8)\n",
    "            image_img = Image.fromarray(image_uint8)\n",
    "        # image_img.show()\n",
    "        # break\n",
    "        if cont_end_loop<= train_max_cont_end_loop:\n",
    "            \n",
    "            train_test.append(scene_name)\n",
    "            \n",
    "            mask.save(os.path.join(train_masks,image_name.replace('_NIR_SWIR.pkl','_mask.png')))\n",
    "            image_img.save(os.path.join(train_images,image_name.replace('.pkl','.png')))\n",
    "\n",
    "            cont_end_loop +=1\n",
    "\n",
    "            # with open(os.path.join(train_masks,image_name.replace('_NIR_SWIR','_mask')),'wb') as mask_file:\n",
    "            #     pickle.dump(mask,mask_file)\n",
    "            # with open(os.path.join(train_images,image_name),'wb') as image_file:\n",
    "            #     pickle.dump(image,image_file)\n",
    "\n",
    "        elif (cont_end_loop> train_max_cont_end_loop) and (cont_end_loop<= val_max_cont_end_loop) :\n",
    "\n",
    "            val_test.append(scene_name)\n",
    "            mask.save(os.path.join(val_masks,image_name.replace('_NIR_SWIR.pkl','_mask.png')))\n",
    "            image_img.save(os.path.join(val_images,image_name.replace('.pkl','.png')))\n",
    "            \n",
    "            cont_end_loop +=1\n",
    "\n",
    "            # with open(os.path.join(val_masks,image_name.replace('_NIR_SWIR','_mask')),'wb') as mask_file:\n",
    "            #     pickle.dump(mask,mask_file)\n",
    "            # with open(os.path.join(val_images,image_name),'wb') as image_file:\n",
    "            #     pickle.dump(image,image_file)\n",
    "        \n",
    "        elif (cont_end_loop> val_max_cont_end_loop) and (cont_end_loop<= test_max_cont_end_loop):\n",
    "            \n",
    "            test_test.append(scene_name)\n",
    "\n",
    "            mask.save(os.path.join(test_masks,image_name.replace('_NIR_SWIR.pkl','_mask.png')))\n",
    "            image_img.save(os.path.join(test_images,image_name.replace('.pkl','.png')))\n",
    "\n",
    "            cont_end_loop +=1\n",
    "\n",
    "            # with open(os.path.join(test_masks,image_name.replace('_NIR_SWIR','_mask')),'wb') as mask_file:\n",
    "            #     pickle.dump(mask,mask_file)\n",
    "            # with open(os.path.join(test_images,image_name),'wb') as image_file:\n",
    "            #     pickle.dump(image,image_file)\n",
    "\n",
    "        else:\n",
    "            # print(cont_end_loop)\n",
    "            break\n",
    "        # print(image_name)\n",
    "        # print(os.path.isfile(os.path.join(events_path,image_name)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2112 null patches have been reclassified. \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "masks_path = '/home/cristopher/Desktop/TASOR-S2/dataset_creation/dataset/masks/event/segmentation_masks'\n",
    "\n",
    "events_path = '/home/cristopher/Desktop/TASOR-S2/dataset_creation/dataset/images/event/NIR_SWIR'\n",
    "notevents_main_folder = '/home/cristopher/Desktop/TASOR-S2/dataset_creation/dataset/images/notevent'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def potsprocess_null_patches(patches_main_folder):\n",
    "\n",
    "    null_patches_path = os.path.join(patches_main_folder,'Null patches')\n",
    "    os.makedirs(null_patches_path,exist_ok=True)\n",
    "\n",
    "    folders_list = ['NIR_SWIR','RGB','Vegetation Red','NIR1']\n",
    "\n",
    "    for dir in folders_list:\n",
    "        os.makedirs(os.path.join(null_patches_path,dir),exist_ok=True)\n",
    "\n",
    "    Path(os.path.join(null_patches_path,'null_events_list.txt')).touch(exist_ok=True)\n",
    "\n",
    "    already_classified_condition = False\n",
    "\n",
    "    patches_removed = []\n",
    "\n",
    "    for notevent_folder in folders_list:\n",
    "        notevent_folder_path = os.path.join(patches_main_folder,notevent_folder)\n",
    "        for notevent_name in os.listdir(notevent_folder_path):\n",
    "            notevent_path = os.path.join(notevent_folder_path,notevent_name)\n",
    "            image_name = os.path.basename(notevent_path)\n",
    "\n",
    "            with open(notevent_path,'rb') as image_file:\n",
    "                image = pickle.load(image_file)\n",
    "            \n",
    "            if (image.max(),image.min()) == (0,0):\n",
    "                with open(os.path.join(null_patches_path,'null_events_list.txt'),'r') as f:\n",
    "                    for classified_name in f.readlines():\n",
    "                        if image_name == classified_name.rstrip('\\n'):\n",
    "                            already_classified_condition =True\n",
    "                            break\n",
    "                if not already_classified_condition:\n",
    "\n",
    "                    with open(os.path.join(null_patches_path,'null_events_list.txt'),'a') as txt_file:\n",
    "                        txt_file.write(f'{image_name}\\n')\n",
    "                    \n",
    "                    folder_name = re.findall(r'\\)_([^\\.]+)\\.pkl', image_name)[0]\n",
    "                    if image_name.replace(f'_{folder_name}.pkl','') not in patches_removed:\n",
    "                        for folder in folders_list:\n",
    "                            folder_id = folder\n",
    "                            if folder == 'Vegetation Red':\n",
    "                                folder_id = 'VEG'\n",
    "\n",
    "                            image_folder_name = image_name.replace(folder_name,folder_id)\n",
    "                            shutil.copyfile(os.path.join(os.path.dirname(notevent_folder_path),folder,image_folder_name),os.path.join(null_patches_path,folder,image_folder_name))\n",
    "                            \n",
    "                            os.remove(os.path.join(patches_main_folder,folder,image_folder_name)) # Delete the file from its original location\n",
    "\n",
    "                        patches_removed.append(image_name.replace(f'_{folder_name}.pkl',''))\n",
    "    \n",
    "    print(f'{len(patches_removed)} null patches have been reclassified. ')\n",
    "\n",
    "# potsprocess_null_patches(notevents_main_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('patches_to_revise.txt','w') as f:\n",
    "    for elem in null_notevents:\n",
    "        f.write(f'{elem}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "masks_path = '/home/cristopher/Desktop/TASOR-S2/dataset_creation/dataset/masks/event/segmentation_masks'\n",
    "\n",
    "events_path = '/home/cristopher/Desktop/TASOR-S2/dataset_creation/dataset/images/event/NIR_SWIR'\n",
    "\n",
    "# dataset_path = '/home/cristopher/Desktop/TASOR-S2/dataset_creation/dataset'\n",
    "\n",
    "\n",
    "dataset_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))),'dataset')\n",
    "\n",
    "masks_path_2 = os.path.join(dataset_path,'masks','weakly_segmentation')\n",
    "\n",
    "events_path_2 = os.path.join(dataset_path,'images','event','NIR_SWIR')\n",
    "\n",
    "\n",
    "train_dataset = (os.path.join(os.path.dirname(dataset_path),'model_training','train_dataset'))\n",
    "os.makedirs(train_dataset,exist_ok=True)\n",
    "\n",
    "train_images = (os.path.join(os.path.dirname(dataset_path),'train_dataset','train','images'))\n",
    "train_masks = (os.path.join(os.path.dirname(dataset_path),'train_dataset','train','masks'))\n",
    "\n",
    "val_images = (os.path.join(os.path.dirname(dataset_path),'train_dataset','val','images'))\n",
    "val_masks = (os.path.join(os.path.dirname(dataset_path),'train_dataset','val','masks'))\n",
    "\n",
    "test_images = (os.path.join(os.path.dirname(dataset_path),'train_dataset','test','images'))\n",
    "test_masks = (os.path.join(os.path.dirname(dataset_path),'train_dataset','test','masks'))\n",
    "\n",
    "\n",
    "os.makedirs(train_images,exist_ok=True)\n",
    "os.makedirs(train_masks,exist_ok=True)\n",
    "os.makedirs(val_images,exist_ok=True)\n",
    "os.makedirs(val_masks,exist_ok=True)\n",
    "os.makedirs(test_images,exist_ok=True)\n",
    "os.makedirs(test_masks,exist_ok=True)\n",
    "\n",
    "train_split_ratio : float = 0.8\n",
    "val_split_ratio : float = 0.1\n",
    "test_split_ratio : float = 0.1\n",
    "\n",
    "train_max_idx = np.round(len(os.listdir(masks_path))*0.8).astype(int)\n",
    "\n",
    "val_max_idx = train_max_idx + np.round(len(os.listdir(masks_path))*0.1).astype(int)\n",
    "\n",
    "test_max_idx = len(os.listdir(masks_path))\n",
    "\n",
    "\n",
    "weakly = False\n",
    "\n",
    "for idx,mask_name in enumerate(os.listdir(masks_path)):\n",
    "    patch_name = re.match(r'(.+)_mask',mask_name).group(1)\n",
    "    patch_name = mask_name.replace('mask_weakly','NIR_SWIR')\n",
    "    \n",
    "    with open(os.path.join(masks_path,mask_name),'rb') as mask_file:\n",
    "        mask = pickle.load(mask_file)\n",
    "\n",
    "    if not weakly: #For supervised binary segmentation\n",
    "        mask[mask == -1] = 0\n",
    "\n",
    "\n",
    "    with open(os.path.join(events_path,patch_name),'rb') as image_file:\n",
    "        image = pickle.load(image_file)\n",
    "\n",
    "\n",
    "    if idx<= train_max_idx:\n",
    "\n",
    "        mask_img.save(os.path.join(train_masks,mask_name.replace('_mask_weakly.pkl','_mask.png')))\n",
    "        image_img.save(os.path.join(train_images,patch_name.replace('.pkl','.png')))\n",
    "        \n",
    "\n",
    "    elif (idx> train_max_idx) and (idx<= val_max_idx) :\n",
    "\n",
    "        mpl.image.imsave(os.path.join(val_masks,mask_name.replace('_mask_weakly.pkl','_mask.png')),mask)\n",
    "        mpl.image.imsave(os.path.join(val_images,patch_name.replace('.pkl','.png')),image)\n",
    "\n",
    "    elif (idx> val_max_idx) and (idx<= test_max_idx):\n",
    "\n",
    "        mpl.image.imsave(os.path.join(test_masks,mask_name.replace('_mask_weakly.pkl','_mask.png')),mask)\n",
    "        mpl.image.imsave(os.path.join(test_images,patch_name.replace('.pkl','.png')),image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "masks_path = '/home/cristopher/Desktop/TASOR-S2/dataset_creation/dataset/masks/event/segmentation_masks'\n",
    "\n",
    "# events_path = '/home/cristopher/Desktop/TASOR-S2/dataset_creation/dataset/images/event/NIR_SWIR'\n",
    "notevents_path = '/home/cristopher/Desktop/TASOR-S2/dataset_creation/dataset/images/notevent/NIR_SWIR'\n",
    "\n",
    "\n",
    "test_max_cont_end_loop = len(os.listdir(masks_path)) # Last index of where the data will be gathered\n",
    "train_max_cont_end_loop = np.round(test_max_cont_end_loop*test_split_ratio).astype(int)\n",
    "\n",
    "val_max_cont_end_loop = train_max_cont_end_loop + np.round(test_max_cont_end_loop*val_split_ratio).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "mask = np.zeros((256,256,1),dtype=np.float32)\n",
    "\n",
    "# n_granules_txt = os.path.join(dataset_path,'granules_completed.txt')\n",
    "\n",
    "with open(os.path.join(dataset_path,'granules_completed.txt'),'r') as file:\n",
    "    n_granules = len(file.readlines())\n",
    "\n",
    "n_images_per_granule = np.round(test_max_cont_end_loop/n_granules)\n",
    "\n",
    "cont_end_loop = 1\n",
    "scene_names_list = []\n",
    "for image_name in os.listdir(notevents_path):\n",
    "        \n",
    "    scene_name = re.match(r'(.+)_G',image_name).group(1)\n",
    "\n",
    "    if scene_names_list.count(scene_name)<n_images_per_granule:\n",
    "                    \n",
    "        scene_names_list.append(scene_name)\n",
    "\n",
    "        with open(os.path.join(notevents_path,image_name),'rb') as image_file:\n",
    "            image = pickle.load(image_file)\n",
    "\n",
    "        if cont_end_loop<= train_max_cont_end_loop:\n",
    "\n",
    "            \n",
    "            mpl.image.imsave(os.path.join(train_dataset,'train','masks',mask_name.replace('_mask_weakly.pkl','_mask.png')),mask)\n",
    "            mpl.image.imsave(os.path.join(train_dataset,'train','images',patch_name.replace('.pkl','.png')),image)\n",
    "            \n",
    "\n",
    "            cont_end_loop +=1\n",
    "\n",
    "        elif (cont_end_loop> train_max_cont_end_loop) and (cont_end_loop<= val_max_cont_end_loop) :\n",
    "\n",
    "            mpl.image.imsave(os.path.join(train_dataset,'val','masks',mask_name.replace('_mask_weakly.pkl','_mask.png')),mask)\n",
    "            mpl.image.imsave(os.path.join(train_dataset,'val','images',patch_name.replace('.pkl','.png')),image)\n",
    "            \n",
    "            cont_end_loop +=1\n",
    "\n",
    "        \n",
    "        elif (cont_end_loop> val_max_cont_end_loop) and (cont_end_loop<= test_max_cont_end_loop):\n",
    "\n",
    "            mpl.image.imsave(os.path.join(train_dataset,'test','masks',mask_name.replace('_mask_weakly.pkl','_mask.png')),mask)\n",
    "            mpl.image.imsave(os.path.join(train_dataset,'test','images',patch_name.replace('.pkl','.png')),image)\n",
    "\n",
    "            cont_end_loop +=1\n",
    "\n",
    "        else:\n",
    "            print('Reached maximum number of not events for the dataset')\n",
    "            break\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
