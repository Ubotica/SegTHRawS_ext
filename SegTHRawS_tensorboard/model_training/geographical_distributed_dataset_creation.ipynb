{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import re\n",
    "# import pickle\n",
    "# import numpy as np\n",
    "\n",
    "# from PIL import Image\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# from itertools import combinations\n",
    "# import random\n",
    "# from copy import deepcopy\n",
    "\n",
    "# import argparse\n",
    "\n",
    "# masks_path = '/home/cristopher/Desktop/TASOR-S2/dataset_creation/dataset/masks/event/segmentation_masks'\n",
    "\n",
    "# events_path = '/home/cristopher/Desktop/TASOR-S2/dataset_creation/dataset/images/event/NIR_SWIR'\n",
    "\n",
    "# new_dataset_path = os.path.dirname(os.path.basename(__file__),'train_dataset_test')\n",
    "\n",
    "# dataset_path = os.path.join(os.getcwd(),'dataset') #Provisional path\n",
    "\n",
    "\n",
    "def create_dataset_folders(new_dataset_path):\n",
    "    \n",
    "    for stage in ['train','val','test']:\n",
    "        for files in ['images','masks']:\n",
    "            os.makedirs(os.path.join(new_dataset_path,stage,files),exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# seed = 42\n",
    "# test_split_ratio = 0.1\n",
    "# val_split_ratio = 0.1\n",
    "\n",
    "# random.seed(seed)\n",
    "\n",
    "# masks_paths = os.listdir(masks_path)\n",
    "\n",
    "# random.shuffle(masks_paths)\n",
    "\n",
    "\n",
    "def get_n_events_per_scene_dict(dataset_path: str,\n",
    "                                seed: int = 42):\n",
    "\n",
    "    random.seed(seed)\n",
    "\n",
    "    masks_paths = os.listdir(os.path.join(dataset_path,'masks','event','segmentation_masks'))\n",
    "    \n",
    "    random.shuffle(masks_paths)\n",
    "\n",
    "    scene_list = []\n",
    "\n",
    "    event_scenes_dict = {}\n",
    "\n",
    "    for mask_name in masks_paths:\n",
    "\n",
    "        scene_name = re.match(r'(.+)_[0-9]+_G',mask_name).group(1)\n",
    "\n",
    "        if re.match(r'(.+)_[0-9]+',scene_name): #This ensures tha scenes with multiple numbers such as Raung_1 and Raung are classified as the same scene\n",
    "            scene_name = re.match(r'(.+)_[0-9]+',scene_name).group(1)\n",
    "        \n",
    "        if scene_name not in scene_list:\n",
    "            number_of_images_per_scene = sum(1 for scene in masks_paths if scene.startswith(scene_name))\n",
    "            scene_list.append(scene_name)\n",
    "            event_scenes_dict[scene_name] = number_of_images_per_scene\n",
    "\n",
    "    assert(len(masks_paths)==sum(event_scenes_dict.values())) # Ensure that all the events were correctly categorized\n",
    "    \n",
    "    return event_scenes_dict\n",
    "\n",
    "\n",
    "def find_scenes_combination(n_elems_per_comb: int = 5,\n",
    "                            input_dict: dict = None,\n",
    "                            n_events_max: int = None):\n",
    "\n",
    "    # Number of maximum iterations\n",
    "    # print(int(math.factorial(len(input_dict))/(math.factorial(n_elems_per_comb)*math.factorial(len(input_dict)-n_elems_per_comb))))\n",
    "\n",
    "    for combination in combinations(input_dict.values(), n_elems_per_comb):\n",
    "        \n",
    "        if sum(combination) > n_events_max-3 and sum(combination) < n_events_max+3:\n",
    "            used_values = []\n",
    "            scenes_combination = []\n",
    "            for value in combination:\n",
    "                    for key,val in input_dict.items():\n",
    "                        if val == value and key not in used_values:\n",
    "                            scenes_combination.append(key)\n",
    "                            used_values.append(val)\n",
    "                            break                    \n",
    "            \n",
    "            return scenes_combination\n",
    "\n",
    "\n",
    "# event_scenes_dict = get_n_events_per_scene_dict(masks_paths)\n",
    "\n",
    "\n",
    "# # Maximum number of events in tests\n",
    "# n_events_test_max = int(test_split_ratio * sum(event_scenes_dict.values()))\n",
    "\n",
    "# scenes_test_combination = find_scenes_combination(input_dict=event_scenes_dict,n_events_max=n_events_test_max)\n",
    "\n",
    "# scenes_val_combination = find_scenes_combination(input_dict=events_copy_dict,n_events_max=val_max_idx) # No need to perform the same on the validation\n",
    "\n",
    "def geo_splitted_events_creation(dataset_path: str,\n",
    "                                 new_dataset_path: str,\n",
    "                                 event_scenes_dict: dict,\n",
    "                                 scenes_test_combination: list,\n",
    "                                 val_split_ratio: float = 0.1,\n",
    "                                 seed: int = 42):\n",
    "\n",
    "    random.seed(seed)\n",
    "\n",
    "    masks_path = os.path.join(dataset_path,'masks','event','segmentation_masks')\n",
    "    events_path = os.path.join(dataset_path,'images','event','NIR_SWIR')\n",
    "\n",
    "    masks_paths =os.listdir(masks_path)\n",
    "    \n",
    "    random.shuffle(masks_paths)\n",
    "    \n",
    "    events_copy_dict = deepcopy(event_scenes_dict)\n",
    "    removed_scenes_names = [] # List used for removing the names of the scenes that have been already classified into testing or validation\n",
    "    val_idx = 0\n",
    "    val_max_idx = int(val_split_ratio * sum(event_scenes_dict.values()))\n",
    "\n",
    "    for event_mask_name in masks_paths:\n",
    "\n",
    "        patch_name = re.match(r'(.+)_mask',event_mask_name).group(1)\n",
    "        patch_name = event_mask_name.replace('mask_weakly','NIR_SWIR')\n",
    "        \n",
    "        with open(os.path.join(masks_path,event_mask_name),'rb') as mask_file:\n",
    "            mask = pickle.load(mask_file)\n",
    "\n",
    "        mask[mask == 122] = 0\n",
    "        # mask[mask == 255] = 1\n",
    "\n",
    "        mask_img = Image.fromarray(mask)\n",
    "\n",
    "        with open(os.path.join(events_path,patch_name),'rb') as image_file:\n",
    "            image = pickle.load(image_file)\n",
    "            image_uint8 = np.round(np.multiply(image,255)).astype(np.uint8)\n",
    "\n",
    "            image_img = Image.fromarray(image_uint8)\n",
    "\n",
    "\n",
    "        scene_name = re.match(r'(.+)_[0-9]+_G',event_mask_name).group(1)\n",
    "\n",
    "        if re.match(r'(.+)_[0-9]+',scene_name): #This ensures tha scenes with multiple numbers such as Raung and Raung 1 are classified as the same\n",
    "            scene_name = re.match(r'(.+)_[0-9]+',scene_name).group(1)\n",
    "\n",
    "\n",
    "        if scene_name in scenes_test_combination:\n",
    "            if scene_name not in removed_scenes_names:\n",
    "                del events_copy_dict[scene_name]\n",
    "                removed_scenes_names.append(scene_name)\n",
    "\n",
    "            \n",
    "            mask_img.save(os.path.join(os.path.join(new_dataset_path,'test','masks'),event_mask_name.replace('_mask_weakly.pkl','_mask.png')))\n",
    "            image_img.save(os.path.join(os.path.join(new_dataset_path,'test','images'),patch_name.replace('.pkl','.png')))\n",
    "            \n",
    "        else:\n",
    "            if val_idx <val_max_idx:\n",
    "\n",
    "                val_idx +=1\n",
    "        \n",
    "                mask_img.save(os.path.join(os.path.join(new_dataset_path,'val','masks'),event_mask_name.replace('_mask_weakly.pkl','_mask.png')))\n",
    "                image_img.save(os.path.join(os.path.join(new_dataset_path,'val','images'),patch_name.replace('.pkl','.png')))\n",
    "        \n",
    "            else:\n",
    "                mask_img.save(os.path.join(os.path.join(new_dataset_path,'train','masks'),event_mask_name.replace('_mask_weakly.pkl','_mask.png')))\n",
    "                image_img.save(os.path.join(os.path.join(new_dataset_path,'train','images'),patch_name.replace('.pkl','.png')))            \n",
    "\n",
    "    print(f'Events generated.')\n",
    "\n",
    "# geo_splitted_events_creation(dataset_path=dataset_path,\n",
    "#                              new_dataset_path = new_dataset_path,\n",
    "#                              event_scenes_dict = event_scenes_dict,\n",
    "#                              scenes_test_combination = scenes_test_combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import re\n",
    "# import pickle\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# from PIL import Image\n",
    "# import random\n",
    "\n",
    "# masks_path = '/home/cristopher/Desktop/TASOR-S2/dataset_creation/dataset/masks/event/segmentation_masks'\n",
    "\n",
    "# events_path = '/home/cristopher/Desktop/TASOR-S2/dataset_creation/dataset/images/event/NIR_SWIR'\n",
    "# notevents_path = '/home/cristopher/Desktop/TASOR-S2/dataset_creation/dataset/images/notevent/NIR_SWIR'\n",
    "\n",
    "# masks_path = os.path.join(dataset_path,'masks','event','segmentation_masks')\n",
    "# notevents_path = os.path.join(dataset_path,'images','notevent','NIR_SWIR')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def geo_splitted_notevents_creation(dataset_path: str,\n",
    "                                    new_dataset_path: str,\n",
    "                                    test_scenes: list,\n",
    "                                    train_split_ratio: float = 0.8,\n",
    "                                    val_split_ratio: float = 0.1,\n",
    "                                    test_split_ratio: float = 0.1,\n",
    "                                    seed: int = 42\n",
    "                                    ):\n",
    "    ### Dataset path corresponds to the original dataset of the images\n",
    "    \n",
    "    # Empty mask for the notevent patches\n",
    "    mask = Image.fromarray(np.zeros((256,256,3),dtype=np.uint8))\n",
    "\n",
    "    #Intitialice the seed\n",
    "    random.seed(seed)\n",
    "    \n",
    "    masks_path = os.path.join(dataset_path,'masks','event','segmentation_masks')\n",
    "    notevents_path = os.path.join(dataset_path,'images','notevent','NIR_SWIR')\n",
    "\n",
    "    notevents_paths = os.listdir(notevents_path)\n",
    "    random.shuffle(notevents_paths)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    #Obtain the number of different scenes of the dataset\n",
    "\n",
    "    with open(os.path.join(dataset_path,'granules_completed.txt'),'r') as file:\n",
    "        available_granules = file.readlines()\n",
    "\n",
    "    scene_main_names_list = []\n",
    "    for image_name in available_granules:\n",
    "        \n",
    "        scene_main_name = re.match(r'(.+)_[0-9]+_G',image_name).group(1)\n",
    "        if re.match(r'(.+)_[0-9]+',scene_main_name): #This ensures tha scenes with multiple numbers such as Raung and Raung 1 are classified as the same\n",
    "            scene_main_name = re.match(r'(.+)_[0-9]+',scene_main_name).group(1)\n",
    "        \n",
    "        if scene_main_name not in scene_main_names_list:\n",
    "\n",
    "            scene_main_names_list.append(scene_main_name)\n",
    "            # print(scene_main_name)\n",
    "\n",
    "    n_scenes_train_val = (len(scene_main_names_list) - len(test_scenes)) #Substract the number of scenes selected for testing\n",
    "\n",
    "    # Define maximum indexes and number of images per scene for training, validation and testing\n",
    "\n",
    "    train_max_idx = int(train_split_ratio* len(os.listdir(masks_path)))\n",
    "    val_max_idx = int(val_split_ratio* len(os.listdir(masks_path)))\n",
    "    test_max_idx = int(test_split_ratio* len(os.listdir(masks_path)))\n",
    "\n",
    "    max_n_train_images = int(train_max_idx/n_scenes_train_val)\n",
    "    max_n_val_images = int(val_max_idx/n_scenes_train_val)\n",
    "    max_n_test_images = int(test_max_idx/len(scenes_test_combination))\n",
    "\n",
    "\n",
    "    #Initialice loop variables\n",
    "    val_idx = 0\n",
    "    test_idx = 0\n",
    "    train_idx = 0\n",
    "    train_scene_names_saved = []\n",
    "    val_scene_names_saved = []\n",
    "    test_scene_names_saved = []\n",
    "\n",
    "    for image_name in notevents_paths:\n",
    "        \n",
    "        scene_main_name = re.match(r'(.+)_[0-9]+_G',image_name).group(1)\n",
    "        if re.match(r'(.+)_[0-9]+',scene_main_name): #This ensures tha scenes with multiple numbers such as Raung and Raung 1 are classified as the same\n",
    "            scene_main_name = re.match(r'(.+)_[0-9]+',scene_main_name).group(1)\n",
    "\n",
    "        # # Extract the coordinates of the patch\n",
    "        # numbers = re.findall(r'\\((.*?)\\)', image_name)\n",
    "        # numbers_list = np.array([int(num) for num in numbers[0].split(',')])\n",
    "        \n",
    "        # if not (numbers_list==0).any(): \n",
    "        ###### NOT IMPLEMENTED YET THE CASE WHEN THE IMAGES ARE NOT IN THE BORDERS\n",
    "\n",
    "        if train_idx >= train_max_idx and val_idx >= val_max_idx  and test_idx >= test_max_idx:\n",
    "            break\n",
    "\n",
    "        with open(os.path.join(notevents_path,image_name),'rb') as image_file:\n",
    "            image = pickle.load(image_file)\n",
    "            image_uint8 = np.round(np.multiply(image,255)).astype(np.uint8)\n",
    "            image_img = Image.fromarray(image_uint8)\n",
    "\n",
    "        ##### TESTING SPLIT #####\n",
    "\n",
    "        if scene_main_name in scenes_test_combination:\n",
    "            if test_scene_names_saved.count(scene_main_name) < max_n_test_images:\n",
    "                test_scene_names_saved.append(scene_main_name)\n",
    "\n",
    "                mask.save(os.path.join(os.path.join(new_dataset_path,'test','masks'),image_name.replace('_NIR_SWIR.pkl','_mask.png')))\n",
    "                image_img.save(os.path.join(os.path.join(new_dataset_path,'test','images'),image_name.replace('.pkl','.png')))\n",
    "\n",
    "            if test_idx >= test_max_idx:\n",
    "                continue\n",
    "            else:\n",
    "                test_idx +=1\n",
    "\n",
    "        else:\n",
    "\n",
    "            ##### VALIDATION SPLIT #####\n",
    "            \n",
    "            if val_idx < val_max_idx:\n",
    "                if val_scene_names_saved.count(scene_main_name) <= max_n_val_images:\n",
    "                    val_scene_names_saved.append(scene_main_name)\n",
    "\n",
    "                    mask.save(os.path.join(os.path.join(new_dataset_path,'val','masks'),image_name.replace('_NIR_SWIR.pkl','_mask.png')))\n",
    "                    image_img.save(os.path.join(os.path.join(new_dataset_path,'val','images'),image_name.replace('.pkl','.png')))\n",
    "\n",
    "                    \n",
    "                    val_idx +=1\n",
    "            else:\n",
    "\n",
    "            ##### TRAINING SPLIT #####\n",
    "                \n",
    "                if train_scene_names_saved.count(scene_main_name) <= max_n_train_images:\n",
    "                    train_scene_names_saved.append(scene_main_name)\n",
    "                    \n",
    "                    mask.save(os.path.join(os.path.join(new_dataset_path,'train','masks'),image_name.replace('_NIR_SWIR.pkl','_mask.png')))\n",
    "                    image_img.save(os.path.join(os.path.join(new_dataset_path,'train','images'),image_name.replace('.pkl','.png')))\n",
    "                    \n",
    "                    train_idx +=1\n",
    "    print(train_idx,val_idx,test_idx)\n",
    "    print(train_max_idx,val_max_idx,test_max_idx)\n",
    "\n",
    "\n",
    "# geo_splitted_notevents_creation(dataset_path = dataset_path,test_scenes = scenes_test_combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events generated.\n",
      "572 71 71\n",
      "572 71 71\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from itertools import combinations\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "import argparse\n",
    "\n",
    "# new_dataset_path = os.path.dirname(os.path.basename(__file__),'train_dataset_test')\n",
    "new_dataset_path = os.path.join(os.getcwd(),'train_dataset_test')\n",
    "# os.makedirs(new_dataset_path,exist_ok=True)\n",
    "\n",
    "create_dataset_folders(new_dataset_path)\n",
    "\n",
    "dataset_path = os.path.join(os.path.dirname(os.getcwd()),'dataset_creation','dataset') #Provisional path\n",
    "\n",
    "seed = 42\n",
    "test_split_ratio = 0.1\n",
    "\n",
    "\n",
    "\n",
    "event_scenes_dict = get_n_events_per_scene_dict(dataset_path=dataset_path,seed=seed)\n",
    "\n",
    "\n",
    "# Maximum number of events in tests\n",
    "n_events_test_max = int(test_split_ratio * sum(event_scenes_dict.values()))\n",
    "\n",
    "scenes_test_combination = find_scenes_combination(input_dict=event_scenes_dict,n_events_max=n_events_test_max)\n",
    "\n",
    "geo_splitted_events_creation(dataset_path=dataset_path,\n",
    "                             new_dataset_path = new_dataset_path,\n",
    "                             event_scenes_dict = event_scenes_dict,\n",
    "                             scenes_test_combination = scenes_test_combination,seed=seed)\n",
    "\n",
    "geo_splitted_notevents_creation(dataset_path = dataset_path,new_dataset_path=new_dataset_path,test_scenes = scenes_test_combination)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
