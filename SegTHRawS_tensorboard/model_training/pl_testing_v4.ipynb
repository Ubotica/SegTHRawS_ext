{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 972,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 973,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "import torchvision.transforms as T\n",
    "import albumentations.pytorch as pytorch\n",
    "import albumentations as albu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 974,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchmetrics import Accuracy, JaccardIndex, FBetaScore\n",
    "from typing import Any, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 975,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x72c0415af870>"
      ]
     },
     "execution_count": 976,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from segmentation_models_pytorch.utils import metrics\n",
    "\n",
    "from segmentation_models_pytorch.losses import FocalLoss, DiceLoss, JaccardLoss\n",
    "\n",
    "import re\n",
    "\n",
    "import onnx\n",
    "import onnxruntime\n",
    "\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "# %matplotlib widget\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rc('font',family='Charter')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import shutil\n",
    "\n",
    "\n",
    "from torchmetrics import BinaryConfusionMatrix\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 977,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThermalDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 stage: str,\n",
    "                 images_path: str,\n",
    "                 augmentation: Any,\n",
    "                 preprocessing: Any,\n",
    "                 shuffle: bool = True,\n",
    "                 random_state: int = 42):\n",
    "\n",
    "        self.__attribute_checking(images_path,\n",
    "                                  stage, shuffle, random_state)\n",
    "\n",
    "        self.images_path = images_path\n",
    "\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "\n",
    "        self.stage = stage\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "        self.total_len = None\n",
    "        self._images, self._masks = self.__create_dataset()\n",
    "\n",
    "    @staticmethod\n",
    "    def __type_checking(images_path: str,\n",
    "                        stage: str, shuffle: bool,\n",
    "                        random_state: int) -> None:\n",
    "        \n",
    "        assert isinstance(images_path, str)\n",
    "        assert isinstance(stage, str)\n",
    "        assert isinstance(shuffle, bool)\n",
    "        assert isinstance(random_state, int)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def __path_checking(images_path: str) -> None:\n",
    "        assert os.path.isdir(images_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def __stage_checking(stage: str) -> None:\n",
    "        assert stage in [\"train\", \"test\", \"val\"]\n",
    "\n",
    "    @classmethod\n",
    "    def __attribute_checking(cls, images_path: str,\n",
    "                             stage: str,\n",
    "                             shuffle: bool,\n",
    "                             random_state: int) -> None:\n",
    "\n",
    "        cls.__type_checking(images_path=images_path,\n",
    "                            stage=stage,\n",
    "                            shuffle=shuffle,\n",
    "                            random_state=random_state)\n",
    "\n",
    "        cls.__path_checking(images_path=images_path)\n",
    "\n",
    "        cls.__stage_checking(stage=stage)\n",
    "\n",
    "    def __create_dataset(self) -> dict:\n",
    "        dict_paths = {\n",
    "            \"image\": [],\n",
    "            \"mask\": []\n",
    "        }\n",
    "\n",
    "        images_path = self.__split_data(self.stage)\n",
    "\n",
    "        for image_name in os.listdir(images_path):\n",
    "            dict_paths[\"image\"].append(os.path.join(images_path,image_name))\n",
    "            dict_paths[\"mask\"].append(os.path.join(os.path.dirname(images_path),'masks',image_name.replace('_NIR_SWIR','_mask')))\n",
    "\n",
    "        dataframe = pd.DataFrame(\n",
    "            data=dict_paths,\n",
    "            index=np.arange(0, len(dict_paths[\"image\"]))\n",
    "        )\n",
    "        self.total_len = len(dataframe)\n",
    "        data_dict = {self.stage: (dataframe[\"image\"].values,dataframe[\"mask\"].values)}\n",
    "\n",
    "        return data_dict[self.stage]\n",
    "\n",
    "    def __split_data(self, stage: str) -> str:\n",
    "        return os.path.join(self.images_path,stage,'images')\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.total_len\n",
    "\n",
    "    def __getitem__(self, idx) -> tuple:\n",
    "\n",
    "        image = Image.open(self._images[idx])\n",
    "        mask = Image.open(self._masks[idx])\n",
    "        \n",
    "        image = np.array(image)\n",
    "\n",
    "        ### FOR FOCAL LOSS\n",
    "        mask = mask.convert('L') # This ensures that the label only have 1 band, which is necessary for binary classification\n",
    "        mask = np.array(mask)[:,:,np.newaxis]\n",
    "        \n",
    "        mask = np.divide(mask,255).astype('float32') #Masks need to be 0-1 values\n",
    "        \n",
    "        # # apply augmentation\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 978,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "        albu.VerticalFlip(p=0.5),\n",
    "\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 979,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThermalDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,images_path: str,\n",
    "                 augmentation: Union[T.Compose, A.Compose],\n",
    "                 preprocessing: Any,\n",
    "                 batch_size: int = 5,\n",
    "                 num_workers: int = os.cpu_count(),\n",
    "                 seed: int = 42):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.images_path = images_path\n",
    "        self.data_train = None\n",
    "        self.data_val = None\n",
    "        self.data_test = None\n",
    "        self.data_predict = None\n",
    "        self.seed = seed\n",
    "\n",
    "        self.train_augmentation = augmentation\n",
    "        self.eval_augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "\n",
    "\n",
    "    def setup(self, stage: str = None) -> None:\n",
    "        self.data_train = ThermalDataset(\n",
    "            images_path=self.images_path,\n",
    "            augmentation=self.train_augmentation,\n",
    "            preprocessing=self.preprocessing,\n",
    "            stage=\"train\",\n",
    "            shuffle=True,\n",
    "            random_state=self.seed\n",
    "            )\n",
    "\n",
    "        self.data_val = ThermalDataset(\n",
    "            images_path=self.images_path,\n",
    "            augmentation=self.eval_augmentation,\n",
    "            preprocessing=self.preprocessing,\n",
    "            stage=\"val\",\n",
    "            shuffle=True,\n",
    "            random_state=self.seed\n",
    "            )\n",
    "\n",
    "        self.data_test = ThermalDataset(\n",
    "            images_path=self.images_path,\n",
    "            augmentation=self.eval_augmentation,\n",
    "            preprocessing=self.preprocessing,\n",
    "            stage=\"test\",\n",
    "            shuffle=True,\n",
    "            random_state=self.seed\n",
    "            )\n",
    "\n",
    "        self.data_predict = ThermalDataset(\n",
    "            images_path=self.images_path,\n",
    "            augmentation=self.eval_augmentation,\n",
    "            preprocessing=self.preprocessing,\n",
    "            stage=\"test\",\n",
    "            shuffle=True,\n",
    "            random_state=self.seed\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            dataset=self.data_train,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            dataset=self.data_val,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            dataset=self.data_test,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            dataset=self.data_predict,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 980,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ThermalModel(pl.LightningModule):\n",
    "#     def __init__(self,\n",
    "#                  model: nn.Module,\n",
    "#                  loss_fn: Any,\n",
    "#                  optim_dict: dict = None,\n",
    "#                  lr: float = None,\n",
    "#                  num_classes: int = 1):\n",
    "#         super().__init__()\n",
    "#         self.save_hyperparameters(ignore=['model','loss_fn'])\n",
    "\n",
    "#         self.num_classes = num_classes\n",
    "#         self.model = model\n",
    "#         # self.criterion = nn.CrossEntropyLoss()\n",
    "#         self.criterion = loss_fn\n",
    "#         self.optim_dict = optim_dict\n",
    "#         self._device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "\n",
    "#         self.step_outputs = {\n",
    "#             \"loss\": [],\n",
    "#             \"accuracy\": [],\n",
    "#             \"jaccard_index\": [],\n",
    "#             \"fbeta_score\": [],\n",
    "#             \"IoU\": []\n",
    "#         }\n",
    "\n",
    "#         self.metrics = {\n",
    "#             \"accuracy\": Accuracy(task=\"binary\",\n",
    "#                                  threshold=0.5,\n",
    "#                                  num_classes=num_classes,\n",
    "#                                  validate_args=True,\n",
    "#                                  ignore_index=None,\n",
    "#                                  average=\"micro\").to(self._device),\n",
    "\n",
    "#             \"jaccard_index\": JaccardIndex(task=\"binary\",\n",
    "#                                           threshold=0.5,\n",
    "#                                           num_classes=num_classes,\n",
    "#                                           validate_args=True,\n",
    "#                                           ignore_index=None,\n",
    "#                                           average=\"macro\").to(self._device),\n",
    "\n",
    "#             \"fbeta_score\": FBetaScore(task=\"binary\",\n",
    "#                                       beta=1.0,\n",
    "#                                       threshold=0.5,\n",
    "#                                       num_classes=num_classes,\n",
    "#                                       average=\"micro\",\n",
    "#                                       ignore_index=None,\n",
    "#                                       validate_args=True).to(self._device),\n",
    "\n",
    "#             \"IoU\": metrics.IoU()\n",
    "#         }\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "#     def shared_step(self, batch, stage: str) -> torch.Tensor:\n",
    "#         x, y = batch\n",
    "#         x, y = x.to(self._device),y.to(self._device)\n",
    "\n",
    "#         assert x.ndim == 4\n",
    "#         assert x.max() <= 3 and x.min() >= -3 \n",
    "#         assert y.ndim == 4\n",
    "#         assert y.max() <= 1 and y.min() >= 0\n",
    "\n",
    "#         logits = self.forward(x.to(torch.float32))\n",
    "        \n",
    "\n",
    "#         # activated = F.softmax(input=logits, dim=1)\n",
    "#         # predictions = torch.argmax(activated, dim=1)\n",
    "\n",
    "#         predictions = torch.round(torch.sigmoid(logits))\n",
    "#         # predictions = logits\n",
    "        \n",
    "#         loss = self.criterion(logits, y)\n",
    "        \n",
    "#         accuracy = self.metrics[\"accuracy\"](predictions, y)\n",
    "#         jaccard_index = self.metrics[\"jaccard_index\"](predictions, y)\n",
    "#         fbeta_score = self.metrics[\"fbeta_score\"](predictions, y)\n",
    "#         IoU_score = self.metrics[\"IoU\"](predictions, y)\n",
    "\n",
    "#         # print(f'stage: {stage}')\n",
    "#         # print(f'Jaccard: {jaccard_index.dtype}')\n",
    "#         # print(f'loss: {loss.dtype}')\n",
    "\n",
    "#         self.step_outputs[\"loss\"].append(loss)\n",
    "#         self.step_outputs[\"accuracy\"].append(accuracy)\n",
    "#         self.step_outputs[\"jaccard_index\"].append(jaccard_index)\n",
    "#         self.step_outputs[\"fbeta_score\"].append(fbeta_score)\n",
    "#         self.step_outputs[\"IoU\"].append(IoU_score)\n",
    "\n",
    "\n",
    "#         self.log(f'{stage}_loss'   , loss          , prog_bar=True , on_step=False , on_epoch=True)\n",
    "#         # self.log(f'{stage}_acc'    , accuracy      , prog_bar=True , on_step=False , on_epoch=True)\n",
    "#         # self.log(f'{stage}_jaccard', jaccard_index , prog_bar=True , on_step=False , on_epoch=True)\n",
    "#         self.log(f'{stage}_fbeta'  , fbeta_score   , prog_bar=True , on_step=False , on_epoch=True)\n",
    "#         self.log(f'{stage}_IoU'    , IoU_score     , prog_bar=True , on_step=False , on_epoch=True)\n",
    "        \n",
    "#         return loss\n",
    "\n",
    "#     def shared_epoch_end(self, stage: Any):\n",
    "#         loss = torch.mean(torch.tensor([\n",
    "#             loss for loss in self.step_outputs[\"loss\"]\n",
    "#         ]))\n",
    "\n",
    "#         accuracy = torch.mean(torch.tensor([\n",
    "#             accuracy for accuracy in self.step_outputs[\"accuracy\"]\n",
    "#         ]))\n",
    "\n",
    "#         jaccard_index = torch.mean(torch.tensor([\n",
    "#             jaccard_index for jaccard_index in self.step_outputs[\"jaccard_index\"]\n",
    "#         ]))\n",
    "\n",
    "#         print(f'stage: {stage}')\n",
    "#         print(f'jaccard: {self.step_outputs[\"jaccard_index\"]}')\n",
    "#         print(f'Result: {jaccard_index}')\n",
    "\n",
    "\n",
    "#         fbeta_score = torch.mean(torch.tensor(\n",
    "#             [fbeta_score for fbeta_score in self.step_outputs[\"fbeta_score\"]\n",
    "#              ]))\n",
    "\n",
    "#         IoU_score = torch.mean(torch.tensor(\n",
    "#                 [IoU_score for IoU_score in self.step_outputs[\"IoU\"]\n",
    "#                  ]))\n",
    "\n",
    "#         for key in self.step_outputs.keys():\n",
    "#             self.step_outputs[key].clear()\n",
    "\n",
    "#         metrics = {\n",
    "#             f\"{stage}_loss\": loss,\n",
    "#             f\"{stage}_accuracy\": accuracy,\n",
    "#             f\"{stage}_jaccard_index\": jaccard_index,\n",
    "#             f\"{stage}_fbeta_score\": fbeta_score,\n",
    "#             f\"{stage}_IoU\": IoU_score\n",
    "#         }\n",
    "#         self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "#     def training_step(self, batch: Any, batch_idx: Any):\n",
    "#         return self.shared_step(batch=batch, stage=\"train\")\n",
    "\n",
    "#     def on_train_epoch_end(self) -> None:\n",
    "#         return self.shared_epoch_end(stage=\"train\")\n",
    "\n",
    "#     def validation_step(self, batch: Any, batch_idx: Any):\n",
    "#         return self.shared_step(batch=batch, stage=\"val\")\n",
    "\n",
    "#     def on_validation_epoch_end(self) -> None:\n",
    "#         return self.shared_epoch_end(stage=\"val\")\n",
    "\n",
    "#     def test_step(self, batch: Any, batch_idx: Any):\n",
    "#         return self.shared_step(batch=batch, stage=\"test\")\n",
    "\n",
    "#     def on_test_epoch_end(self) -> None:\n",
    "#         return self.shared_epoch_end(stage=\"test\")\n",
    "\n",
    "#     def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0):\n",
    "#         x, y = batch\n",
    "\n",
    "#         assert x.ndim == 4\n",
    "#         assert x.max() <= 3 and x.min() >= -3\n",
    "#         assert y.ndim == 4\n",
    "#         assert y.max() <= 1 and y.min() >= 0\n",
    "\n",
    "#         logits = self.forward(x.to(torch.float32))\n",
    "#         # predictions = logits\n",
    "#         predictions = torch.round(torch.sigmoid(logits))\n",
    "\n",
    "#         # activated = F.softmax(input=logits, dim=1)\n",
    "#         # predictions = torch.argmax(activated, dim=1)\n",
    "\n",
    "#         return predictions\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         optimizer = torch.optim.Adam(\n",
    "#             params=self.parameters(),\n",
    "#             lr=self.hparams.lr\n",
    "#         )\n",
    "\n",
    "#         scheduler_dict = {\n",
    "#             \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#                 optimizer=optimizer,\n",
    "#                 patience=5\n",
    "#             ),\n",
    "#             # \"scheduler\": pl_bolts.optim.lr_scheduler.LinearWarmupCOsineAnnealingLR(\n",
    "#             #     optimizer=optimizer,\n",
    "#             #     warmup_epochs=10,\n",
    "#             #     max_epochs=30,\n",
    "#             # ),\n",
    "#             \"interval\": \"epoch\",\n",
    "#             \"monitor\": \"val_loss\"\n",
    "#         }\n",
    "        \n",
    "#         optimization_dictionary = {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_dict}\n",
    "#         return self.optim_dict if self.optim_dict else optimization_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 981,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Combined_Focal_Dice_Loss(pl.LightningModule):\n",
    "    '''\n",
    "    Combined weighted loss between Focal Loss and Dice Loss  \n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 focal_loss_weight: float = 0.5,\n",
    "                 dice_weight: float = None,\n",
    "                 log_dice_loss: bool = False):\n",
    "        \n",
    "        super(Combined_Focal_Dice_Loss, self).__init__()\n",
    "        \n",
    "        self.focal_loss_weight = focal_loss_weight\n",
    "        self.dice_weight = (1 - focal_loss_weight) if dice_weight is None else dice_weight\n",
    "\n",
    "        if self.focal_loss_weight + self.dice_weight != 1:\n",
    "            warnings.warn(\"Sum of Focal and Dice loss weights is not 1.0: \"\n",
    "                          f\"{self.focal_loss_weight:.2f} + {self.dice_weight:.2f} = \"\n",
    "                          f\"{self.focal_loss_weight + self.dice_weight:.2f}\")\n",
    "\n",
    "        self.log_dice_loss = log_dice_loss\n",
    "\n",
    "\n",
    "    # def dice_score(y_pred, y_true, eps=1e-15, smooth=1.):\n",
    "    #     intersection = (y_pred * y_true).sum()\n",
    "    #     union = y_pred.sum() + y_true.sum()\n",
    "    #     return (2. * intersection + smooth) / (union + smooth + eps)\n",
    "\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        focal_loss_fn = FocalLoss(mode= 'binary')\n",
    "        focal_loss_fn.__name__ = 'focal_loss'\n",
    "        dice_loss_fn = DiceLoss(mode= 'binary',from_logits=True,log_loss=self.log_dice_loss) #Typically Dice use the masks and not logits, that is why from logits is used because y_pred are the logits\n",
    "        dice_loss_fn.__name__ = 'dice_loss'\n",
    "\n",
    "\n",
    "        focal_loss = focal_loss_fn(y_pred, y_true) \n",
    "        dice_loss = dice_loss_fn(y_pred, y_true) \n",
    "        \n",
    "        # y_pred = torch.sigmoid(y_pred)\n",
    "        # dice_loss = 1- dice_score(y_pred, y_true)\n",
    "        # log_dice_loss = -torch.log(dice_score(y_pred, y_true))\n",
    "        \n",
    "        loss = self.focal_loss_weight * focal_loss + self.dice_weight * dice_loss\n",
    "\n",
    "        return loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 982,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ThermalModel(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 model: nn.Module,\n",
    "                 loss_fn: Any,\n",
    "                 optim_dict: dict = None,\n",
    "                 lr: float = None,\n",
    "                 num_classes: int = 1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['model','loss_fn'])\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.model = model\n",
    "        # self.criterion = nn.CrossEntropyLoss()\n",
    "        self.criterion = loss_fn\n",
    "        self.optim_dict = optim_dict\n",
    "        self._device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "\n",
    "        # self.step_outputs = {\n",
    "        #     \"loss\": [],\n",
    "        #     \"accuracy\": [],\n",
    "        #     \"jaccard_index\": [],\n",
    "        #     \"fbeta_score\": [],\n",
    "        #     \"IoU\": []\n",
    "        # }\n",
    "\n",
    "        self.step_outputs = {\n",
    "            \"tp\": [],\n",
    "            \"tn\": [],\n",
    "            \"fp\": [],\n",
    "            \"fn\": []\n",
    "        }\n",
    "\n",
    "        self.train_tp = []\n",
    "        self.train_tn = []\n",
    "        self.train_fp = []\n",
    "        self.train_fn = []\n",
    "        \n",
    "        self.val_tp = []       \n",
    "        self.val_tn = []\n",
    "        self.val_fp = []\n",
    "        self.val_fn = []\n",
    "        \n",
    "        \n",
    "        self.stage_outputs = {\n",
    "            \"train\": self.step_outputs,\n",
    "            \"val\": self.step_outputs,\n",
    "            \"test\": self.step_outputs\n",
    "        }\n",
    "\n",
    "        self.metrics = {\n",
    "            \"accuracy\": Accuracy(task=\"binary\",\n",
    "                                 threshold=0.5,\n",
    "                                 num_classes=num_classes,\n",
    "                                 validate_args=True,\n",
    "                                 ignore_index=None,\n",
    "                                 average=\"micro\").to(self._device),\n",
    "\n",
    "            \"jaccard_index\": JaccardIndex(task=\"binary\",\n",
    "                                          threshold=0.5,\n",
    "                                          num_classes=num_classes,\n",
    "                                          validate_args=True,\n",
    "                                          ignore_index=None,\n",
    "                                          average=\"macro\").to(self._device),\n",
    "\n",
    "            \"fbeta_score\": FBetaScore(task=\"binary\",\n",
    "                                      beta=1.0,\n",
    "                                      threshold=0.5,\n",
    "                                      num_classes=num_classes,\n",
    "                                      average=\"micro\",\n",
    "                                      ignore_index=None,\n",
    "                                      validate_args=True).to(self._device),\n",
    "\n",
    "            \"IoU\": metrics.IoU()\n",
    "        }\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # output = self.model(x)\n",
    "\n",
    "        # if not self.training:\n",
    "        #     print('Not training')\n",
    "        #     output = torch.sigmoid(output)\n",
    "\n",
    "        # return output\n",
    "        \n",
    "    \n",
    "        return self.model(x)\n",
    "\n",
    "    def shared_step(self, batch, stage: str) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        x, y = x.to(self._device),y.to(self._device)\n",
    "\n",
    "        assert x.ndim == 4\n",
    "        assert x.max() <= 3 and x.min() >= -3 \n",
    "        assert y.ndim == 4\n",
    "        assert y.max() <= 1 and y.min() >= 0\n",
    "\n",
    "        logits = self.forward(x.to(torch.float32))\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        # testing  = (torch.softmax(logits, dim=0))\n",
    "        testing  = (torch.sigmoid(logits))\n",
    "        predictions = (testing > 0.5).float()\n",
    "\n",
    "\n",
    "\n",
    "        tp, fp, fn, tn  = smp.metrics.get_stats(predictions.long(),y.long(),mode = 'binary')\n",
    "\n",
    "        # print(predictions)\n",
    "        # print(tp,fp,fn,tn)\n",
    "\n",
    "        # confmat = BinaryConfusionMatrix()(predictions.detach().cpu(),y.detach().cpu().float())\n",
    "        # print(confmat)\n",
    "\n",
    "        # print(torch.sum(tp).detach().cpu().numpy(),torch.sum(tn).detach().cpu().numpy(),torch.sum(fp).detach().cpu().numpy(),torch.sum(fn).detach().cpu().numpy())\n",
    "        \n",
    "        loss = self.criterion(logits, y)\n",
    "        \n",
    "\n",
    "\n",
    "        # accuracy = self.metrics[\"accuracy\"](predictions, y)\n",
    "        # jaccard_index = self.metrics[\"jaccard_index\"](predictions, y)\n",
    "        fbeta_score = self.metrics[\"fbeta_score\"](predictions, y)\n",
    "        IoU_score = self.metrics[\"IoU\"](predictions, y)\n",
    "\n",
    "        # print(f'stage: {stage}')\n",
    "        # print(f'Jaccard: {jaccard_index.dtype}')\n",
    "        # print(f'loss: {loss.dtype}')\n",
    "        \n",
    "        # if torch.any(y!=0): # This avoids problems with the empty masks, that provides or 1 IoU or nan Jaccard Index \n",
    "\n",
    "        #     # self.stage_outputs[stage][\"loss\"].append(loss)\n",
    "        #     # self.stage_outputs[stage][\"accuracy\"].append(accuracy)\n",
    "        #     # self.stage_outputs[stage][\"jaccard_index\"].append(jaccard_index)\n",
    "        #     # self.stage_outputs[stage][\"fbeta_score\"].append(fbeta_score)\n",
    "        #     # self.stage_outputs[stage][\"IoU\"].append(IoU_score)\n",
    "\n",
    "\n",
    "        #     self.log(f'{stage}_loss'   , loss          , prog_bar=True , on_step=False , on_epoch=True)\n",
    "        #     self.log(f'{stage}_acc'    , accuracy      , prog_bar=True , on_step=False , on_epoch=True)\n",
    "        #     self.log(f'{stage}_jaccard', jaccard_index , prog_bar=True , on_step=False , on_epoch=True)\n",
    "        #     self.log(f'{stage}_fbeta'  , fbeta_score   , prog_bar=True , on_step=False , on_epoch=True)\n",
    "        #     self.log(f'{stage}_IoU'    , IoU_score     , prog_bar=True , on_step=False , on_epoch=True)\n",
    "\n",
    "            # self.log(f'{stage}_tp'     , tp            , prog_bar=True , on_step=False , on_epoch=True)\n",
    "            # self.log(f'{stage}_fp'     , fp            , prog_bar=True , on_step=False , on_epoch=True)\n",
    "            # self.log(f'{stage}_fn'     , fn            , prog_bar=True , on_step=False , on_epoch=True)\n",
    "            # self.log(f'{stage}_tn'     , tn            , prog_bar=True , on_step=False , on_epoch=True)\n",
    "\n",
    "        # else:\n",
    "            # print(stage,loss,jaccard_index,IoU_score,fbeta_score)\n",
    "            # for i in range(5):\n",
    "            #     plt.figure()\n",
    "            #     plt.subplot(1,2,1)\n",
    "            #     plt.imshow(predictions.detach().cpu().numpy().squeeze()[i,:,:])\n",
    "            #     plt.subplot(1,2,2)\n",
    "            #     plt.imshow(y.detach().cpu().numpy().squeeze()[i,:,:])\n",
    "            #     plt.show()\n",
    "\n",
    "\n",
    "        self.log(f'{stage}_loss'   , loss          , prog_bar=True , on_step=False , on_epoch=True)\n",
    "        self.log(f'{stage}_fbeta'  , fbeta_score   , prog_bar=True , on_step=False , on_epoch=True)\n",
    "        self.log(f'{stage}_IoU'    , IoU_score     , prog_bar=True , on_step=False , on_epoch=True)\n",
    "\n",
    "\n",
    "\n",
    "        if stage == 'train':\n",
    "            self.train_tp.append(torch.sum(tp).detach().cpu().numpy())\n",
    "            self.train_tn.append(torch.sum(tn).detach().cpu().numpy())\n",
    "            self.train_fp.append(torch.sum(fp).detach().cpu().numpy())\n",
    "            self.train_fn.append(torch.sum(fn).detach().cpu().numpy())\n",
    "            \n",
    "        if stage == 'val':\n",
    "            self.val_tp.append(torch.sum(tp).detach().cpu().numpy())\n",
    "            self.val_tn.append(torch.sum(tn).detach().cpu().numpy())\n",
    "            self.val_fp.append(torch.sum(fp).detach().cpu().numpy())\n",
    "            self.val_fn.append(torch.sum(fn).detach().cpu().numpy())\n",
    "\n",
    "        # self.stage_outputs[stage]['tp'].append(torch.sum(tp).detach().cpu().numpy())\n",
    "        # self.stage_outputs[stage]['tn'].append(torch.sum(tn).detach().cpu().numpy())\n",
    "        # self.stage_outputs[stage]['fp'].append(torch.sum(fp).detach().cpu().numpy())\n",
    "        # self.stage_outputs[stage]['fn'].append(torch.sum(fn).detach().cpu().numpy())\n",
    "\n",
    "        # self.log(f'{stage}_tp'     , tp            , prog_bar=True , on_step=False , on_epoch=True)\n",
    "        # self.log(f'{stage}_fp'     , fp            , prog_bar=True , on_step=False , on_epoch=True)\n",
    "        # self.log(f'{stage}_fn'     , fn            , prog_bar=True , on_step=False , on_epoch=True)\n",
    "        # self.log(f'{stage}_tn'     , tn            , prog_bar=True , on_step=False , on_epoch=True)\n",
    "\n",
    "        \n",
    "        # # self.optimizers().step()\n",
    "        # self.lr_schedulers().step()\n",
    "\n",
    "        \n",
    "        return loss\n",
    "\n",
    "        # return {\n",
    "        #     \"loss\": loss,\n",
    "        #     \"tp\": tp,\n",
    "        #     \"fp\": fp,\n",
    "        #     \"fn\": fn,\n",
    "        #     \"tn\": tn,\n",
    "        # }\n",
    "\n",
    "    def shared_epoch_end(self, stage: Any):\n",
    "\n",
    "        # print(self.stage_outputs)\n",
    "\n",
    "        if stage == 'train':\n",
    "\n",
    "            tp = np.sum(self.train_tp)\n",
    "            fp = np.sum(self.train_fp)\n",
    "            tn = np.sum(self.train_tn)\n",
    "            fn = np.sum(self.train_fn)\n",
    "            self.log(f'{stage}_tp' , tp , prog_bar=True , on_step=False , on_epoch=True)\n",
    "            self.log(f'{stage}_fp' , fp , prog_bar=True , on_step=False , on_epoch=True)\n",
    "            self.log(f'{stage}_fn' , fn , prog_bar=True , on_step=False , on_epoch=True)\n",
    "            self.log(f'{stage}_tn' , tn , prog_bar=True , on_step=False , on_epoch=True)\n",
    "\n",
    "            self.train_tp = []            \n",
    "            self.train_fp = []\n",
    "            self.train_tn = []\n",
    "            self.train_fn = []\n",
    "\n",
    "        if stage == 'val':\n",
    "\n",
    "            tp = np.sum(self.val_tp)\n",
    "            fp = np.sum(self.val_fp)\n",
    "            tn = np.sum(self.val_tn)\n",
    "            fn = np.sum(self.val_fn)\n",
    "\n",
    "            # print(stage,tp,fp,tn,fn)\n",
    "\n",
    "            self.log(f'{stage}_tp' , tp , prog_bar=True , on_step=False , on_epoch=True)\n",
    "            self.log(f'{stage}_fp' , fp , prog_bar=True , on_step=False , on_epoch=True)\n",
    "            self.log(f'{stage}_fn' , fn , prog_bar=True , on_step=False , on_epoch=True)\n",
    "            self.log(f'{stage}_tn' , tn , prog_bar=True , on_step=False , on_epoch=True)\n",
    "\n",
    "\n",
    "            self.val_tp = []\n",
    "            self.val_fp = []\n",
    "            self.val_tn = []\n",
    "            self.val_fn = []\n",
    "\n",
    "\n",
    "\n",
    "        # tp = np.sum(self.stage_outputs[stage]['tp'])\n",
    "        # fp = np.sum(self.stage_outputs[stage]['fp'])\n",
    "        # tn = np.sum(self.stage_outputs[stage]['tn'])\n",
    "        # fn = np.sum(self.stage_outputs[stage]['fn'])\n",
    "\n",
    "        # tp = torch.cat([x[\"tp\"] for x in self.step_outputs])\n",
    "        # fp = torch.cat([x[\"fp\"] for x in self.step_outputs])\n",
    "        # fn = torch.cat([x[\"fn\"] for x in self.step_outputs])\n",
    "        # tn = torch.cat([x[\"tn\"] for x in self.step_outputs])\n",
    "\n",
    "        # self.log(f'{stage}_tp' , tp , prog_bar=True , on_step=False , on_epoch=True)\n",
    "        # self.log(f'{stage}_fp' , fp , prog_bar=True , on_step=False , on_epoch=True)\n",
    "        # self.log(f'{stage}_fn' , fn , prog_bar=True , on_step=False , on_epoch=True)\n",
    "        # self.log(f'{stage}_tn' , tn , prog_bar=True , on_step=False , on_epoch=True)\n",
    "\n",
    "\n",
    "\n",
    "        # for key in self.stage_outputs[stage].keys():\n",
    "        #     self.stage_outputs[stage][key] = []\n",
    "\n",
    "        # loss = torch.cat([x[\"loss\"] for x in outputs])\n",
    "\n",
    "        # loss = torch.mean(torch.tensor([\n",
    "        #     loss for loss in self.stage_outputs[stage][\"loss\"]\n",
    "        # ]))\n",
    "        \n",
    "        # accuracy = torch.mean(torch.tensor([\n",
    "        #     accuracy for accuracy in self.stage_outputs[stage][\"accuracy\"]\n",
    "        # ]))\n",
    "\n",
    "        # jaccard_index = torch.mean(torch.tensor([\n",
    "        #     jaccard_index for jaccard_index in self.stage_outputs[stage][\"jaccard_index\"]\n",
    "        # ]))\n",
    "\n",
    "        # fbeta_score = torch.mean(torch.tensor(\n",
    "        #     [fbeta_score for fbeta_score in self.stage_outputs[stage][\"fbeta_score\"]\n",
    "        #      ]))\n",
    "\n",
    "        # IoU_score = torch.mean(torch.tensor(\n",
    "        #         [IoU_score for IoU_score in self.stage_outputs[stage][\"IoU\"]\n",
    "        #          ]))\n",
    "        # # print(f'stage: {stage}')\n",
    "        # # print(f'Result: {loss,jaccard_index,fbeta_score,IoU_score}')\n",
    "\n",
    "        # metrics = {\n",
    "        #     f\"{stage}_loss\": loss,\n",
    "        #     f\"{stage}_accuracy\": accuracy,\n",
    "        #     f\"{stage}_jaccard_index\": jaccard_index,\n",
    "        #     f\"{stage}_fbeta_score\": fbeta_score,\n",
    "        #     f\"{stage}_IoU\": IoU_score\n",
    "        # }\n",
    "        # # self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "        # for key in self.stage_outputs[stage].keys():\n",
    "            \n",
    "        #     print(stage,key)\n",
    "            \n",
    "        #     self.stage_outputs[stage][key].clear()\n",
    "\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: Any):\n",
    "        return self.shared_step(batch=batch, stage=\"train\")\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        return self.shared_epoch_end(stage=\"train\")\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: Any):\n",
    "        return self.shared_step(batch=batch, stage=\"val\")\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        return self.shared_epoch_end(stage=\"val\")\n",
    "\n",
    "    def test_step(self, batch: Any, batch_idx: Any):\n",
    "        return self.shared_step(batch=batch, stage=\"test\")\n",
    "\n",
    "    # def on_test_epoch_end(self) -> None:\n",
    "    #     return self.shared_epoch_end(stage=\"test\")\n",
    "\n",
    "    def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0):\n",
    "        x, y = batch\n",
    "\n",
    "        assert x.ndim == 4\n",
    "        assert x.max() <= 3 and x.min() >= -3\n",
    "        assert y.ndim == 4\n",
    "        assert y.max() <= 1 and y.min() >= 0\n",
    "\n",
    "        logits = self.forward(x.to(torch.float32))\n",
    "        # predictions = torch.round(logits)\n",
    "        # predictions = torch.round(torch.sigmoid(logits))\n",
    "        \n",
    "        prob_mask = logits.sigmoid()\n",
    "        predictions = (prob_mask > 0.5).float()\n",
    "        # predictions = (logits > 0.5).float()\n",
    "\n",
    "        # activated = F.softmax(input=logits, dim=1)\n",
    "        # predictions = torch.argmax(activated, dim=1)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            params=self.parameters(),\n",
    "            lr=self.hparams.lr\n",
    "        )\n",
    "\n",
    "        scheduler_dict = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer=optimizer,\n",
    "                patience=5\n",
    "            ),\n",
    "            # 'scheduler': torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            #     optimizer=optimizer,\n",
    "            #     T_max=2,\n",
    "            #     eta_min=0.0009\n",
    "            # ),\n",
    "            # \"scheduler\": pl_bolts.optimizers.lr_scheduler.LinearWarmupCosineAnnealingLR(\n",
    "            #     optimizer=optimizer,\n",
    "            #     warmup_epochs=2,\n",
    "            #     max_epochs=3,\n",
    "            #     eta_min = 0.001\n",
    "            # ),\n",
    "            # \"interval\": \"step\",\n",
    "            \n",
    "            \"interval\": \"epoch\",\n",
    "            \"monitor\": \"val_loss\"\n",
    "        }\n",
    "        \n",
    "        optimization_dictionary = {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_dict}\n",
    "        return self.optim_dict if self.optim_dict else optimization_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 983,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import nn\n",
    "\n",
    "\n",
    "# class Activation(nn.Module):\n",
    "#     def __init__(self, activation, **params):\n",
    "#         super().__init__()\n",
    "\n",
    "#         if activation is None or activation == \"identity\":\n",
    "#             self.activation = nn.Identity(**params)\n",
    "#         elif activation == \"sigmoid\":\n",
    "#             self.activation = nn.Sigmoid()\n",
    "#         elif activation == \"softmax2d\":\n",
    "#             self.activation = nn.Softmax(dim=1, **params)\n",
    "#         elif activation == \"softmax\":\n",
    "#             self.activation = nn.Softmax(**params)\n",
    "#         elif activation == \"logsoftmax\":\n",
    "#             self.activation = nn.LogSoftmax(**params)\n",
    "#         elif activation == \"tanh\":\n",
    "#             self.activation = nn.Tanh()\n",
    "#         elif callable(activation):\n",
    "#             self.activation = activation(**params)\n",
    "#         else:\n",
    "#             raise ValueError(\n",
    "#                 f\"Activation should be callable/sigmoid/softmax/logsoftmax/tanh\"\n",
    "#                 f\"/None; got {activation}\"\n",
    "#             )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.activation(x)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 984,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(callbacks: list,\n",
    "         model: Union[list, tuple],\n",
    "         loss_fn: Any,\n",
    "         augmentation: Any,\n",
    "         preprocessing: Any,\n",
    "         logger: Any,\n",
    "         images_path: str,\n",
    "         optim_dict: dict,\n",
    "         min_epochs: int,\n",
    "         max_epochs: int,\n",
    "         batch_size: int = 16,\n",
    "         precision: str = '16-mixed'\n",
    "         ) -> None:\n",
    "\n",
    "    # Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        fast_dev_run=False,\n",
    "        accelerator=\"auto\",\n",
    "        strategy=\"auto\",\n",
    "        devices=\"auto\",\n",
    "        num_nodes=1,\n",
    "        logger=logger,\n",
    "        callbacks=callbacks,\n",
    "        max_epochs=max_epochs,\n",
    "        min_epochs=min_epochs,\n",
    "        precision=precision # Mixed precision training\n",
    "    )\n",
    "\n",
    "    # Datamodule\n",
    "    datamodule = ThermalDataModule(\n",
    "        images_path=images_path,\n",
    "        augmentation=augmentation,\n",
    "        preprocessing=preprocessing,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=os.cpu_count()\n",
    "    )\n",
    "\n",
    "    # LightningModule\n",
    "    lightning_model = ThermalModel(\n",
    "        model=model,\n",
    "        loss_fn=loss_fn,\n",
    "        optim_dict=optim_dict,\n",
    "        lr=3e-4\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.fit(model=lightning_model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 985,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Constants\n",
    "SEED: int = 42\n",
    "ACTION: str = \"ignore\"\n",
    "DATA_PATH: str = os.path.join(os.getcwd(),'train_dataset')\n",
    "CHECKPOINT: Any = None\n",
    "    \n",
    "# Model Constants\n",
    "CLASSES = 1\n",
    "IN_CHANNELS = 3\n",
    "\n",
    "optim_dict = None\n",
    "\n",
    "# ENCODER = 'se_resnext50_32x4d'\n",
    "ENCODER = 'mobilenet_v2'\n",
    "# ENCODER = 'resnet18'\n",
    "# ENCODER = 'timm-mobilenetv3_large_100'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "    \n",
    "\n",
    "\n",
    "ACTIVATION = None\n",
    "# ACTIVATION = 'sigmoid' # could be None for logits. If used, the sigmoid after the forward function needs to be removed\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "min_epochs = 150\n",
    "max_epochs = 200\n",
    "\n",
    "n_cpu = os.cpu_count()\n",
    "\n",
    "model_name = 'Unet'\n",
    "# model_name = 'DeepLabV3Plus'\n",
    "\n",
    "model_name_path = os.path.join(os.getcwd(),'models',f'{model_name}_{ENCODER}')\n",
    "os.makedirs(model_name_path,exist_ok=True)  \n",
    "run_idx =sum(1 for file in os.listdir(model_name_path) if file.startswith('run'))\n",
    "\n",
    "model_main_path = os.path.join(model_name_path,f'run_{run_idx}')\n",
    "os.makedirs(model_main_path,exist_ok=True)\n",
    "\n",
    "# print(model_main_path)\n",
    "\n",
    "# model_main_path = os.path.join(os.getcwd(),'models',f'{model_name}_{ENCODER}_{run_idx}')\n",
    "metrics_path = os.path.join(model_main_path,'metrics')\n",
    "os.makedirs(metrics_path,exist_ok=True)\n",
    "\n",
    "# model = smp.DeepLabV3Plus(\n",
    "#     encoder_name=ENCODER, \n",
    "#     encoder_weights=ENCODER_WEIGHTS, \n",
    "#     classes=1, \n",
    "#     activation=ACTIVATION,\n",
    "# )\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    in_channels = 3,\n",
    "    classes=CLASSES, \n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "\n",
    "precision = '16-mixed' # 32 # 32 is the original, and 16 is mixed precission\n",
    "# precision = 32\n",
    "\n",
    "loss = FocalLoss(mode= 'binary')\n",
    "loss.__name__ = 'focal_loss'\n",
    "\n",
    "# # loss = DiceLoss(mode= 'binary')\n",
    "# # loss.__name__ = 'dice_loss'\n",
    "\n",
    "# loss = JaccardLoss(mode= 'binary')\n",
    "# loss.__name__ = 'jaccard_loss'\n",
    "\n",
    "# # loss = losses.DiceLoss()\n",
    "# # loss = losses.JaccardLoss()\n",
    "\n",
    "# metrics = [\n",
    "#     metrics.IoU(),\n",
    "# ]\n",
    "\n",
    "# optimizer = torch.optim.Adam([ \n",
    "#     dict(params=model.parameters(), lr=1e-3),\n",
    "# ])\n",
    "\n",
    "augmentation=get_training_augmentation()\n",
    "preprocessing=get_preprocessing(preprocessing_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 986,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "\n",
    "ENCODER = 'mobilenet_v2'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "\n",
    "preprocessing=get_preprocessing(preprocessing_fn)\n",
    "\n",
    "\n",
    "image_path = '/home/cristopher/Documents/SegTHRawS_training/model_training/train_dataset/test/images/Australia_1_G1_(384, 0, 640, 256)_NIR_SWIR.png'\n",
    "\n",
    "mean_mobilenet_v2 =  [0.485, 0.456, 0.406]\n",
    "std_mobilenet_v2 =  [0.229, 0.224, 0.225]\n",
    "\n",
    "# Define the directory containing the PNG images\n",
    "image_directory = os.path.join(os.getcwd(),'inputImages')\n",
    "\n",
    "image = mpimg.imread(image_path)\n",
    "\n",
    "sample = preprocessing(image=image)\n",
    "image_processed = sample['image']\n",
    "\n",
    "# print(np.transpose((image-mean_mobilenet_v2)/std_mobilenet_v2,(2,0,1)))\n",
    "\n",
    "# print(image_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 987,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        dirpath=model_main_path,\n",
    "        filename=f\"{model_name}_{ENCODER}_\"+\"{epoch}\",\n",
    "        save_top_k=10,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\"\n",
    "    ),\n",
    "\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=1e-5,\n",
    "        patience=8,\n",
    "        verbose=False,\n",
    "        mode=\"min\"\n",
    "    ),\n",
    "\n",
    "    LearningRateMonitor(\n",
    "        logging_interval=\"step\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 988,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lightning.pytorch.loggers import TensorBoardLogger\n",
    "# logger = TensorBoardLogger(save_dir=\"./logs\", name=model_name)\n",
    "\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "logger = CSVLogger(f\"{model_main_path}/csv_logs\", name=f\"{model_name}_{ENCODER}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 989,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/cristopher/.local/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /home/cristopher/Documents/SegTHRawS_training/model_training/models/Unet_mobilenet_v2/run_38 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type      | Params\n",
      "----------------------------------------\n",
      "0 | model     | Unet      | 6.6 M \n",
      "1 | criterion | FocalLoss | 0     \n",
      "----------------------------------------\n",
      "6.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.6 M     Total params\n",
      "26.516    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]tensor([[356270, 691856],\n",
      "        [    79,    371]])\n",
      "371 356270 691856 79\n",
      "Sanity Checking DataLoader 0:  50%|     | 1/2 [00:00<00:00,  6.11it/s]tensor([[348730, 699298],\n",
      "        [   129,    419]])\n",
      "419 348730 699298 129\n",
      "Epoch 0:   0%|          | 0/72 [00:00<?, ?it/s]                            tensor([[599837, 448668],\n",
      "        [    28,     43]])\n",
      "43 599837 448668 28\n",
      "Epoch 0:   1%|         | 1/72 [00:01<01:22,  0.86it/s, v_num=0]tensor([[599145, 448548],\n",
      "        [   282,    601]])\n",
      "601 599145 448548 282\n",
      "Epoch 0:   3%|         | 2/72 [00:01<00:48,  1.45it/s, v_num=0]tensor([[625344, 422616],\n",
      "        [   243,    373]])\n",
      "373 625344 422616 243\n",
      "Epoch 0:   4%|         | 3/72 [00:01<00:36,  1.89it/s, v_num=0]tensor([[687321, 360283],\n",
      "        [   416,    556]])\n",
      "556 687321 360283 416\n",
      "Epoch 0:   6%|         | 4/72 [00:01<00:30,  2.26it/s, v_num=0]tensor([[725724, 322146],\n",
      "        [   291,    415]])\n",
      "415 725724 322146 291\n",
      "Epoch 0:   7%|         | 5/72 [00:02<00:27,  2.47it/s, v_num=0]tensor([[845328, 202352],\n",
      "        [   511,    385]])\n",
      "385 845328 202352 511\n",
      "Epoch 0:   8%|         | 6/72 [00:02<00:24,  2.70it/s, v_num=0]tensor([[898962, 148671],\n",
      "        [   427,    516]])\n",
      "516 898962 148671 427\n",
      "Epoch 0:  10%|         | 7/72 [00:02<00:22,  2.89it/s, v_num=0]tensor([[929664, 118415],\n",
      "        [   309,    188]])\n",
      "188 929664 118415 309\n",
      "Epoch 0:  11%|         | 8/72 [00:02<00:20,  3.06it/s, v_num=0]tensor([[960715,  87470],\n",
      "        [   181,    210]])\n",
      "210 960715 87470 181\n",
      "Epoch 0:  12%|        | 9/72 [00:02<00:19,  3.19it/s, v_num=0]tensor([[980440,  67675],\n",
      "        [   268,    193]])\n",
      "193 980440 67675 268\n",
      "Epoch 0:  14%|        | 10/72 [00:03<00:18,  3.30it/s, v_num=0]tensor([[1005433,   41078],\n",
      "        [   1140,     925]])\n",
      "925 1005433 41078 1140\n",
      "Epoch 0:  15%|        | 11/72 [00:03<00:17,  3.40it/s, v_num=0]tensor([[1007884,   40642],\n",
      "        [     23,      27]])\n",
      "27 1007884 40642 23\n",
      "Epoch 0:  17%|        | 12/72 [00:03<00:17,  3.50it/s, v_num=0]tensor([[1015676,   32164],\n",
      "        [    352,     384]])\n",
      "384 1015676 32164 352\n",
      "Epoch 0:  18%|        | 13/72 [00:03<00:16,  3.59it/s, v_num=0]tensor([[1025405,   20271],\n",
      "        [   1823,    1077]])\n",
      "1077 1025405 20271 1823\n",
      "Epoch 0:  19%|        | 14/72 [00:03<00:15,  3.66it/s, v_num=0]tensor([[1028198,   19495],\n",
      "        [    515,     368]])\n",
      "368 1028198 19495 515\n",
      "Epoch 0:  21%|        | 15/72 [00:04<00:15,  3.74it/s, v_num=0]tensor([[1023438,   23992],\n",
      "        [    918,     228]])\n",
      "228 1023438 23992 918\n",
      "Epoch 0:  22%|       | 16/72 [00:04<00:14,  3.81it/s, v_num=0]tensor([[1027676,   20868],\n",
      "        [     29,       3]])\n",
      "3 1027676 20868 29\n",
      "Epoch 0:  24%|       | 17/72 [00:04<00:14,  3.88it/s, v_num=0]tensor([[1034206,   14187],\n",
      "        [    123,      60]])\n",
      "60 1034206 14187 123\n",
      "Epoch 0:  25%|       | 18/72 [00:04<00:13,  3.94it/s, v_num=0]tensor([[1031965,   15797],\n",
      "        [    522,     292]])\n",
      "292 1031965 15797 522\n",
      "Epoch 0:  26%|       | 19/72 [00:04<00:13,  4.00it/s, v_num=0]tensor([[1032409,   13308],\n",
      "        [   2456,     403]])\n",
      "403 1032409 13308 2456\n",
      "Epoch 0:  28%|       | 20/72 [00:04<00:12,  4.04it/s, v_num=0]tensor([[1041434,    7050],\n",
      "        [     88,       4]])\n",
      "4 1041434 7050 88\n",
      "Epoch 0:  29%|       | 21/72 [00:05<00:12,  4.08it/s, v_num=0]tensor([[1038422,    7628],\n",
      "        [   2171,     355]])\n",
      "355 1038422 7628 2171\n",
      "Epoch 0:  31%|       | 22/72 [00:05<00:12,  4.13it/s, v_num=0]tensor([[1041191,    6573],\n",
      "        [    292,     520]])\n",
      "520 1041191 6573 292\n",
      "Epoch 0:  32%|      | 23/72 [00:05<00:11,  4.17it/s, v_num=0]tensor([[1039014,    8165],\n",
      "        [    894,     503]])\n",
      "503 1039014 8165 894\n",
      "Epoch 0:  33%|      | 24/72 [00:05<00:11,  4.21it/s, v_num=0]tensor([[1037462,   11034],\n",
      "        [     39,      41]])\n",
      "41 1037462 11034 39\n",
      "Epoch 0:  35%|      | 25/72 [00:05<00:11,  4.23it/s, v_num=0]tensor([[1042371,    5627],\n",
      "        [    316,     262]])\n",
      "262 1042371 5627 316\n",
      "Epoch 0:  36%|      | 26/72 [00:06<00:10,  4.27it/s, v_num=0]tensor([[1046029,    2440],\n",
      "        [     88,      19]])\n",
      "19 1046029 2440 88\n",
      "Epoch 0:  38%|      | 27/72 [00:06<00:10,  4.31it/s, v_num=0]tensor([[1035712,    8411],\n",
      "        [   3197,    1256]])\n",
      "1256 1035712 8411 3197\n",
      "Epoch 0:  39%|      | 28/72 [00:06<00:10,  4.33it/s, v_num=0]tensor([[1045538,    2988],\n",
      "        [     48,       2]])\n",
      "2 1045538 2988 48\n",
      "Epoch 0:  40%|      | 29/72 [00:06<00:09,  4.36it/s, v_num=0]tensor([[1044029,    3556],\n",
      "        [    702,     289]])\n",
      "289 1044029 3556 702\n",
      "Epoch 0:  42%|     | 30/72 [00:06<00:09,  4.38it/s, v_num=0]tensor([[1044865,    3073],\n",
      "        [    552,      86]])\n",
      "86 1044865 3073 552\n",
      "Epoch 0:  43%|     | 31/72 [00:07<00:09,  4.40it/s, v_num=0]tensor([[1045167,    2897],\n",
      "        [    379,     133]])\n",
      "133 1045167 2897 379\n",
      "Epoch 0:  44%|     | 32/72 [00:07<00:09,  4.41it/s, v_num=0]tensor([[1044635,    2529],\n",
      "        [   1161,     251]])\n",
      "251 1044635 2529 1161\n",
      "Epoch 0:  46%|     | 33/72 [00:07<00:08,  4.44it/s, v_num=0]tensor([[1042889,    3192],\n",
      "        [   1635,     860]])\n",
      "860 1042889 3192 1635\n",
      "Epoch 0:  47%|     | 34/72 [00:07<00:08,  4.46it/s, v_num=0]tensor([[1046064,    1981],\n",
      "        [    199,     332]])\n",
      "332 1046064 1981 199\n",
      "Epoch 0:  49%|     | 35/72 [00:07<00:08,  4.49it/s, v_num=0]tensor([[1045486,    1881],\n",
      "        [   1177,      32]])\n",
      "32 1045486 1881 1177\n",
      "Epoch 0:  50%|     | 36/72 [00:07<00:07,  4.51it/s, v_num=0]tensor([[1046613,    1455],\n",
      "        [    261,     247]])\n",
      "247 1046613 1455 261\n",
      "Epoch 0:  51%|    | 37/72 [00:08<00:07,  4.53it/s, v_num=0]tensor([[1044865,    3098],\n",
      "        [    574,      39]])\n",
      "39 1044865 3098 574\n",
      "Epoch 0:  53%|    | 38/72 [00:08<00:07,  4.56it/s, v_num=0]tensor([[1044757,    1207],\n",
      "        [   2020,     592]])\n",
      "592 1044757 1207 2020\n",
      "Epoch 0:  54%|    | 39/72 [00:08<00:07,  4.58it/s, v_num=0]tensor([[1047523,     849],\n",
      "        [    189,      15]])\n",
      "15 1047523 849 189\n",
      "Epoch 0:  56%|    | 40/72 [00:08<00:06,  4.60it/s, v_num=0]tensor([[1045945,    1157],\n",
      "        [   1279,     195]])\n",
      "195 1045945 1157 1279\n",
      "Epoch 0:  57%|    | 41/72 [00:08<00:06,  4.62it/s, v_num=0]tensor([[1044978,    1139],\n",
      "        [   2014,     445]])\n",
      "445 1044978 1139 2014\n",
      "Epoch 0:  58%|    | 42/72 [00:09<00:06,  4.64it/s, v_num=0]tensor([[1046089,    1028],\n",
      "        [   1379,      80]])\n",
      "80 1046089 1028 1379\n",
      "Epoch 0:  60%|    | 43/72 [00:09<00:06,  4.66it/s, v_num=0]tensor([[1045713,    2836],\n",
      "        [     26,       1]])\n",
      "1 1045713 2836 26\n",
      "Epoch 0:  61%|    | 44/72 [00:09<00:05,  4.67it/s, v_num=0]tensor([[1045640,    2897],\n",
      "        [     30,       9]])\n",
      "9 1045640 2897 30\n",
      "Epoch 0:  62%|   | 45/72 [00:09<00:05,  4.69it/s, v_num=0]tensor([[1048220,     309],\n",
      "        [     44,       3]])\n",
      "3 1048220 309 44\n",
      "Epoch 0:  64%|   | 46/72 [00:09<00:05,  4.70it/s, v_num=0]tensor([[1046373,     349],\n",
      "        [   1570,     284]])\n",
      "284 1046373 349 1570\n",
      "Epoch 0:  65%|   | 47/72 [00:09<00:05,  4.71it/s, v_num=0]tensor([[1045253,     262],\n",
      "        [   2847,     214]])\n",
      "214 1045253 262 2847\n",
      "Epoch 0:  67%|   | 48/72 [00:10<00:05,  4.74it/s, v_num=0]tensor([[1047913,     381],\n",
      "        [    237,      45]])\n",
      "45 1047913 381 237\n",
      "Epoch 0:  68%|   | 49/72 [00:10<00:04,  4.76it/s, v_num=0]tensor([[1046831,     769],\n",
      "        [    604,     372]])\n",
      "372 1046831 769 604\n",
      "Epoch 0:  69%|   | 50/72 [00:10<00:04,  4.79it/s, v_num=0]tensor([[1047500,     743],\n",
      "        [    323,      10]])\n",
      "10 1047500 743 323\n",
      "Epoch 0:  71%|   | 51/72 [00:10<00:04,  4.81it/s, v_num=0]tensor([[1046544,    1901],\n",
      "        [     93,      38]])\n",
      "38 1046544 1901 93\n",
      "Epoch 0:  72%|  | 52/72 [00:10<00:04,  4.83it/s, v_num=0]tensor([[1044035,    1155],\n",
      "        [   1397,    1989]])\n",
      "1989 1044035 1155 1397\n",
      "Epoch 0:  74%|  | 53/72 [00:10<00:03,  4.85it/s, v_num=0]tensor([[1045009,    1424],\n",
      "        [   1559,     584]])\n",
      "584 1045009 1424 1559\n",
      "Epoch 0:  75%|  | 54/72 [00:11<00:03,  4.88it/s, v_num=0]tensor([[1041982,     526],\n",
      "        [   5232,     836]])\n",
      "836 1041982 526 5232\n",
      "Epoch 0:  76%|  | 55/72 [00:11<00:03,  4.89it/s, v_num=0]tensor([[1047562,     463],\n",
      "        [    465,      86]])\n",
      "86 1047562 463 465\n",
      "Epoch 0:  78%|  | 56/72 [00:11<00:03,  4.92it/s, v_num=0]tensor([[1046879,     158],\n",
      "        [    639,     900]])\n",
      "900 1046879 158 639\n",
      "Epoch 0:  79%|  | 57/72 [00:11<00:03,  4.94it/s, v_num=0]tensor([[1047607,     380],\n",
      "        [    276,     313]])\n",
      "313 1047607 380 276\n",
      "Epoch 0:  81%|  | 58/72 [00:11<00:02,  4.96it/s, v_num=0]tensor([[1046397,     577],\n",
      "        [   1271,     331]])\n",
      "331 1046397 577 1271\n",
      "Epoch 0:  82%| | 59/72 [00:11<00:02,  4.98it/s, v_num=0]tensor([[1047462,     633],\n",
      "        [    203,     278]])\n",
      "278 1047462 633 203\n",
      "Epoch 0:  83%| | 60/72 [00:11<00:02,  5.00it/s, v_num=0]tensor([[1044200,     462],\n",
      "        [   3682,     232]])\n",
      "232 1044200 462 3682\n",
      "Epoch 0:  85%| | 61/72 [00:12<00:02,  5.03it/s, v_num=0]tensor([[1047846,     343],\n",
      "        [    192,     195]])\n",
      "195 1047846 343 192\n",
      "Epoch 0:  86%| | 62/72 [00:12<00:01,  5.04it/s, v_num=0]tensor([[1046741,     482],\n",
      "        [   1098,     255]])\n",
      "255 1046741 482 1098\n",
      "Epoch 0:  88%| | 63/72 [00:12<00:01,  5.06it/s, v_num=0]tensor([[1048418,     122],\n",
      "        [     30,       6]])\n",
      "6 1048418 122 30\n",
      "Epoch 0:  89%| | 64/72 [00:12<00:01,  5.08it/s, v_num=0]tensor([[1047516,      69],\n",
      "        [    930,      61]])\n",
      "61 1047516 69 930\n",
      "Epoch 0:  90%| | 65/72 [00:12<00:01,  5.10it/s, v_num=0]tensor([[1048191,     173],\n",
      "        [    191,      21]])\n",
      "21 1048191 173 191\n",
      "Epoch 0:  92%|| 66/72 [00:12<00:01,  5.12it/s, v_num=0]tensor([[1048243,      83],\n",
      "        [    206,      44]])\n",
      "44 1048243 83 206\n",
      "Epoch 0:  93%|| 67/72 [00:13<00:00,  5.13it/s, v_num=0]tensor([[1045960,     620],\n",
      "        [   1472,     524]])\n",
      "524 1045960 620 1472\n",
      "Epoch 0:  94%|| 68/72 [00:13<00:00,  5.15it/s, v_num=0]tensor([[1043355,    1094],\n",
      "        [   2888,    1239]])\n",
      "1239 1043355 1094 2888\n",
      "Epoch 0:  96%|| 69/72 [00:13<00:00,  5.17it/s, v_num=0]tensor([[1047450,     248],\n",
      "        [    437,     441]])\n",
      "441 1047450 248 437\n",
      "Epoch 0:  97%|| 70/72 [00:13<00:00,  5.19it/s, v_num=0]tensor([[1046626,    1116],\n",
      "        [    443,     391]])\n",
      "391 1046626 1116 443\n",
      "Epoch 0:  99%|| 71/72 [00:13<00:00,  5.21it/s, v_num=0]tensor([[589730,     57],\n",
      "        [    27,     10]])\n",
      "10 589730 57 27\n",
      "Epoch 0: 100%|| 72/72 [00:13<00:00,  5.24it/s, v_num=0]tensor([[1048083,      43],\n",
      "        [    439,      11]])\n",
      "11 1048083 43 439\n",
      "tensor([[1047862,     166],\n",
      "        [    397,     151]])\n",
      "151 1047862 166 397\n",
      "tensor([[1048347,      74],\n",
      "        [     89,      66]])\n",
      "66 1048347 74 89\n",
      "tensor([[1047664,     344],\n",
      "        [    459,     109]])\n",
      "109 1047664 344 459\n",
      "tensor([[1046588,     604],\n",
      "        [    756,     628]])\n",
      "628 1046588 604 756\n",
      "tensor([[1048336,     154],\n",
      "        [     83,       3]])\n",
      "3 1048336 154 83\n",
      "tensor([[1047742,     152],\n",
      "        [    458,     224]])\n",
      "224 1047742 152 458\n",
      "tensor([[1048501,      18],\n",
      "        [     57,       0]])\n",
      "0 1048501 18 57\n",
      "tensor([[1047918,     203],\n",
      "        [    325,     130]])\n",
      "130 1047918 203 325\n",
      "Epoch 0: 100%|| 72/72 [00:15<00:00,  4.53it/s, v_num=0, val_loss=0.012, val_fbeta=0.257, val_IoU=0.160, val_tp=1322.0, val_fp=1758.0, val_fn=3063.0, val_tn=9.43e+6, train_loss=0.0508, train_fbeta=0.137, train_IoU=0.0826]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|| 72/72 [00:16<00:00,  4.47it/s, v_num=0, val_loss=0.012, val_fbeta=0.257, val_IoU=0.160, val_tp=1322.0, val_fp=1758.0, val_fn=3063.0, val_tn=9.43e+6, train_loss=0.0508, train_fbeta=0.137, train_IoU=0.0826]\n"
     ]
    }
   ],
   "source": [
    "main(\n",
    "    callbacks=callbacks,\n",
    "    model=model,\n",
    "    loss_fn= loss, #Combined_Focal_Dice_Loss(),\n",
    "    augmentation=augmentation,\n",
    "    preprocessing=preprocessing,\n",
    "    logger=logger,\n",
    "    images_path=DATA_PATH,\n",
    "    optim_dict=optim_dict,\n",
    "    min_epochs=1, #min_epochs,\n",
    "    max_epochs=1, #max_epochs\n",
    "    batch_size=16,\n",
    "    precision='16-mixed' \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 990,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir models/current_best_model/version_33\n",
    "\n",
    "# %tensorboard --logdir logs/DeepLabV3Plus/version_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 991,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoints_paths = [os.path.join(model_main_path,checkpoint_path) for checkpoint_path in os.listdir(model_main_path) if checkpoint_path[-5:]=='.ckpt']\n",
    "checkpoint_path = max(checkpoints_paths, key=lambda x: int(re.search(r'epoch=(\\d+)', x).group(1)))\n",
    "\n",
    "for checkpoint in checkpoints_paths:\n",
    "    if checkpoint != checkpoint_path:\n",
    "        os.remove(checkpoint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# ### Perform the testing, c  NEED TO create a function\n",
    "trained_model = ThermalModel.load_from_checkpoint(checkpoint_path=checkpoint_path,model=model,loss_fn=loss)\n",
    "trained_model.eval();\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    fast_dev_run=False,\n",
    "    accelerator=\"auto\",\n",
    "    strategy=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    num_nodes=1,\n",
    "    logger=logger,\n",
    "    callbacks=callbacks,\n",
    "    max_epochs=1,\n",
    "    min_epochs=1,\n",
    "    precision=precision #Mixed precision training\n",
    ")\n",
    "\n",
    "# Datamodule\n",
    "datamodule = ThermalDataModule(\n",
    "    images_path=DATA_PATH,\n",
    "    augmentation=augmentation,\n",
    "    preprocessing=preprocessing,\n",
    "    batch_size=12,\n",
    "    num_workers=os.cpu_count()\n",
    ")\n",
    "\n",
    "loss_2 = FocalLoss(mode= 'binary')\n",
    "loss_2.__name__ = 'focal_loss'\n",
    "\n",
    "# LightningModule\n",
    "lightning_model = ThermalModel(\n",
    "    model=model,\n",
    "    loss_fn=loss_2,\n",
    "    optim_dict=optim_dict,\n",
    "    lr=3e-4\n",
    ")\n",
    "\n",
    "# test_metrics = trainer.test(model=trained_model,datamodule=datamodule)[0]\n",
    "# trainer.predict(model=trained_model,datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 993,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ThermalModel_sigmoid(\n",
       "  (model): ThermalModel(\n",
       "    (model): Unet(\n",
       "      (encoder): MobileNetV2Encoder(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (2): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "                (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (3): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (4): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (5): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "                (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (6): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "                (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (7): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "                (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (8): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (9): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (10): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (11): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (12): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "                (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (13): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "                (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (14): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "                (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (15): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "                (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (16): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "                (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (17): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "                (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (18): Conv2dNormActivation(\n",
       "            (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (decoder): UnetDecoder(\n",
       "        (center): Identity()\n",
       "        (blocks): ModuleList(\n",
       "          (0): DecoderBlock(\n",
       "            (conv1): Conv2dReLU(\n",
       "              (0): Conv2d(1376, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention1): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "            (conv2): Conv2dReLU(\n",
       "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention2): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "          )\n",
       "          (1): DecoderBlock(\n",
       "            (conv1): Conv2dReLU(\n",
       "              (0): Conv2d(288, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention1): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "            (conv2): Conv2dReLU(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention2): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "          )\n",
       "          (2): DecoderBlock(\n",
       "            (conv1): Conv2dReLU(\n",
       "              (0): Conv2d(152, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention1): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "            (conv2): Conv2dReLU(\n",
       "              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention2): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "          )\n",
       "          (3): DecoderBlock(\n",
       "            (conv1): Conv2dReLU(\n",
       "              (0): Conv2d(80, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention1): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "            (conv2): Conv2dReLU(\n",
       "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention2): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "          )\n",
       "          (4): DecoderBlock(\n",
       "            (conv1): Conv2dReLU(\n",
       "              (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention1): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "            (conv2): Conv2dReLU(\n",
       "              (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention2): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (segmentation_head): SegmentationHead(\n",
       "        (0): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Identity()\n",
       "        (2): Activation(\n",
       "          (activation): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (criterion): FocalLoss()\n",
       "  )\n",
       "  (activation): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 993,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# class ThermalModel_sigmoid(pl.LightningModule):\n",
    "class ThermalModel_sigmoid(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 model: pl.LightningModule,\n",
    "                 activation: Any = 'sigmoid'):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "\n",
    "        self.model = model\n",
    "        self.metrics  = model.metrics\n",
    "        self._device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "\n",
    "        if activation == \"sigmoid\":\n",
    "            self.activation = nn.Sigmoid()\n",
    "\n",
    "    # def Activation(self, activation, **params):\n",
    "    #     if activation == \"sigmoid\":\n",
    "    #         self.activation = nn.Sigmoid()\n",
    "\n",
    "        # elif activation == \"softmax2d\":\n",
    "        #     self.activation = nn.Softmax(dim=1, **params)\n",
    "        # elif activation == \"softmax\":\n",
    "        #     self.activation = nn.Softmax(**params)\n",
    "        # elif activation == \"logsoftmax\":\n",
    "        #     self.activation = nn.LogSoftmax(**params)\n",
    "        # elif activation == \"tanh\":\n",
    "        #     self.activation = nn.Tanh()\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Activation should be callable/sigmoid/softmax/logsoftmax/tanh\"\n",
    "                f\"/None; got {activation}\"\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            x = self.model(x)\n",
    "\n",
    "            output = (self.activation(x)>0.5).float()\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "    def test_step(self, batch: Any, batch_idx: Any):\n",
    "\n",
    "        x, y = batch\n",
    "        x, y = x.to(self._device),y.to(self._device)\n",
    "\n",
    "        assert x.ndim == 4\n",
    "        assert x.max() <= 3 and x.min() >= -3 \n",
    "        assert y.ndim == 4\n",
    "        assert y.max() <= 1 and y.min() >= 0\n",
    "\n",
    "        predictions = self.forward(x.to(torch.float32))\n",
    "        \n",
    "        stage = 'test'\n",
    "\n",
    "        fbeta_score = self.metrics[\"fbeta_score\"](predictions, y)\n",
    "        IoU_score = self.metrics[\"IoU\"](predictions, y)\n",
    "\n",
    "\n",
    "        self.log(f'{stage}_fbeta'  , fbeta_score   , prog_bar=True , on_step=False , on_epoch=True)\n",
    "        self.log(f'{stage}_IoU'    , IoU_score     , prog_bar=True , on_step=False , on_epoch=True)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "new_model = ThermalModel_sigmoid(model=trained_model,activation='sigmoid')\n",
    "new_model\n",
    "\n",
    "# import matplotlib.image as mpimg\n",
    "\n",
    "# image  = mpimg.imread('train_dataset/test/images/Australia_0_G0_(0, 384, 256, 640)_NIR_SWIR.png')\n",
    "\n",
    "# image_model  = np.transpose(image,(2,0,1))[np.newaxis]\n",
    "\n",
    "\n",
    "# new_model(image_model)\n",
    "\n",
    "\n",
    "# torch.save(new_model,os.path.join(model_main_path,'new_model'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_model = ThermalModel.load_from_checkpoint(checkpoint_path='models/Unet_mobilenet_v2_metrics_without_empty_masks/Unet_mobilenet_v2_epoch=149.ckpt',model=model,loss_fn=loss)\n",
    "# trained_model.eval();\n",
    "\n",
    "\n",
    "# trained_model = new_model\n",
    "\n",
    "# x = torch.randn(1, 3, 256, 256).cpu()\n",
    "# model_onnx = trained_model.cpu()\n",
    "# model_onnx.eval()\n",
    "\n",
    "# checkpoint_path = '/home/cristopher/Documents/SegTHRawS_training/model_training/models/Unet_mobilenet_v2_metrics_without_empty_masks/Unet_mobilenet_v2_epoch=149.ckpt'\n",
    "# trained_model = ThermalModel.load_from_checkpoint(checkpoint_path=checkpoint_path,model=model,loss_fn=loss)\n",
    "\n",
    "# onnx_model_path = checkpoint_path.replace('.ckpt','.onnx')\n",
    "# onnx_model_path = checkpoint_path.replace('.ckpt','_sigmoid.onnx')\n",
    "# model_onnx = trained_model.cpu()\n",
    "# model_onnx.eval()\n",
    "\n",
    "# torch_out = model_onnx(x)\n",
    "# import warnings\n",
    "# warnings.filterwarnings(category=FutureWarning,action='ignore')\n",
    "# warnings.filterwarnings(category=torch.jit.TracerWarning,action='ignore')\n",
    "\n",
    "\n",
    "# torch.onnx.export(model_onnx,                                   # model being run\n",
    "#                   x,                                            # model input (or a tuple for multiple inputs)\n",
    "#                   onnx_model_path,                              # where to save the model (can be a file or file-like object)\n",
    "#                   export_params=True,                           # store the trained parameter weights inside the model file\n",
    "#                   opset_version=15,                             # the ONNX version to export the model to\n",
    "#                   do_constant_folding=True,                     # whether to execute constant folding for optimization\n",
    "#                   input_names = ['input'],                      # the model's input names\n",
    "#                   output_names = ['output'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.image as mpimg\n",
    "\n",
    "# image  = mpimg.imread('train_dataset/test/images/Australia_0_G0_(0, 384, 256, 640)_NIR_SWIR.png')\n",
    "\n",
    "# image_model  = np.transpose(image,(2,0,1))[np.newaxis]\n",
    "\n",
    "\n",
    "# trained_model(image_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ThermalModel_sigmoid(\n",
       "  (model): ThermalModel(\n",
       "    (model): Unet(\n",
       "      (encoder): MobileNetV2Encoder(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (2): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "                (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (3): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (4): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (5): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "                (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (6): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "                (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (7): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "                (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (8): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (9): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (10): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (11): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (12): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "                (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (13): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "                (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (14): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "                (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (15): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "                (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (16): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "                (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (17): InvertedResidual(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "                (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU6(inplace=True)\n",
       "              )\n",
       "              (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (18): Conv2dNormActivation(\n",
       "            (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (decoder): UnetDecoder(\n",
       "        (center): Identity()\n",
       "        (blocks): ModuleList(\n",
       "          (0): DecoderBlock(\n",
       "            (conv1): Conv2dReLU(\n",
       "              (0): Conv2d(1376, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention1): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "            (conv2): Conv2dReLU(\n",
       "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention2): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "          )\n",
       "          (1): DecoderBlock(\n",
       "            (conv1): Conv2dReLU(\n",
       "              (0): Conv2d(288, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention1): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "            (conv2): Conv2dReLU(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention2): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "          )\n",
       "          (2): DecoderBlock(\n",
       "            (conv1): Conv2dReLU(\n",
       "              (0): Conv2d(152, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention1): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "            (conv2): Conv2dReLU(\n",
       "              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention2): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "          )\n",
       "          (3): DecoderBlock(\n",
       "            (conv1): Conv2dReLU(\n",
       "              (0): Conv2d(80, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention1): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "            (conv2): Conv2dReLU(\n",
       "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention2): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "          )\n",
       "          (4): DecoderBlock(\n",
       "            (conv1): Conv2dReLU(\n",
       "              (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention1): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "            (conv2): Conv2dReLU(\n",
       "              (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention2): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (segmentation_head): SegmentationHead(\n",
       "        (0): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Identity()\n",
       "        (2): Activation(\n",
       "          (activation): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (criterion): FocalLoss()\n",
       "  )\n",
       "  (activation): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 996,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 997,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(new_model,os.path.join(model_main_path,'trained_new_model'))\n",
    "\n",
    "# trained_model = ThermalModel_sigmoid.load_from_checkpoint(checkpoint_path=os.path.join(model_main_path,'trained_new_model'),model=model,loss_fn=loss)\n",
    "\n",
    "\n",
    "model_2 = torch.load(os.path.join(model_main_path,'trained_new_model'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 998,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|| 9/9 [00:00<00:00, 27.17it/s]\n",
      "\n",
      "       Test metric             DataLoader 0\n",
      "\n",
      "        test_IoU            0.2809372544288635\n",
      "       test_fbeta           0.4355628788471222\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_2.eval()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    fast_dev_run=False,\n",
    "    accelerator=\"auto\",\n",
    "    strategy=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    num_nodes=1,\n",
    "    logger=logger,\n",
    "    callbacks=callbacks,\n",
    "    max_epochs=1,\n",
    "    min_epochs=1,\n",
    "    precision=precision #Mixed precision training\n",
    ")\n",
    "\n",
    "# Datamodule\n",
    "datamodule = ThermalDataModule(\n",
    "    images_path=DATA_PATH,\n",
    "    augmentation=augmentation,\n",
    "    preprocessing=preprocessing,\n",
    "    batch_size=16,\n",
    "    num_workers=os.cpu_count()\n",
    ")\n",
    "\n",
    "loss_2 = FocalLoss(mode= 'binary')\n",
    "loss_2.__name__ = 'focal_loss'\n",
    "\n",
    "# # LightningModule\n",
    "# lightning_model = ThermalModel(\n",
    "#     model=model,\n",
    "#     loss_fn=loss_2,\n",
    "#     optim_dict=optim_dict,\n",
    "#     lr=3e-4\n",
    "# )\n",
    "\n",
    "test_metrics = trainer.test(model=model_2,datamodule=datamodule)[0]\n",
    "# trainer.predict(model=model_2,datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 999,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# %matplotlib inline\n",
    "# # %matplotlib widget\n",
    "# import os\n",
    "\n",
    "# from matplotlib.ticker import AutoMinorLocator, MultipleLocator\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# model_path = os.path.join(os.getcwd(),'models','current_best_model')\n",
    "\n",
    "# metric_id_list = []\n",
    "\n",
    "# for file_name in os.listdir(model_path):\n",
    "#     if file_name[-4:]=='.csv':\n",
    "#         fig, ax = plt.subplots(figsize=(7,5))\n",
    "        \n",
    "#         csv_file = pd.read_csv(os.path.join(model_path,file_name))\n",
    "#         metric_name = file_name.replace('_evolution.csv','')\n",
    "        \n",
    "\n",
    "#         ax.yaxis.set_minor_locator(AutoMinorLocator(4))\n",
    "\n",
    "#         if metric_name[:5]=='train':\n",
    "#             ax.set_xlabel('# epochs')\n",
    "\n",
    "#             metric_id = metric_name.replace('train_','')    \n",
    "#             if metric_id not in metric_id_list:\n",
    "#                 for inner_file_name in os.listdir(model_path):\n",
    "#                     if inner_file_name[-4:]=='.csv' and inner_file_name.replace('_evolution.csv','')==f'val_{metric_id}':\n",
    "#                         csv_file_inner = pd.read_csv(os.path.join(model_path,inner_file_name))\n",
    "                        \n",
    "#                         csv_values = csv_file['Value']\n",
    "#                         csv_inner_values = csv_file_inner['Value']\n",
    "                        \n",
    "#                         ax.plot(csv_values,color='r',label='Training')\n",
    "#                         ax.plot(csv_inner_values,color='b',label='Validation')\n",
    "#                         ax.set_title(f'{metric_id} evolution')\n",
    "#                         ax.set_ylabel(f'{metric_id}')\n",
    "#                         plt.annotate('%0.2f' % csv_values[len(csv_values)-1], xy=(len(csv_values)-1, csv_values[len(csv_values)-1]))\n",
    "#                         plt.annotate('%0.2f' % csv_inner_values[len(csv_inner_values)-1], xy=(len(csv_inner_values)-1, csv_inner_values[len(csv_inner_values)-1]))\n",
    "#                         metric_id_list.append(metric_id)\n",
    "#                         break\n",
    "#             else:\n",
    "#                 metric_id = None\n",
    "#                 plt.close(fig)\n",
    "#         elif metric_name[:3]=='val':\n",
    "#             ax.set_xlabel('# epochs')\n",
    "            \n",
    "#             metric_id = metric_name.replace('val_','')\n",
    "#             if metric_id not in metric_id_list:\n",
    "#                 for inner_file_name in os.listdir(model_path):\n",
    "#                     if inner_file_name[-4:]=='.csv' and inner_file_name.replace('_evolution.csv','')==f'train_{metric_id}':\n",
    "#                         csv_file_inner = pd.read_csv(os.path.join(model_path,inner_file_name))\n",
    "                        \n",
    "#                         csv_values = csv_file['Value']\n",
    "#                         csv_inner_values = csv_file_inner['Value']\n",
    "\n",
    "#                         ax.plot(csv_values,color='b',label='Validation')\n",
    "#                         ax.plot(csv_inner_values,color='r',label='Training')\n",
    "#                         ax.set_title(f'{metric_id} evolution')\n",
    "#                         ax.set_ylabel(f'{metric_id}')\n",
    "\n",
    "#                         plt.annotate('%0.2f' % csv_values[len(csv_values)-1], xy=(len(csv_values)-1, csv_values[len(csv_values)-1]))\n",
    "#                         plt.annotate('%0.2f' % csv_inner_values[len(csv_inner_values)-1], xy=(len(csv_inner_values)-1, csv_inner_values[len(csv_inner_values)-1]))\n",
    "                        \n",
    "#                         metric_id_list.append(metric_id)\n",
    "#             else:\n",
    "#                 metric_id = None\n",
    "#                 plt.close(fig)\n",
    "\n",
    "#         else:\n",
    "#             csv_values = csv_file['Value']\n",
    "#             ax.set_title(f'{metric_name} evolution')\n",
    "#             ax.set_ylabel(f'{metric_name}')\n",
    "#             ax.set_xlabel('Time')\n",
    "#             ax.plot(csv_values,color='g',label='learning rate')\n",
    "#             plt.annotate('%.0E' % csv_values[0], xy=(0, csv_values[0]))\n",
    "#             plt.annotate('%.0E' % csv_values[len(csv_values)-1], xy=(len(csv_values)-1, csv_values[len(csv_values)-1]))\n",
    "#             metric_id = 'lr'\n",
    "#         # ax.plot(csv_file['Value'])\n",
    "#         if metric_id:\n",
    "#             ax.legend()\n",
    "#             plt.savefig(os.path.join(model_path,metric_id+'_evolution.png'))\n",
    "#         # break\n",
    "#         # break\n",
    "#         # plt.show()\n",
    "#         # break\n",
    "#         # plt.savefig(os.path.join(model_path,file_name.replace('.csv','.png')))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1000,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.randn(1, 3, 256, 256).cpu()\n",
    "# # model_onnx = trained_model.cpu()\n",
    "# # model_onnx.eval()\n",
    "\n",
    "# # checkpoint_path = '/home/cristopher/Documents/SegTHRawS_training/model_training/models/Unet_mobilenet_v2_metrics_without_empty_masks/Unet_mobilenet_v2_epoch=149.ckpt'\n",
    "# # trained_model = ThermalModel.load_from_checkpoint(checkpoint_path=checkpoint_path,model=model,loss_fn=loss)\n",
    "# # onnx_model_path = checkpoint_path.replace('.ckpt','.onnx')\n",
    "\n",
    "# # trained_model = torch.load('interesting_models/unet_mobilenet_v2_sigmoid_test_iou_0_91')\n",
    "# # onnx_model_path = 'interesting_models/unet_mobilenet_v2_sigmoid_test_iou_0_91.onnx'\n",
    "# model_onnx = trained_model.cpu()\n",
    "# model_onnx.eval()\n",
    "\n",
    "# torch_out = model_onnx(x)\n",
    "# import warnings\n",
    "# warnings.filterwarnings(category=FutureWarning,action='ignore')\n",
    "# warnings.filterwarnings(category=torch.jit.TracerWarning,action='ignore')\n",
    "\n",
    "# # Export the model\n",
    "# # torch.onnx.export(model_onnx,                                   # model being run\n",
    "# #                   x,                                            # model input (or a tuple for multiple inputs)\n",
    "# #                   onnx_model_path,                              # where to save the model (can be a file or file-like object)\n",
    "# #                   export_params=True,                           # store the trained parameter weights inside the model file\n",
    "# #                   opset_version=16,                             # the ONNX version to export the model to\n",
    "# #                   do_constant_folding=True,                     # whether to execute constant folding for optimization\n",
    "# #                   input_names = ['input'],                      # the model's input names\n",
    "# #                   output_names = ['output'],                    # the model's output names\n",
    "# #                   dynamic_axes={'input' : {0 : 'batch_size'},   # variable length axes\n",
    "# #                                 'output' : {0 : 'batch_size'}})\n",
    "\n",
    "# torch.onnx.export(model_onnx,                                   # model being run\n",
    "#                   x,                                            # model input (or a tuple for multiple inputs)\n",
    "#                   onnx_model_path,                              # where to save the model (can be a file or file-like object)\n",
    "#                   export_params=True,                           # store the trained parameter weights inside the model file\n",
    "#                   opset_version=15,                             # the ONNX version to export the model to\n",
    "#                   do_constant_folding=True,                     # whether to execute constant folding for optimization\n",
    "#                   input_names = ['input'],                      # the model's input names\n",
    "#                   output_names = ['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1001,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import configparser\n",
    "\n",
    "# model_info = configparser.ConfigParser()\n",
    "# model_info.read(os.path.join(onnx_model_path,'info.ini'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1002,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# onnx_model = onnx.load(onnx_model_path)\n",
    "# onnx.checker.check_model(onnx_model)\n",
    "\n",
    "\n",
    "# ort_session = onnxruntime.InferenceSession(onnx_model_path, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "# def to_numpy(tensor):\n",
    "#     return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "# # compute ONNX Runtime output prediction\n",
    "# ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n",
    "# ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# # compare ONNX Runtime and PyTorch results\n",
    "# np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n",
    "\n",
    "# print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_version_folder_path = os.path.join(model_main_path,'csv_logs',f'{model_name}_{ENCODER}')\n",
    "\n",
    "\n",
    "# version_files = os.listdir(metrics_version_folder_path)\n",
    "\n",
    "# version_files.sort(key=lambda x: os.path.getmtime(os.path.join(metrics_version_folder_path,x)),reverse=True)\n",
    "\n",
    "# metrics_df = pd.read_csv(os.path.join(metrics_version_folder_path,version_files[0],'metrics.csv'))\n",
    "# metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = metrics_df.copy()\n",
    "\n",
    "# selected_rows = df.dropna(subset=['epoch'],axis=0)\n",
    "# input_epoch = df['epoch'].max()\n",
    "\n",
    "\n",
    "# #Move the test values from epoch 0 to the epoch of the model \n",
    "# selected_rows.loc[df['test_IoU'].notna(),'epoch'] = input_epoch\n",
    "\n",
    "# combined_df = pd.DataFrame(columns=selected_rows.columns)\n",
    "\n",
    "# for _, group in selected_rows.groupby('epoch'):\n",
    "#     if len(group)==2:\n",
    "\n",
    "#         row1 = group.iloc[0]\n",
    "#         row2 = group.iloc[1]\n",
    "\n",
    "#         # print('Row 1: ',row1)\n",
    "#         # print('Row 2: ',row2)\n",
    "        \n",
    "#         combined_row = pd.DataFrame({\n",
    "#             col: [row1[col] if pd.notna(row1[col]) else row2[col]] for col in df.columns\n",
    "#         })\n",
    "\n",
    "#         combined_df = pd.concat([combined_df,combined_row],ignore_index=True)\n",
    "#     elif len(group)==3:\n",
    "#         row1 = group.iloc[0]\n",
    "#         row2 = group.iloc[1]\n",
    "#         row3 = group.iloc[2]\n",
    "\n",
    "#         # print('Row 1: ',row1)\n",
    "#         # print('Row 2: ',row2)\n",
    "        \n",
    "#         combined_row = pd.DataFrame({\n",
    "#             col: [row1[col] if pd.notna(row1[col]) else row2[col] if pd.notna(row2[col]) else row3[col]] for col in df.columns\n",
    "#         })\n",
    "\n",
    "#         combined_df = pd.concat([combined_df,combined_row],ignore_index=True)\n",
    "\n",
    "# csv_name = os.path.basename(checkpoint_path).replace('ckpt','csv')\n",
    "\n",
    "# metrics_csv_path = os.path.join(metrics_path,f'metrics_{csv_name}   ')\n",
    "\n",
    "# plots_path = os.path.join(os.path.dirname(metrics_csv_path),'plots')\n",
    "# os.makedirs(plots_path,exist_ok=True)\n",
    "\n",
    "# combined_df.to_csv(metrics_csv_path,index=False)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# ax.plot(metrics_df['lr-Adam'].dropna(),color='g')\n",
    "# ax.set_xlabel('# epochs')\n",
    "# ax.set_ylabel(f'Lr evolution')\n",
    "# ax.set_title(f'Lr evolution',fontname=\"Charter\",weight='bold')\n",
    "\n",
    "# plt.yscale('log')\n",
    "# # plt.savefig(os.path.join(plots_path,'Lr_evolution.png'))\n",
    "\n",
    "# # # ax.plot(train_values,color='b',label=f'Validation {train_values[len(train_values)-1]:.3E}')\n",
    "# # plt.gca().yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter(\"{x:.3e}\"))\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # shutil.rmtree(os.path.join(metrics_version_folder_path,version_files[0])) #Delete the old metrics file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1005,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_df_test = pd.read_csv('/home/cristopher/Documents/SegTHRawS training/model_training/models/Unet_mobilenet_v2_metrics_without_empty_masks/metrics.csv')\n",
    "# # plt.plot(metrics_df_test['lr-Adam'].dropna())\n",
    "# # plt.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "\n",
    "# # # ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "# # # plt.yscale('log')\n",
    "\n",
    "# # plt.gca().yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter(\"{x:.3e}\"))\n",
    "# # plt.set_xlim(0,100)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# ax.plot(metrics_df_test['lr-Adam'].dropna())\n",
    "# ax.set_xlabel('# epochs')\n",
    "# ax.set_ylabel(f'Lr')\n",
    "\n",
    "# plt.yscale('log')\n",
    "\n",
    "# # ax.plot(train_values,color='b',label=f'Validation {train_values[len(train_values)-1]:.3E}')\n",
    "# # ax.set_xlabel('# epochs')\n",
    "# # ax.set_ylabel(f'{metric_id[0].upper()+metric_id[1:]}')\n",
    "\n",
    "# # ax.set_title(f'{metric_id[0].upper()+metric_id[1:]} evolution',fontname=\"Charter\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1006,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# metric_id_list = []\n",
    "\n",
    "\n",
    "\n",
    "# # csv_file = pd.read_csv(os.path.join(metrics_csv_path))\n",
    "\n",
    "# csv_file = metrics_df.copy()\n",
    "\n",
    "# columns_to_drop = ['lr-Adam','step','test_acc','test_IoU','test_fbeta','train_acc','val_acc']\n",
    "\n",
    "# new_test_df = csv_file.drop(columns=columns_to_drop)\n",
    "\n",
    "\n",
    "\n",
    "# for metric_name in new_test_df.columns[1:]:\n",
    "#     fig, ax = plt.subplots(figsize=(7,5))\n",
    "#     if metric_name[:5]=='train':\n",
    "#         ax.set_xlabel('# epochs')\n",
    "\n",
    "#         metric_id = metric_name.replace('train_','')    \n",
    "#         if metric_id not in metric_id_list:\n",
    "#             val_metric_name = metric_name.replace('train','val')\n",
    "#             train_values = new_test_df[metric_name].astype('float32')\n",
    "#             val_values = new_test_df[val_metric_name].astype('float32')\n",
    "#             ax.plot(train_values,color='b',label=f'Validation {train_values[len(train_values)-1]:.3E}')\n",
    "#             ax.plot(val_values,color='r',label=  f'Training    {val_values[len(val_values)-1]:.3E}')\n",
    "#             if metric_id == 'fbeta':\n",
    "#                 ax.set_title('F-1 evolution',fontname=\"Charter\",weight='bold')\n",
    "#             else:\n",
    "#                 ax.set_title(f'{metric_id[0].upper()+metric_id[1:]} evolution',fontname=\"Charter\",weight='bold')\n",
    "            \n",
    "#             ax.set_ylabel(f'{metric_id[0].upper()+metric_id[1:]}')\n",
    "\n",
    "#             print(metric_id)\n",
    "            \n",
    "#             # plt.annotate('%0.2f' % train_values[len(train_values)-1], xy=(len(train_values)-1, train_values[len(train_values)-1]))\n",
    "#             # plt.annotate('%0.2E' % train_values[len(train_values)-1], xy=(new_test_df['epoch'][len(new_test_df['epoch'])-1], train_values[len(train_values)-1]),xytext=(-40,5),textcoords='offset pixels')\n",
    "\n",
    "#             # plt.annotate('%0.2E' % val_values[len(val_values)-1], xy=(new_test_df['epoch'][len(new_test_df['epoch'])-1], val_values[len(val_values)-1]),xytext=(-40,5),textcoords='offset pixels')\n",
    "#             metric_id_list.append(metric_id)\n",
    "#         else:\n",
    "#             metric_id = None\n",
    "#             plt.close(fig)\n",
    "#     elif metric_name[:3]=='val':\n",
    "#         ax.set_xlabel('# epochs')\n",
    "        \n",
    "#         metric_id = metric_name.replace('val_','')\n",
    "#         if metric_id not in metric_id_list:\n",
    "\n",
    "#             train_metric_name = metric_name.replace('val','train')\n",
    "#             train_values = new_test_df[metric_name].astype('float32')\n",
    "#             val_values = new_test_df[train_metric_name].astype('float32')\n",
    "\n",
    "#             ax.plot(train_values,color='b',label=f'Validation {train_values[len(train_values)-1]:.3E}',fontname=\"Charter\")\n",
    "#             ax.plot(val_values,color='r',label=f'Training {val_values[len(val_values)-1]:.3E}',fontname=\"Charter\")\n",
    "#             if metric_id == 'fbeta':\n",
    "#                 ax.set_title('F-1 evolution',fontname=\"Charter\",weight='bold')\n",
    "#             else:\n",
    "#                 ax.set_title(f'{metric_id[0].upper()+metric_id[1:]} evolution',fontname=\"Charter\",weight='bold')\n",
    "            \n",
    "#             ax.set_ylabel(f'{metric_id[0].upper()+metric_id[1:]}')\n",
    "\n",
    "#             # plt.annotate('%0.2E' % train_values[len(train_values)-1], xy=(new_test_df['epoch'][len(new_test_df['epoch'])-1], train_values[len(train_values)-1]),xytext=(-40,5),textcoords='offset pixels')\n",
    "\n",
    "#             # plt.annotate('%0.2E' % val_values[len(val_values)-1], xy=(new_test_df['epoch'][len(new_test_df['epoch'])-1], val_values[len(val_values)-1]),xytext=(-40,5),textcoords='offset pixels')\n",
    "            \n",
    "#             metric_id_list.append(metric_id)\n",
    "#         else:\n",
    "            \n",
    "#             metric_id = None\n",
    "#             plt.close(fig)\n",
    "#     else:\n",
    "#         print(metric_name)\n",
    "#         print('ERROR')\n",
    "#         plt.close(fig)\n",
    "\n",
    "\n",
    "#     if metric_id:\n",
    "#         ax.legend()\n",
    "#         # plt.savefig(os.path.join(plots_path,metric_id+'_evolution.png'))\n",
    "\n",
    "#         plt.show()\n",
    "#     # plt.savefig(os.path.join(model_path,file_name.replace('.csv','.png')))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = combined_df.copy()\n",
    "\n",
    "# columns_to_drop = ['lr-Adam','step','test_IoU','test_acc','test_fbeta','test_jaccard','test_loss','train_acc','val_acc']\n",
    "\n",
    "# new_test_df = test_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# for column in new_test_df.columns[1:]:\n",
    "#     fig, ax = plt.subplots()\n",
    "#     ax.set_xlabel('# epochs')\n",
    "#     # ax.set_title(column)\n",
    "#     ax.set_title(f'{column} evolution')\n",
    "#     ax.set_ylabel(column)\n",
    "#     ax.plot(new_test_df['epoch'],new_test_df[column],color='k')\n",
    "\n",
    "\n",
    "#     if column[-4:]=='loss':\n",
    "#         plt.annotate('%0.2E' % new_test_df[column][len(new_test_df[column])-1], xy=(new_test_df['epoch'][len(new_test_df['epoch'])-1], new_test_df[column][len(new_test_df[column])-1]),xytext=(-50,10),textcoords='offset pixels')\n",
    "#     else:\n",
    "#         ax.set_ylim(0,1)\n",
    "#         plt.annotate('%0.3E' % new_test_df[column][len(new_test_df[column])-1], xy=(new_test_df['epoch'][len(new_test_df['epoch'])-1], new_test_df[column][len(new_test_df[column])-1]),xytext=(-40,6),textcoords='offset pixels')\n",
    "\n",
    "\n",
    "# # ax.yaxis.set_minor_locator(AutoMinorLocator(4))\n",
    "# # print(new_test_df[column][len(new_test_df[column])-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# %matplotlib inline\n",
    "# # %matplotlib widget\n",
    "# import os\n",
    "\n",
    "# import matplotlib as mpl\n",
    "# mpl.rc('font',family='Charter')\n",
    "\n",
    "# from matplotlib.ticker import AutoMinorLocator, MultipleLocator\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # model_path = os.path.join(os.getcwd(),'models','current_best_model')\n",
    "\n",
    "# metric_id_list = []\n",
    "\n",
    "# model_name = 'Unet'\n",
    "\n",
    "# ENCODER = 'mobilenet_v2'\n",
    "\n",
    "# file_name = os.path.join(os.getcwd(),'models',f'{model_name}_{ENCODER}_metrics_without_empty_masks',f'metrics_{model_name}_{ENCODER}.csv')\n",
    "# plots_path = os.path.join(os.path.dirname(file_name),'plots')\n",
    "# os.makedirs(plots_path,exist_ok=True)\n",
    "\n",
    "\n",
    "# csv_file = pd.read_csv(os.path.join(file_name))\n",
    "\n",
    "# columns_to_drop = ['lr-Adam','step','test_IoU','test_acc','test_fbeta','test_jaccard','test_loss','train_acc','val_acc']\n",
    "\n",
    "# new_test_df = csv_file.drop(columns=columns_to_drop)\n",
    "\n",
    "# # ax.yaxis.set_minor_locator(AutoMinorLocator(4))\n",
    "\n",
    "# for metric_name in new_test_df.columns[1:]:\n",
    "#     fig, ax = plt.subplots(figsize=(7,5))\n",
    "#     if metric_name[:5]=='train':\n",
    "#         ax.set_xlabel('# epochs')\n",
    "\n",
    "#         metric_id = metric_name.replace('train_','')    \n",
    "#         if metric_id not in metric_id_list:\n",
    "#             val_metric_name = metric_name.replace('train','val')\n",
    "#             train_values = new_test_df[metric_name].astype('float32')\n",
    "#             val_values = new_test_df[val_metric_name].astype('float32')\n",
    "#             ax.plot(train_values,color='b',label=f'Validation {train_values[len(train_values)-1]:.3E}')\n",
    "#             ax.plot(val_values,color='r',label=  f'Training    {val_values[len(val_values)-1]:.3E}')\n",
    "#             if metric_id == 'fbeta':\n",
    "#                 ax.set_title('F-1 evolution',fontname=\"Charter\")\n",
    "#             else:\n",
    "#                 ax.set_title(f'{metric_id[0].upper()+metric_id[1:]} evolution',fontname=\"Charter\")\n",
    "            \n",
    "#             ax.set_ylabel(f'{metric_id[0].upper()+metric_id[1:]}')\n",
    "            \n",
    "#             # plt.annotate('%0.2f' % train_values[len(train_values)-1], xy=(len(train_values)-1, train_values[len(train_values)-1]))\n",
    "#             # plt.annotate('%0.2E' % train_values[len(train_values)-1], xy=(new_test_df['epoch'][len(new_test_df['epoch'])-1], train_values[len(train_values)-1]),xytext=(-40,5),textcoords='offset pixels')\n",
    "\n",
    "#             # plt.annotate('%0.2E' % val_values[len(val_values)-1], xy=(new_test_df['epoch'][len(new_test_df['epoch'])-1], val_values[len(val_values)-1]),xytext=(-40,5),textcoords='offset pixels')\n",
    "#             metric_id_list.append(metric_id)\n",
    "#         else:\n",
    "#             metric_id = None\n",
    "#             plt.close(fig)\n",
    "#     elif metric_name[:3]=='val':\n",
    "#         ax.set_xlabel('# epochs')\n",
    "        \n",
    "#         metric_id = metric_name.replace('val_','')\n",
    "#         if metric_id not in metric_id_list:\n",
    "\n",
    "#             train_metric_name = metric_name.replace('val','train')\n",
    "#             train_values = new_test_df[metric_name].astype('float32')\n",
    "#             val_values = new_test_df[train_metric_name].astype('float32')\n",
    "\n",
    "#             ax.plot(train_values,color='b',label=f'Validation {train_values[len(train_values)-1]:.3E}',fontname=\"Charter\")\n",
    "#             ax.plot(val_values,color='r',label=f'Training {val_values[len(val_values)-1]:.3E}',fontname=\"Charter\")\n",
    "#             ax.set_title(f'{metric_id[0].upper()+metric_id[1:]} evolution',fontname=\"Charter\")\n",
    "#             ax.set_ylabel(f'{metric_id[0].upper()+metric_id[1:]}')\n",
    "\n",
    "#             # plt.annotate('%0.2E' % train_values[len(train_values)-1], xy=(new_test_df['epoch'][len(new_test_df['epoch'])-1], train_values[len(train_values)-1]),xytext=(-40,5),textcoords='offset pixels')\n",
    "\n",
    "#             # plt.annotate('%0.2E' % val_values[len(val_values)-1], xy=(new_test_df['epoch'][len(new_test_df['epoch'])-1], val_values[len(val_values)-1]),xytext=(-40,5),textcoords='offset pixels')\n",
    "            \n",
    "#             metric_id_list.append(metric_id)\n",
    "#         else:\n",
    "#             metric_id = None\n",
    "#             plt.close(fig)\n",
    "#     else:\n",
    "#         print('ERROR')\n",
    "#         plt.close(fig)\n",
    "\n",
    "\n",
    "#     if metric_id:\n",
    "#         ax.legend()\n",
    "#         plt.savefig(os.path.join(plots_path,metric_id+'_evolution.png'))\n",
    "\n",
    "#         plt.show()\n",
    "#     # plt.savefig(os.path.join(model_path,file_name.replace('.csv','.png')))\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
