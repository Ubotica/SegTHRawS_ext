{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "import torchvision.transforms as T\n",
    "import albumentations.pytorch as pytorch\n",
    "import albumentations as albu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchmetrics import Accuracy, JaccardIndex, FBetaScore\n",
    "from typing import Any, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x798e741e3110>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from segmentation_models_pytorch.utils import metrics\n",
    "\n",
    "from segmentation_models_pytorch.losses import FocalLoss, DiceLoss, JaccardLoss\n",
    "\n",
    "import re\n",
    "\n",
    "import onnx\n",
    "import onnxruntime\n",
    "\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "# %matplotlib widget\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rc('font',family='Charter')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import shutil\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThermalDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 stage: str,\n",
    "                 images_path: str,\n",
    "                 augmentation: Any,\n",
    "                 preprocessing: Any,\n",
    "                 shuffle: bool = True,\n",
    "                 random_state: int = 42):\n",
    "\n",
    "        self.__attribute_checking(images_path,\n",
    "                                  stage, shuffle, random_state)\n",
    "\n",
    "        self.images_path = images_path\n",
    "\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "\n",
    "        self.stage = stage\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "        self.total_len = None\n",
    "        self._images, self._masks = self.__create_dataset()\n",
    "\n",
    "    @staticmethod\n",
    "    def __type_checking(images_path: str,\n",
    "                        stage: str, shuffle: bool,\n",
    "                        random_state: int) -> None:\n",
    "        \n",
    "        assert isinstance(images_path, str)\n",
    "        assert isinstance(stage, str)\n",
    "        assert isinstance(shuffle, bool)\n",
    "        assert isinstance(random_state, int)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def __path_checking(images_path: str) -> None:\n",
    "        assert os.path.isdir(images_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def __stage_checking(stage: str) -> None:\n",
    "        assert stage in [\"train\", \"test\", \"val\"]\n",
    "\n",
    "    @classmethod\n",
    "    def __attribute_checking(cls, images_path: str,\n",
    "                             stage: str,\n",
    "                             shuffle: bool,\n",
    "                             random_state: int) -> None:\n",
    "\n",
    "        cls.__type_checking(images_path=images_path,\n",
    "                            stage=stage,\n",
    "                            shuffle=shuffle,\n",
    "                            random_state=random_state)\n",
    "\n",
    "        cls.__path_checking(images_path=images_path)\n",
    "\n",
    "        cls.__stage_checking(stage=stage)\n",
    "\n",
    "    def __create_dataset(self) -> dict:\n",
    "        dict_paths = {\n",
    "            \"image\": [],\n",
    "            \"mask\": []\n",
    "        }\n",
    "\n",
    "        images_path = self.__split_data(self.stage)\n",
    "\n",
    "        for image_name in os.listdir(images_path):\n",
    "            dict_paths[\"image\"].append(os.path.join(images_path,image_name))\n",
    "            dict_paths[\"mask\"].append(os.path.join(os.path.dirname(images_path),'masks',image_name.replace('_NIR_SWIR','_mask')))\n",
    "\n",
    "        dataframe = pd.DataFrame(\n",
    "            data=dict_paths,\n",
    "            index=np.arange(0, len(dict_paths[\"image\"]))\n",
    "        )\n",
    "        self.total_len = len(dataframe)\n",
    "        data_dict = {self.stage: (dataframe[\"image\"].values,dataframe[\"mask\"].values)}\n",
    "\n",
    "        return data_dict[self.stage]\n",
    "\n",
    "    def __split_data(self, stage: str) -> str:\n",
    "        return os.path.join(self.images_path,stage,'images')\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.total_len\n",
    "\n",
    "    def __getitem__(self, idx) -> tuple:\n",
    "\n",
    "        image = Image.open(self._images[idx])\n",
    "        mask = Image.open(self._masks[idx])\n",
    "        \n",
    "        image = np.array(image)\n",
    "\n",
    "        ### FOR FOCAL LOSS\n",
    "        mask = mask.convert('L') # This ensures that the label only have 1 band, which is necessary for binary classification\n",
    "        mask = np.array(mask)[:,:,np.newaxis]\n",
    "        \n",
    "        mask = np.divide(mask,255).astype('float32') #Masks need to be 0-1 values\n",
    "        \n",
    "        # # apply augmentation\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "        albu.VerticalFlip(p=0.5),\n",
    "\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThermalDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,images_path: str,\n",
    "                 augmentation: Union[T.Compose, A.Compose],\n",
    "                 preprocessing: Any,\n",
    "                 batch_size: int = 5,\n",
    "                 num_workers: int = os.cpu_count(),\n",
    "                 seed: int = 42):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.images_path = images_path\n",
    "        self.data_train = None\n",
    "        self.data_val = None\n",
    "        self.data_test = None\n",
    "        self.data_predict = None\n",
    "        self.seed = seed\n",
    "\n",
    "        self.train_augmentation = augmentation\n",
    "        self.eval_augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "\n",
    "\n",
    "    def setup(self, stage: str = None) -> None:\n",
    "        self.data_train = ThermalDataset(\n",
    "            images_path=self.images_path,\n",
    "            augmentation=self.train_augmentation,\n",
    "            preprocessing=self.preprocessing,\n",
    "            stage=\"train\",\n",
    "            shuffle=True,\n",
    "            random_state=self.seed\n",
    "            )\n",
    "\n",
    "        self.data_val = ThermalDataset(\n",
    "            images_path=self.images_path,\n",
    "            augmentation=self.eval_augmentation,\n",
    "            preprocessing=self.preprocessing,\n",
    "            stage=\"val\",\n",
    "            shuffle=True,\n",
    "            random_state=self.seed\n",
    "            )\n",
    "\n",
    "        self.data_test = ThermalDataset(\n",
    "            images_path=self.images_path,\n",
    "            augmentation=self.eval_augmentation,\n",
    "            preprocessing=self.preprocessing,\n",
    "            stage=\"test\",\n",
    "            shuffle=True,\n",
    "            random_state=self.seed\n",
    "            )\n",
    "\n",
    "        self.data_predict = ThermalDataset(\n",
    "            images_path=self.images_path,\n",
    "            augmentation=self.eval_augmentation,\n",
    "            preprocessing=self.preprocessing,\n",
    "            stage=\"test\",\n",
    "            shuffle=True,\n",
    "            random_state=self.seed\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            dataset=self.data_train,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            dataset=self.data_val,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            dataset=self.data_test,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            dataset=self.data_predict,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ThermalModel(pl.LightningModule):\n",
    "#     def __init__(self,\n",
    "#                  model: nn.Module,\n",
    "#                  loss_fn: Any,\n",
    "#                  optim_dict: dict = None,\n",
    "#                  lr: float = None,\n",
    "#                  num_classes: int = 1):\n",
    "#         super().__init__()\n",
    "#         self.save_hyperparameters(ignore=['model','loss_fn'])\n",
    "\n",
    "#         self.num_classes = num_classes\n",
    "#         self.model = model\n",
    "#         # self.criterion = nn.CrossEntropyLoss()\n",
    "#         self.criterion = loss_fn\n",
    "#         self.optim_dict = optim_dict\n",
    "#         self._device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "\n",
    "#         self.step_outputs = {\n",
    "#             \"loss\": [],\n",
    "#             \"accuracy\": [],\n",
    "#             \"jaccard_index\": [],\n",
    "#             \"fbeta_score\": [],\n",
    "#             \"IoU\": []\n",
    "#         }\n",
    "\n",
    "#         self.metrics = {\n",
    "#             \"accuracy\": Accuracy(task=\"binary\",\n",
    "#                                  threshold=0.5,\n",
    "#                                  num_classes=num_classes,\n",
    "#                                  validate_args=True,\n",
    "#                                  ignore_index=None,\n",
    "#                                  average=\"micro\").to(self._device),\n",
    "\n",
    "#             \"jaccard_index\": JaccardIndex(task=\"binary\",\n",
    "#                                           threshold=0.5,\n",
    "#                                           num_classes=num_classes,\n",
    "#                                           validate_args=True,\n",
    "#                                           ignore_index=None,\n",
    "#                                           average=\"macro\").to(self._device),\n",
    "\n",
    "#             \"fbeta_score\": FBetaScore(task=\"binary\",\n",
    "#                                       beta=1.0,\n",
    "#                                       threshold=0.5,\n",
    "#                                       num_classes=num_classes,\n",
    "#                                       average=\"micro\",\n",
    "#                                       ignore_index=None,\n",
    "#                                       validate_args=True).to(self._device),\n",
    "\n",
    "#             \"IoU\": metrics.IoU()\n",
    "#         }\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "#     def shared_step(self, batch, stage: str) -> torch.Tensor:\n",
    "#         x, y = batch\n",
    "#         x, y = x.to(self._device),y.to(self._device)\n",
    "\n",
    "#         assert x.ndim == 4\n",
    "#         assert x.max() <= 3 and x.min() >= -3 \n",
    "#         assert y.ndim == 4\n",
    "#         assert y.max() <= 1 and y.min() >= 0\n",
    "\n",
    "#         logits = self.forward(x.to(torch.float32))\n",
    "        \n",
    "\n",
    "#         # activated = F.softmax(input=logits, dim=1)\n",
    "#         # predictions = torch.argmax(activated, dim=1)\n",
    "\n",
    "#         predictions = torch.round(torch.sigmoid(logits))\n",
    "#         # predictions = logits\n",
    "        \n",
    "#         loss = self.criterion(logits, y)\n",
    "        \n",
    "#         accuracy = self.metrics[\"accuracy\"](predictions, y)\n",
    "#         jaccard_index = self.metrics[\"jaccard_index\"](predictions, y)\n",
    "#         fbeta_score = self.metrics[\"fbeta_score\"](predictions, y)\n",
    "#         IoU_score = self.metrics[\"IoU\"](predictions, y)\n",
    "\n",
    "#         # print(f'stage: {stage}')\n",
    "#         # print(f'Jaccard: {jaccard_index.dtype}')\n",
    "#         # print(f'loss: {loss.dtype}')\n",
    "\n",
    "#         self.step_outputs[\"loss\"].append(loss)\n",
    "#         self.step_outputs[\"accuracy\"].append(accuracy)\n",
    "#         self.step_outputs[\"jaccard_index\"].append(jaccard_index)\n",
    "#         self.step_outputs[\"fbeta_score\"].append(fbeta_score)\n",
    "#         self.step_outputs[\"IoU\"].append(IoU_score)\n",
    "\n",
    "\n",
    "#         self.log(f'{stage}_loss'   , loss          , prog_bar=True , on_step=False , on_epoch=True)\n",
    "#         # self.log(f'{stage}_acc'    , accuracy      , prog_bar=True , on_step=False , on_epoch=True)\n",
    "#         # self.log(f'{stage}_jaccard', jaccard_index , prog_bar=True , on_step=False , on_epoch=True)\n",
    "#         self.log(f'{stage}_fbeta'  , fbeta_score   , prog_bar=True , on_step=False , on_epoch=True)\n",
    "#         self.log(f'{stage}_IoU'    , IoU_score     , prog_bar=True , on_step=False , on_epoch=True)\n",
    "        \n",
    "#         return loss\n",
    "\n",
    "#     def shared_epoch_end(self, stage: Any):\n",
    "#         loss = torch.mean(torch.tensor([\n",
    "#             loss for loss in self.step_outputs[\"loss\"]\n",
    "#         ]))\n",
    "\n",
    "#         accuracy = torch.mean(torch.tensor([\n",
    "#             accuracy for accuracy in self.step_outputs[\"accuracy\"]\n",
    "#         ]))\n",
    "\n",
    "#         jaccard_index = torch.mean(torch.tensor([\n",
    "#             jaccard_index for jaccard_index in self.step_outputs[\"jaccard_index\"]\n",
    "#         ]))\n",
    "\n",
    "#         print(f'stage: {stage}')\n",
    "#         print(f'jaccard: {self.step_outputs[\"jaccard_index\"]}')\n",
    "#         print(f'Result: {jaccard_index}')\n",
    "\n",
    "\n",
    "#         fbeta_score = torch.mean(torch.tensor(\n",
    "#             [fbeta_score for fbeta_score in self.step_outputs[\"fbeta_score\"]\n",
    "#              ]))\n",
    "\n",
    "#         IoU_score = torch.mean(torch.tensor(\n",
    "#                 [IoU_score for IoU_score in self.step_outputs[\"IoU\"]\n",
    "#                  ]))\n",
    "\n",
    "#         for key in self.step_outputs.keys():\n",
    "#             self.step_outputs[key].clear()\n",
    "\n",
    "#         metrics = {\n",
    "#             f\"{stage}_loss\": loss,\n",
    "#             f\"{stage}_accuracy\": accuracy,\n",
    "#             f\"{stage}_jaccard_index\": jaccard_index,\n",
    "#             f\"{stage}_fbeta_score\": fbeta_score,\n",
    "#             f\"{stage}_IoU\": IoU_score\n",
    "#         }\n",
    "#         self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "#     def training_step(self, batch: Any, batch_idx: Any):\n",
    "#         return self.shared_step(batch=batch, stage=\"train\")\n",
    "\n",
    "#     def on_train_epoch_end(self) -> None:\n",
    "#         return self.shared_epoch_end(stage=\"train\")\n",
    "\n",
    "#     def validation_step(self, batch: Any, batch_idx: Any):\n",
    "#         return self.shared_step(batch=batch, stage=\"val\")\n",
    "\n",
    "#     def on_validation_epoch_end(self) -> None:\n",
    "#         return self.shared_epoch_end(stage=\"val\")\n",
    "\n",
    "#     def test_step(self, batch: Any, batch_idx: Any):\n",
    "#         return self.shared_step(batch=batch, stage=\"test\")\n",
    "\n",
    "#     def on_test_epoch_end(self) -> None:\n",
    "#         return self.shared_epoch_end(stage=\"test\")\n",
    "\n",
    "#     def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0):\n",
    "#         x, y = batch\n",
    "\n",
    "#         assert x.ndim == 4\n",
    "#         assert x.max() <= 3 and x.min() >= -3\n",
    "#         assert y.ndim == 4\n",
    "#         assert y.max() <= 1 and y.min() >= 0\n",
    "\n",
    "#         logits = self.forward(x.to(torch.float32))\n",
    "#         # predictions = logits\n",
    "#         predictions = torch.round(torch.sigmoid(logits))\n",
    "\n",
    "#         # activated = F.softmax(input=logits, dim=1)\n",
    "#         # predictions = torch.argmax(activated, dim=1)\n",
    "\n",
    "#         return predictions\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         optimizer = torch.optim.Adam(\n",
    "#             params=self.parameters(),\n",
    "#             lr=self.hparams.lr\n",
    "#         )\n",
    "\n",
    "#         scheduler_dict = {\n",
    "#             \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#                 optimizer=optimizer,\n",
    "#                 patience=5\n",
    "#             ),\n",
    "#             # \"scheduler\": pl_bolts.optim.lr_scheduler.LinearWarmupCOsineAnnealingLR(\n",
    "#             #     optimizer=optimizer,\n",
    "#             #     warmup_epochs=10,\n",
    "#             #     max_epochs=30,\n",
    "#             # ),\n",
    "#             \"interval\": \"epoch\",\n",
    "#             \"monitor\": \"val_loss\"\n",
    "#         }\n",
    "        \n",
    "#         optimization_dictionary = {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_dict}\n",
    "#         return self.optim_dict if self.optim_dict else optimization_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Combined_Focal_Dice_Loss(pl.LightningModule):\n",
    "    '''\n",
    "    Combined weighted loss between Focal Loss and Dice Loss  \n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 focal_loss_weight: float = 0.5,\n",
    "                 dice_weight: float = None,\n",
    "                 log_dice_loss: bool = False):\n",
    "        \n",
    "        super(Combined_Focal_Dice_Loss, self).__init__()\n",
    "        \n",
    "        self.focal_loss_weight = focal_loss_weight\n",
    "        self.dice_weight = (1 - focal_loss_weight) if dice_weight is None else dice_weight\n",
    "\n",
    "        if self.focal_loss_weight + self.dice_weight != 1:\n",
    "            warnings.warn(\"Sum of Focal and Dice loss weights is not 1.0: \"\n",
    "                          f\"{self.focal_loss_weight:.2f} + {self.dice_weight:.2f} = \"\n",
    "                          f\"{self.focal_loss_weight + self.dice_weight:.2f}\")\n",
    "\n",
    "        self.log_dice_loss = log_dice_loss\n",
    "\n",
    "\n",
    "    # def dice_score(y_pred, y_true, eps=1e-15, smooth=1.):\n",
    "    #     intersection = (y_pred * y_true).sum()\n",
    "    #     union = y_pred.sum() + y_true.sum()\n",
    "    #     return (2. * intersection + smooth) / (union + smooth + eps)\n",
    "\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        focal_loss_fn = FocalLoss(mode= 'binary')\n",
    "        focal_loss_fn.__name__ = 'focal_loss'\n",
    "        dice_loss_fn = DiceLoss(mode= 'binary',from_logits=True,log_loss=self.log_dice_loss) #Typically Dice use the masks and not logits, that is why from logits is used because y_pred are the logits\n",
    "        dice_loss_fn.__name__ = 'dice_loss'\n",
    "\n",
    "\n",
    "        focal_loss = focal_loss_fn(y_pred, y_true) \n",
    "        dice_loss = dice_loss_fn(y_pred, y_true) \n",
    "        \n",
    "        # y_pred = torch.sigmoid(y_pred)\n",
    "        # dice_loss = 1- dice_score(y_pred, y_true)\n",
    "        # log_dice_loss = -torch.log(dice_score(y_pred, y_true))\n",
    "        \n",
    "        loss = self.focal_loss_weight * focal_loss + self.dice_weight * dice_loss\n",
    "\n",
    "        return loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "class ThermalModel(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 model: nn.Module,\n",
    "                 loss_fn: Any,\n",
    "                 optim_dict: dict = None,\n",
    "                 lr: float = None,\n",
    "                 num_classes: int = 1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['model','loss_fn'])\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.model = model\n",
    "        # self.criterion = nn.CrossEntropyLoss()\n",
    "        self.criterion = loss_fn\n",
    "        self.optim_dict = optim_dict\n",
    "        self._device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "\n",
    "        # self.step_outputs = {\n",
    "        #     \"loss\": [],\n",
    "        #     \"accuracy\": [],\n",
    "        #     \"jaccard_index\": [],\n",
    "        #     \"fbeta_score\": [],\n",
    "        #     \"IoU\": []\n",
    "        # }\n",
    "\n",
    "        # self.stage_outputs = {\n",
    "        #     \"train\": self.step_outputs,\n",
    "        #     \"val\": self.step_outputs,\n",
    "        #     \"test\": self.step_outputs\n",
    "        # }\n",
    "\n",
    "        self.metrics = {\n",
    "            \"accuracy\": Accuracy(task=\"binary\",\n",
    "                                 threshold=0.5,\n",
    "                                 num_classes=num_classes,\n",
    "                                 validate_args=True,\n",
    "                                 ignore_index=None,\n",
    "                                 average=\"micro\").to(self._device),\n",
    "\n",
    "            \"jaccard_index\": JaccardIndex(task=\"binary\",\n",
    "                                          threshold=0.5,\n",
    "                                          num_classes=num_classes,\n",
    "                                          validate_args=True,\n",
    "                                          ignore_index=None,\n",
    "                                          average=\"macro\").to(self._device),\n",
    "\n",
    "            \"fbeta_score\": FBetaScore(task=\"binary\",\n",
    "                                      beta=1.0,\n",
    "                                      threshold=0.5,\n",
    "                                      num_classes=num_classes,\n",
    "                                      average=\"micro\",\n",
    "                                      ignore_index=None,\n",
    "                                      validate_args=True).to(self._device),\n",
    "\n",
    "            \"IoU\": metrics.IoU()\n",
    "        }\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def shared_step(self, batch, stage: str) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        x, y = x.to(self._device),y.to(self._device)\n",
    "\n",
    "        assert x.ndim == 4\n",
    "        assert x.max() <= 3 and x.min() >= -3 \n",
    "        assert y.ndim == 4\n",
    "        assert y.max() <= 1 and y.min() >= 0\n",
    "\n",
    "        logits = self.forward(x.to(torch.float32))\n",
    "        \n",
    "\n",
    "        # print(x.cpu().detach().numpy().max(),x.cpu().detach().numpy().min())\n",
    "\n",
    "        # activated = F.softmax(input=logits, dim=1)\n",
    "        # predictions = torch.argmax(activated, dim=1)\n",
    "        \n",
    "\n",
    "\n",
    "        # prob_mask = logits.sigmoid()\n",
    "        # prob_mask = nn.Sigmoid()(logits)\n",
    "        # predictions = (prob_mask > 0.5).float()\n",
    "\n",
    "\n",
    "\n",
    "        # testing  = (torch.softmax(logits, dim=0))\n",
    "        testing  = (torch.sigmoid(logits))\n",
    "        predictions = (testing > 0.5).float()\n",
    "\n",
    "\n",
    "\n",
    "        # print(testing.shape,np.unique(testing))\n",
    "\n",
    "\n",
    "    \n",
    "        # print(y.max(),y.min(),np.unique(y.detach().cpu().numpy()))\n",
    "\n",
    "        \n",
    "        # predictions = (logits > 0.5).float()\n",
    "        \n",
    "        # print(y.dtype,type(y))\n",
    "        # print(predictions.dtype,type(predictions))\n",
    "\n",
    "        # print(y.max(),y.min())\n",
    "        # print(predictions.max(),predictions.min())\n",
    "\n",
    "\n",
    "        # print('Y:\\n',y)\n",
    "        # print('Predictions:\\n',predictions)\n",
    "\n",
    "\n",
    "        # y_arr = y.cpu().detach().numpy().astype(np.float32)\n",
    "\n",
    "        # prediction_arr = predictions.cpu().detach().numpy().astype(np.float32)\n",
    "\n",
    "        # print(type(y_arr),y_arr.dtype)\n",
    "        # print(type(prediction_arr),prediction_arr.dtype)\n",
    "\n",
    "        # print(y_arr[0].shape,prediction_arr.shape)\n",
    "\n",
    "        # cv2.imshow('x',x.cpu().detach().numpy().astype(np.float32)[0][0])\n",
    "        # # cv2.imshow('y',y.cpu().detach().numpy().astype(np.float32)[0][0])\n",
    "        # # cv2.imshow('predictions',predictions.detach().cpu().numpy()[0][0])\n",
    "        # cv2.waitKey(0)\n",
    "        # cv2.destroyAllWindows()\n",
    "\n",
    "        \n",
    "\n",
    "        # predictions = torch.round(torch.sigmoid(logits))\n",
    "        # predictions = torch.round(logits)\n",
    "\n",
    "        # tp, fp, fn, tn  = smp.metrics.get_stats(predictions.long(),y.long(),mode = 'binary')\n",
    "\n",
    "        # print(predictions)\n",
    "        # print(tp,fp,fn,tn)\n",
    "        \n",
    "        loss = self.criterion(logits, y)\n",
    "        \n",
    "\n",
    "        # print(F.binary_cross_entropy(predictions,y)) #Works because this doesn't expect logits\n",
    "\n",
    "        # print(loss)\n",
    "\n",
    "        accuracy = self.metrics[\"accuracy\"](predictions, y)\n",
    "        jaccard_index = self.metrics[\"jaccard_index\"](predictions, y)\n",
    "        fbeta_score = self.metrics[\"fbeta_score\"](predictions, y)\n",
    "        IoU_score = self.metrics[\"IoU\"](predictions, y)\n",
    "\n",
    "        # print(f'stage: {stage}')\n",
    "        # print(f'Jaccard: {jaccard_index.dtype}')\n",
    "        # print(f'loss: {loss.dtype}')\n",
    "        \n",
    "        if torch.any(y!=0): # This avoids problems with the empty masks, that provides or 1 IoU or nan Jaccard Index \n",
    "\n",
    "            # self.stage_outputs[stage][\"loss\"].append(loss)\n",
    "            # self.stage_outputs[stage][\"accuracy\"].append(accuracy)\n",
    "            # self.stage_outputs[stage][\"jaccard_index\"].append(jaccard_index)\n",
    "            # self.stage_outputs[stage][\"fbeta_score\"].append(fbeta_score)\n",
    "            # self.stage_outputs[stage][\"IoU\"].append(IoU_score)\n",
    "\n",
    "\n",
    "            self.log(f'{stage}_loss'   , loss          , prog_bar=True , on_step=False , on_epoch=True)\n",
    "            self.log(f'{stage}_acc'    , accuracy      , prog_bar=True , on_step=False , on_epoch=True)\n",
    "            self.log(f'{stage}_jaccard', jaccard_index , prog_bar=True , on_step=False , on_epoch=True)\n",
    "            self.log(f'{stage}_fbeta'  , fbeta_score   , prog_bar=True , on_step=False , on_epoch=True)\n",
    "            self.log(f'{stage}_IoU'    , IoU_score     , prog_bar=True , on_step=False , on_epoch=True)\n",
    "\n",
    "            # self.log(f'{stage}_tp'     , tp            , prog_bar=True , on_step=False , on_epoch=True)\n",
    "            # self.log(f'{stage}_fp'     , fp            , prog_bar=True , on_step=False , on_epoch=True)\n",
    "            # self.log(f'{stage}_fn'     , fn            , prog_bar=True , on_step=False , on_epoch=True)\n",
    "            # self.log(f'{stage}_tn'     , tn            , prog_bar=True , on_step=False , on_epoch=True)\n",
    "\n",
    "        # else:\n",
    "            # print(stage,loss,jaccard_index,IoU_score,fbeta_score)\n",
    "            # for i in range(5):\n",
    "            #     plt.figure()\n",
    "            #     plt.subplot(1,2,1)\n",
    "            #     plt.imshow(predictions.detach().cpu().numpy().squeeze()[i,:,:])\n",
    "            #     plt.subplot(1,2,2)\n",
    "            #     plt.imshow(y.detach().cpu().numpy().squeeze()[i,:,:])\n",
    "            #     plt.show()\n",
    "\n",
    "\n",
    "        # self.log(f'{stage}_loss'   , loss          , prog_bar=True , on_step=False , on_epoch=True)\n",
    "        # self.log(f'{stage}_acc'    , accuracy      , prog_bar=True , on_step=False , on_epoch=True)\n",
    "        # self.log(f'{stage}_jaccard', jaccard_index , prog_bar=True , on_step=False , on_epoch=True)\n",
    "        # self.log(f'{stage}_fbeta'  , fbeta_score   , prog_bar=True , on_step=False , on_epoch=True)\n",
    "        # self.log(f'{stage}_IoU'    , IoU_score     , prog_bar=True , on_step=False , on_epoch=True)\n",
    "        \n",
    "        # self.optimizers().step()\n",
    "        # self.lr_schedulers().step()\n",
    "\n",
    "        \n",
    "        return loss\n",
    "\n",
    "        # return {\n",
    "        #     \"loss\": loss,\n",
    "        #     \"tp\": tp,\n",
    "        #     \"fp\": fp,\n",
    "        #     \"fn\": fn,\n",
    "        #     \"tn\": tn,\n",
    "        # }\n",
    "\n",
    "    def shared_epoch_end(self,outputs, stage: Any):\n",
    "\n",
    "        tp = torch.cat([x[\"tp\"] for x in outputs])\n",
    "        fp = torch.cat([x[\"fp\"] for x in outputs])\n",
    "        fn = torch.cat([x[\"fn\"] for x in outputs])\n",
    "        tn = torch.cat([x[\"tn\"] for x in outputs])\n",
    "        loss = torch.cat([x[\"loss\"] for x in outputs])\n",
    "\n",
    "        loss = torch.mean(torch.tensor([\n",
    "            loss for loss in self.stage_outputs[stage][\"loss\"]\n",
    "        ]))\n",
    "        \n",
    "        accuracy = torch.mean(torch.tensor([\n",
    "            accuracy for accuracy in self.stage_outputs[stage][\"accuracy\"]\n",
    "        ]))\n",
    "\n",
    "        jaccard_index = torch.mean(torch.tensor([\n",
    "            jaccard_index for jaccard_index in self.stage_outputs[stage][\"jaccard_index\"]\n",
    "        ]))\n",
    "\n",
    "        fbeta_score = torch.mean(torch.tensor(\n",
    "            [fbeta_score for fbeta_score in self.stage_outputs[stage][\"fbeta_score\"]\n",
    "             ]))\n",
    "\n",
    "        IoU_score = torch.mean(torch.tensor(\n",
    "                [IoU_score for IoU_score in self.stage_outputs[stage][\"IoU\"]\n",
    "                 ]))\n",
    "        # print(f'stage: {stage}')\n",
    "        # print(f'Result: {loss,jaccard_index,fbeta_score,IoU_score}')\n",
    "\n",
    "        metrics = {\n",
    "            f\"{stage}_loss\": loss,\n",
    "            f\"{stage}_accuracy\": accuracy,\n",
    "            f\"{stage}_jaccard_index\": jaccard_index,\n",
    "            f\"{stage}_fbeta_score\": fbeta_score,\n",
    "            f\"{stage}_IoU\": IoU_score\n",
    "        }\n",
    "        # self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "        for key in self.stage_outputs[stage].keys():\n",
    "            \n",
    "            print(stage,key)\n",
    "            \n",
    "            self.stage_outputs[stage][key].clear()\n",
    "\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: Any):\n",
    "        return self.shared_step(batch=batch, stage=\"train\")\n",
    "\n",
    "    # def on_train_epoch_end(self) -> None:\n",
    "    #     return self.shared_epoch_end(stage=\"train\")\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: Any):\n",
    "        return self.shared_step(batch=batch, stage=\"val\")\n",
    "\n",
    "    # def on_validation_epoch_end(self) -> None:\n",
    "    #     return self.shared_epoch_end(stage=\"val\")\n",
    "\n",
    "    def test_step(self, batch: Any, batch_idx: Any):\n",
    "        return self.shared_step(batch=batch, stage=\"test\")\n",
    "\n",
    "    # def on_test_epoch_end(self) -> None:\n",
    "    #     return self.shared_epoch_end(stage=\"test\")\n",
    "\n",
    "    def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0):\n",
    "        x, y = batch\n",
    "\n",
    "        assert x.ndim == 4\n",
    "        assert x.max() <= 3 and x.min() >= -3\n",
    "        assert y.ndim == 4\n",
    "        assert y.max() <= 1 and y.min() >= 0\n",
    "\n",
    "        logits = self.forward(x.to(torch.float32))\n",
    "        # predictions = torch.round(logits)\n",
    "        # predictions = torch.round(torch.sigmoid(logits))\n",
    "        \n",
    "        # prob_mask = logits.sigmoid()\n",
    "        # predictions = (prob_mask > 0.5).float()\n",
    "        predictions = (logits > 0.5).float()\n",
    "\n",
    "\n",
    "        # activated = F.softmax(input=logits, dim=1)\n",
    "        # predictions = torch.argmax(activated, dim=1)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            params=self.parameters(),\n",
    "            lr=self.hparams.lr\n",
    "        )\n",
    "\n",
    "        scheduler_dict = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer=optimizer,\n",
    "                patience=5\n",
    "            ),\n",
    "            \n",
    "            # \"scheduler\": pl_bolts.optimizers.lr_scheduler.LinearWarmupCosineAnnealingLR(\n",
    "            #     optimizer=optimizer,\n",
    "            #     warmup_epochs=2,\n",
    "            #     max_epochs=3,\n",
    "            #     eta_min = 0.001\n",
    "            # ),\n",
    "            # \"interval\": \"step\",\n",
    "            \n",
    "            \"interval\": \"epoch\",\n",
    "            \"monitor\": \"val_loss\"\n",
    "        }\n",
    "        \n",
    "        optimization_dictionary = {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_dict}\n",
    "        return self.optim_dict if self.optim_dict else optimization_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(callbacks: list,\n",
    "         model: Union[list, tuple],\n",
    "         loss_fn: Any,\n",
    "         augmentation: Any,\n",
    "         preprocessing: Any,\n",
    "         logger: Any,\n",
    "         images_path: str,\n",
    "         optim_dict: dict,\n",
    "         min_epochs: int,\n",
    "         max_epochs: int,\n",
    "         precision: int\n",
    "         ) -> None:\n",
    "\n",
    "    # Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        fast_dev_run=False,\n",
    "        accelerator=\"auto\",\n",
    "        strategy=\"auto\",\n",
    "        devices=\"auto\",\n",
    "        num_nodes=1,\n",
    "        logger=logger,\n",
    "        callbacks=callbacks,\n",
    "        max_epochs=max_epochs,\n",
    "        min_epochs=min_epochs,\n",
    "        precision=precision # Mixed precision training\n",
    "    )\n",
    "\n",
    "    # Datamodule\n",
    "    datamodule = ThermalDataModule(\n",
    "        images_path=images_path,\n",
    "        augmentation=augmentation,\n",
    "        preprocessing=preprocessing,\n",
    "        batch_size=5,\n",
    "        num_workers=os.cpu_count()\n",
    "    )\n",
    "\n",
    "    # LightningModule\n",
    "    lightning_model = ThermalModel(\n",
    "        model=model,\n",
    "        loss_fn=loss_fn,\n",
    "        optim_dict=optim_dict,\n",
    "        lr=3e-4\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.fit(model=lightning_model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cristopher/Documents/SegTHRawS_training/model_training/models/Unet_mobilenet_v2/run_0\n"
     ]
    }
   ],
   "source": [
    "# Run Constants\n",
    "SEED: int = 42\n",
    "ACTION: str = \"ignore\"\n",
    "DATA_PATH: str = os.path.join(os.getcwd(),'train_dataset')\n",
    "CHECKPOINT: Any = None\n",
    "    \n",
    "# Model Constants\n",
    "CLASSES = 1\n",
    "IN_CHANNELS = 3\n",
    "\n",
    "optim_dict = None\n",
    "\n",
    "# ENCODER = 'se_resnext50_32x4d'\n",
    "ENCODER = 'mobilenet_v2'\n",
    "# ENCODER = 'resnet18'\n",
    "# ENCODER = 'timm-mobilenetv3_large_100'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "    \n",
    "\n",
    "\n",
    "ACTIVATION = None\n",
    "# ACTIVATION = 'sigmoid' # could be None for logits. If used, the sigmoid after the forward function needs to be removed\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "min_epochs = 150\n",
    "max_epochs = 200\n",
    "\n",
    "n_cpu = os.cpu_count()\n",
    "\n",
    "model_name = 'Unet'\n",
    "# model_name = 'DeepLabV3Plus'\n",
    "\n",
    "model_name_path = os.path.join(os.getcwd(),'models',f'{model_name}_{ENCODER}')\n",
    "os.makedirs(model_name_path,exist_ok=True)  \n",
    "run_idx =sum(1 for file in os.listdir(model_name_path) if file.startswith('run'))\n",
    "\n",
    "model_main_path = os.path.join(model_name_path,f'run_{run_idx}')\n",
    "os.makedirs(model_main_path,exist_ok=True)\n",
    "\n",
    "print(model_main_path)\n",
    "\n",
    "# model_main_path = os.path.join(os.getcwd(),'models',f'{model_name}_{ENCODER}_{run_idx}')\n",
    "metrics_path = os.path.join(model_main_path,'metrics')\n",
    "os.makedirs(metrics_path,exist_ok=True)\n",
    "\n",
    "# model = smp.DeepLabV3Plus(\n",
    "#     encoder_name=ENCODER, \n",
    "#     encoder_weights=ENCODER_WEIGHTS, \n",
    "#     classes=1, \n",
    "#     activation=ACTIVATION,\n",
    "# )\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    in_channels = 3,\n",
    "    classes=CLASSES, \n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "\n",
    "precision = '16-mixed' # 32 # 32 is the original, and 16 is mixed precission\n",
    "# precision = 32\n",
    "\n",
    "loss = FocalLoss(mode= 'binary')\n",
    "loss.__name__ = 'focal_loss'\n",
    "\n",
    "# # loss = DiceLoss(mode= 'binary')\n",
    "# # loss.__name__ = 'dice_loss'\n",
    "\n",
    "# loss = JaccardLoss(mode= 'binary')\n",
    "# loss.__name__ = 'jaccard_loss'\n",
    "\n",
    "# # loss = losses.DiceLoss()\n",
    "# # loss = losses.JaccardLoss()\n",
    "\n",
    "# metrics = [\n",
    "#     metrics.IoU(),\n",
    "# ]\n",
    "\n",
    "# optimizer = torch.optim.Adam([ \n",
    "#     dict(params=model.parameters(), lr=1e-3),\n",
    "# ])\n",
    "\n",
    "augmentation=get_training_augmentation()\n",
    "preprocessing=get_preprocessing(preprocessing_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        dirpath=model_main_path,\n",
    "        filename=f\"{model_name}_{ENCODER}_\"+\"{epoch}\",\n",
    "        save_top_k=10,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\"\n",
    "    ),\n",
    "\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=2e-4,\n",
    "        patience=8,\n",
    "        verbose=False,\n",
    "        mode=\"min\"\n",
    "    ),\n",
    "\n",
    "    LearningRateMonitor(\n",
    "        logging_interval=\"step\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lightning.pytorch.loggers import TensorBoardLogger\n",
    "# logger = TensorBoardLogger(save_dir=\"./logs\", name=model_name)\n",
    "\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "logger = CSVLogger(f\"{model_main_path}/csv_logs\", name=f\"{model_name}_{ENCODER}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit None Automatic Mixed Precision (AMP)\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type      | Params\n",
      "----------------------------------------\n",
      "0 | model     | Unet      | 6.6 M \n",
      "1 | criterion | FocalLoss | 0     \n",
      "----------------------------------------\n",
      "6.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.6 M     Total params\n",
      "13.258    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  54%|█████▍    | 140/258 [00:16<00:13,  8.74it/s, loss=0.46, v_num=0, val_loss=0.498, val_acc=0.999, val_jaccard=0.198, val_fbeta=0.317, val_IoU=0.198, train_loss=0.507, train_acc=0.982, train_jaccard=0.182, train_fbeta=0.265, train_IoU=0.182] \n",
      "Epoch 2:  49%|████▉     | 127/258 [00:04<00:04, 30.77it/s, loss=0.000196, v_num=0, val_loss=0.000114, val_acc=1.000, val_jaccard=0.469, val_fbeta=0.602, val_IoU=0.469, train_loss=0.000238, train_acc=1.000, train_jaccard=0.521, train_fbeta=0.643, train_IoU=0.521]"
     ]
    }
   ],
   "source": [
    "main(\n",
    "    callbacks=callbacks,\n",
    "    model=model,\n",
    "    loss_fn= loss, #Combined_Focal_Dice_Loss(),\n",
    "    augmentation=augmentation,\n",
    "    preprocessing=preprocessing,\n",
    "    logger=logger,\n",
    "    images_path=DATA_PATH,\n",
    "    optim_dict=optim_dict,\n",
    "    min_epochs=10, #min_epochs,\n",
    "    max_epochs=10, #max_epochs\n",
    "    precision=16 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir models/current_best_model/version_33\n",
    "\n",
    "# %tensorboard --logdir logs/DeepLabV3Plus/version_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# checkpoints_paths = [os.path.join(model_main_path,checkpoint_path) for checkpoint_path in os.listdir(model_main_path) if checkpoint_path[-5:]=='.ckpt']\n",
    "# checkpoint_path = max(checkpoints_paths, key=lambda x: int(re.search(r'epoch=(\\d+)', x).group(1)))\n",
    "\n",
    "# for checkpoint in checkpoints_paths:\n",
    "#     if checkpoint != checkpoint_path:\n",
    "#         os.remove(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Perform the testing, c  NEED TO create a function\n",
    "# trained_model = ThermalModel.load_from_checkpoint(checkpoint_path=checkpoint_path,model=model,loss_fn=loss)\n",
    "# trained_model.eval();\n",
    "\n",
    "# trainer = pl.Trainer(\n",
    "#     fast_dev_run=False,\n",
    "#     accelerator=\"auto\",\n",
    "#     strategy=\"auto\",\n",
    "#     devices=\"auto\",\n",
    "#     num_nodes=1,\n",
    "#     logger=logger,\n",
    "#     callbacks=callbacks,\n",
    "#     max_epochs=1,\n",
    "#     min_epochs=1,\n",
    "#     precision=precision #Mixed precision training\n",
    "# )\n",
    "\n",
    "# # Datamodule\n",
    "# datamodule = ThermalDataModule(\n",
    "#     images_path=DATA_PATH,\n",
    "#     augmentation=augmentation,\n",
    "#     preprocessing=preprocessing,\n",
    "#     batch_size=5,\n",
    "#     num_workers=os.cpu_count()\n",
    "# )\n",
    "\n",
    "# loss_2 = FocalLoss(mode= 'binary')\n",
    "# loss_2.__name__ = 'focal_loss'\n",
    "\n",
    "# # LightningModule\n",
    "# lightning_model = ThermalModel(\n",
    "#     model=model,\n",
    "#     loss_fn=loss_2,\n",
    "#     optim_dict=optim_dict,\n",
    "#     lr=3e-4\n",
    "# )\n",
    "\n",
    "# test_metrics = trainer.test(model=trained_model,datamodule=datamodule)[0]\n",
    "# # trainer.predict(model=trained_model,datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(trained_model,os.path.join(model_main_path,'trained_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# %matplotlib inline\n",
    "# # %matplotlib widget\n",
    "# import os\n",
    "\n",
    "# from matplotlib.ticker import AutoMinorLocator, MultipleLocator\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# model_path = os.path.join(os.getcwd(),'models','current_best_model')\n",
    "\n",
    "# metric_id_list = []\n",
    "\n",
    "# for file_name in os.listdir(model_path):\n",
    "#     if file_name[-4:]=='.csv':\n",
    "#         fig, ax = plt.subplots(figsize=(7,5))\n",
    "        \n",
    "#         csv_file = pd.read_csv(os.path.join(model_path,file_name))\n",
    "#         metric_name = file_name.replace('_evolution.csv','')\n",
    "        \n",
    "\n",
    "#         ax.yaxis.set_minor_locator(AutoMinorLocator(4))\n",
    "\n",
    "#         if metric_name[:5]=='train':\n",
    "#             ax.set_xlabel('# epochs')\n",
    "\n",
    "#             metric_id = metric_name.replace('train_','')    \n",
    "#             if metric_id not in metric_id_list:\n",
    "#                 for inner_file_name in os.listdir(model_path):\n",
    "#                     if inner_file_name[-4:]=='.csv' and inner_file_name.replace('_evolution.csv','')==f'val_{metric_id}':\n",
    "#                         csv_file_inner = pd.read_csv(os.path.join(model_path,inner_file_name))\n",
    "                        \n",
    "#                         csv_values = csv_file['Value']\n",
    "#                         csv_inner_values = csv_file_inner['Value']\n",
    "                        \n",
    "#                         ax.plot(csv_values,color='r',label='Training')\n",
    "#                         ax.plot(csv_inner_values,color='b',label='Validation')\n",
    "#                         ax.set_title(f'{metric_id} evolution')\n",
    "#                         ax.set_ylabel(f'{metric_id}')\n",
    "#                         plt.annotate('%0.2f' % csv_values[len(csv_values)-1], xy=(len(csv_values)-1, csv_values[len(csv_values)-1]))\n",
    "#                         plt.annotate('%0.2f' % csv_inner_values[len(csv_inner_values)-1], xy=(len(csv_inner_values)-1, csv_inner_values[len(csv_inner_values)-1]))\n",
    "#                         metric_id_list.append(metric_id)\n",
    "#                         break\n",
    "#             else:\n",
    "#                 metric_id = None\n",
    "#                 plt.close(fig)\n",
    "#         elif metric_name[:3]=='val':\n",
    "#             ax.set_xlabel('# epochs')\n",
    "            \n",
    "#             metric_id = metric_name.replace('val_','')\n",
    "#             if metric_id not in metric_id_list:\n",
    "#                 for inner_file_name in os.listdir(model_path):\n",
    "#                     if inner_file_name[-4:]=='.csv' and inner_file_name.replace('_evolution.csv','')==f'train_{metric_id}':\n",
    "#                         csv_file_inner = pd.read_csv(os.path.join(model_path,inner_file_name))\n",
    "                        \n",
    "#                         csv_values = csv_file['Value']\n",
    "#                         csv_inner_values = csv_file_inner['Value']\n",
    "\n",
    "#                         ax.plot(csv_values,color='b',label='Validation')\n",
    "#                         ax.plot(csv_inner_values,color='r',label='Training')\n",
    "#                         ax.set_title(f'{metric_id} evolution')\n",
    "#                         ax.set_ylabel(f'{metric_id}')\n",
    "\n",
    "#                         plt.annotate('%0.2f' % csv_values[len(csv_values)-1], xy=(len(csv_values)-1, csv_values[len(csv_values)-1]))\n",
    "#                         plt.annotate('%0.2f' % csv_inner_values[len(csv_inner_values)-1], xy=(len(csv_inner_values)-1, csv_inner_values[len(csv_inner_values)-1]))\n",
    "                        \n",
    "#                         metric_id_list.append(metric_id)\n",
    "#             else:\n",
    "#                 metric_id = None\n",
    "#                 plt.close(fig)\n",
    "\n",
    "#         else:\n",
    "#             csv_values = csv_file['Value']\n",
    "#             ax.set_title(f'{metric_name} evolution')\n",
    "#             ax.set_ylabel(f'{metric_name}')\n",
    "#             ax.set_xlabel('Time')\n",
    "#             ax.plot(csv_values,color='g',label='learning rate')\n",
    "#             plt.annotate('%.0E' % csv_values[0], xy=(0, csv_values[0]))\n",
    "#             plt.annotate('%.0E' % csv_values[len(csv_values)-1], xy=(len(csv_values)-1, csv_values[len(csv_values)-1]))\n",
    "#             metric_id = 'lr'\n",
    "#         # ax.plot(csv_file['Value'])\n",
    "#         if metric_id:\n",
    "#             ax.legend()\n",
    "#             plt.savefig(os.path.join(model_path,metric_id+'_evolution.png'))\n",
    "#         # break\n",
    "#         # break\n",
    "#         # plt.show()\n",
    "#         # break\n",
    "#         # plt.savefig(os.path.join(model_path,file_name.replace('.csv','.png')))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.randn(1, 3, 256, 256).cpu()\n",
    "# # model_onnx = trained_model.cpu()\n",
    "# # model_onnx.eval()\n",
    "\n",
    "# checkpoint_path = '/home/cristopher/Documents/SegTHRawS_training/model_training/models/Unet_mobilenet_v2_metrics_without_empty_masks/Unet_mobilenet_v2_epoch=149.ckpt'\n",
    "# trained_model = ThermalModel.load_from_checkpoint(checkpoint_path=checkpoint_path,model=model,loss_fn=loss)\n",
    "\n",
    "# onnx_model_path = checkpoint_path.replace('.ckpt','.onnx')\n",
    "# model_onnx = trained_model.cpu()\n",
    "# model_onnx.eval()\n",
    "\n",
    "# torch_out = model_onnx(x)\n",
    "# import warnings\n",
    "# warnings.filterwarnings(category=FutureWarning,action='ignore')\n",
    "# warnings.filterwarnings(category=torch.jit.TracerWarning,action='ignore')\n",
    "\n",
    "# # Export the model\n",
    "# # torch.onnx.export(model_onnx,                                   # model being run\n",
    "# #                   x,                                            # model input (or a tuple for multiple inputs)\n",
    "# #                   onnx_model_path,                              # where to save the model (can be a file or file-like object)\n",
    "# #                   export_params=True,                           # store the trained parameter weights inside the model file\n",
    "# #                   opset_version=16,                             # the ONNX version to export the model to\n",
    "# #                   do_constant_folding=True,                     # whether to execute constant folding for optimization\n",
    "# #                   input_names = ['input'],                      # the model's input names\n",
    "# #                   output_names = ['output'],                    # the model's output names\n",
    "# #                   dynamic_axes={'input' : {0 : 'batch_size'},   # variable length axes\n",
    "# #                                 'output' : {0 : 'batch_size'}})\n",
    "\n",
    "# torch.onnx.export(model_onnx,                                   # model being run\n",
    "#                   x,                                            # model input (or a tuple for multiple inputs)\n",
    "#                   onnx_model_path,                              # where to save the model (can be a file or file-like object)\n",
    "#                   export_params=True,                           # store the trained parameter weights inside the model file\n",
    "#                   opset_version=16,                             # the ONNX version to export the model to\n",
    "#                   do_constant_folding=True,                     # whether to execute constant folding for optimization\n",
    "#                   input_names = ['input'],                      # the model's input names\n",
    "#                   output_names = ['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import configparser\n",
    "\n",
    "# model_info = configparser.ConfigParser()\n",
    "# model_info.read(os.path.join(onnx_model_path,'info.ini'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# onnx_model = onnx.load(onnx_model_path)\n",
    "# onnx.checker.check_model(onnx_model)\n",
    "\n",
    "\n",
    "# ort_session = onnxruntime.InferenceSession(onnx_model_path, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "# def to_numpy(tensor):\n",
    "#     return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "# # compute ONNX Runtime output prediction\n",
    "# ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n",
    "# ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# # compare ONNX Runtime and PyTorch results\n",
    "# np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n",
    "\n",
    "# print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_version_folder_path = os.path.join(model_main_path,'csv_logs',f'{model_name}_{ENCODER}')\n",
    "\n",
    "\n",
    "# version_files = os.listdir(metrics_version_folder_path)\n",
    "\n",
    "# version_files.sort(key=lambda x: os.path.getmtime(os.path.join(metrics_version_folder_path,x)),reverse=True)\n",
    "\n",
    "# metrics_df = pd.read_csv(os.path.join(metrics_version_folder_path,version_files[0],'metrics.csv'))\n",
    "# metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = metrics_df.copy()\n",
    "\n",
    "# selected_rows = df.dropna(subset=['epoch'],axis=0)\n",
    "# input_epoch = df['epoch'].max()\n",
    "\n",
    "\n",
    "# #Move the test values from epoch 0 to the epoch of the model \n",
    "# selected_rows.loc[df['test_IoU'].notna(),'epoch'] = input_epoch\n",
    "\n",
    "# combined_df = pd.DataFrame(columns=selected_rows.columns)\n",
    "\n",
    "# for _, group in selected_rows.groupby('epoch'):\n",
    "#     if len(group)==2:\n",
    "\n",
    "#         row1 = group.iloc[0]\n",
    "#         row2 = group.iloc[1]\n",
    "\n",
    "#         # print('Row 1: ',row1)\n",
    "#         # print('Row 2: ',row2)\n",
    "        \n",
    "#         combined_row = pd.DataFrame({\n",
    "#             col: [row1[col] if pd.notna(row1[col]) else row2[col]] for col in df.columns\n",
    "#         })\n",
    "\n",
    "#         combined_df = pd.concat([combined_df,combined_row],ignore_index=True)\n",
    "#     elif len(group)==3:\n",
    "#         row1 = group.iloc[0]\n",
    "#         row2 = group.iloc[1]\n",
    "#         row3 = group.iloc[2]\n",
    "\n",
    "#         # print('Row 1: ',row1)\n",
    "#         # print('Row 2: ',row2)\n",
    "        \n",
    "#         combined_row = pd.DataFrame({\n",
    "#             col: [row1[col] if pd.notna(row1[col]) else row2[col] if pd.notna(row2[col]) else row3[col]] for col in df.columns\n",
    "#         })\n",
    "\n",
    "#         combined_df = pd.concat([combined_df,combined_row],ignore_index=True)\n",
    "\n",
    "# csv_name = os.path.basename(checkpoint_path).replace('ckpt','csv')\n",
    "\n",
    "# metrics_csv_path = os.path.join(metrics_path,f'metrics_{csv_name}   ')\n",
    "\n",
    "# plots_path = os.path.join(os.path.dirname(metrics_csv_path),'plots')\n",
    "# os.makedirs(plots_path,exist_ok=True)\n",
    "\n",
    "# combined_df.to_csv(metrics_csv_path,index=False)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# ax.plot(metrics_df['lr-Adam'].dropna(),color='g')\n",
    "# ax.set_xlabel('# epochs')\n",
    "# ax.set_ylabel(f'Lr evolution')\n",
    "# ax.set_title(f'Lr evolution',fontname=\"Charter\",weight='bold')\n",
    "\n",
    "# plt.yscale('log')\n",
    "# plt.savefig(os.path.join(plots_path,'Lr_evolution.png'))\n",
    "\n",
    "# # ax.plot(train_values,color='b',label=f'Validation {train_values[len(train_values)-1]:.3E}')\n",
    "# plt.gca().yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter(\"{x:.3e}\"))\n",
    "\n",
    "\n",
    "\n",
    "# shutil.rmtree(os.path.join(metrics_version_folder_path,version_files[0])) #Delete the old metrics file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_df_test = pd.read_csv('/home/cristopher/Documents/SegTHRawS training/model_training/models/Unet_mobilenet_v2_metrics_without_empty_masks/metrics.csv')\n",
    "# # plt.plot(metrics_df_test['lr-Adam'].dropna())\n",
    "# # plt.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "\n",
    "# # # ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "# # # plt.yscale('log')\n",
    "\n",
    "# # plt.gca().yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter(\"{x:.3e}\"))\n",
    "# # plt.set_xlim(0,100)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# ax.plot(metrics_df_test['lr-Adam'].dropna())\n",
    "# ax.set_xlabel('# epochs')\n",
    "# ax.set_ylabel(f'Lr')\n",
    "\n",
    "# plt.yscale('log')\n",
    "\n",
    "# # ax.plot(train_values,color='b',label=f'Validation {train_values[len(train_values)-1]:.3E}')\n",
    "# # ax.set_xlabel('# epochs')\n",
    "# # ax.set_ylabel(f'{metric_id[0].upper()+metric_id[1:]}')\n",
    "\n",
    "# # ax.set_title(f'{metric_id[0].upper()+metric_id[1:]} evolution',fontname=\"Charter\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# metric_id_list = []\n",
    "\n",
    "\n",
    "\n",
    "# csv_file = pd.read_csv(os.path.join(metrics_csv_path))\n",
    "\n",
    "# columns_to_drop = ['lr-Adam','step','test_IoU','test_acc','test_fbeta','test_jaccard','test_loss','train_acc','val_acc']\n",
    "\n",
    "# new_test_df = csv_file.drop(columns=columns_to_drop)\n",
    "\n",
    "\n",
    "\n",
    "# for metric_name in new_test_df.columns[1:]:\n",
    "#     fig, ax = plt.subplots(figsize=(7,5))\n",
    "#     if metric_name[:5]=='train':\n",
    "#         ax.set_xlabel('# epochs')\n",
    "\n",
    "#         metric_id = metric_name.replace('train_','')    \n",
    "#         if metric_id not in metric_id_list:\n",
    "#             val_metric_name = metric_name.replace('train','val')\n",
    "#             train_values = new_test_df[metric_name].astype('float32')\n",
    "#             val_values = new_test_df[val_metric_name].astype('float32')\n",
    "#             ax.plot(train_values,color='b',label=f'Validation {train_values[len(train_values)-1]:.3E}')\n",
    "#             ax.plot(val_values,color='r',label=  f'Training    {val_values[len(val_values)-1]:.3E}')\n",
    "#             if metric_id == 'fbeta':\n",
    "#                 ax.set_title('F-1 evolution',fontname=\"Charter\",weight='bold')\n",
    "#             else:\n",
    "#                 ax.set_title(f'{metric_id[0].upper()+metric_id[1:]} evolution',fontname=\"Charter\",weight='bold')\n",
    "            \n",
    "#             ax.set_ylabel(f'{metric_id[0].upper()+metric_id[1:]}')\n",
    "            \n",
    "#             # plt.annotate('%0.2f' % train_values[len(train_values)-1], xy=(len(train_values)-1, train_values[len(train_values)-1]))\n",
    "#             # plt.annotate('%0.2E' % train_values[len(train_values)-1], xy=(new_test_df['epoch'][len(new_test_df['epoch'])-1], train_values[len(train_values)-1]),xytext=(-40,5),textcoords='offset pixels')\n",
    "\n",
    "#             # plt.annotate('%0.2E' % val_values[len(val_values)-1], xy=(new_test_df['epoch'][len(new_test_df['epoch'])-1], val_values[len(val_values)-1]),xytext=(-40,5),textcoords='offset pixels')\n",
    "#             metric_id_list.append(metric_id)\n",
    "#         else:\n",
    "#             metric_id = None\n",
    "#             plt.close(fig)\n",
    "#     elif metric_name[:3]=='val':\n",
    "#         ax.set_xlabel('# epochs')\n",
    "        \n",
    "#         metric_id = metric_name.replace('val_','')\n",
    "#         if metric_id not in metric_id_list:\n",
    "\n",
    "#             train_metric_name = metric_name.replace('val','train')\n",
    "#             train_values = new_test_df[metric_name].astype('float32')\n",
    "#             val_values = new_test_df[train_metric_name].astype('float32')\n",
    "\n",
    "#             ax.plot(train_values,color='b',label=f'Validation {train_values[len(train_values)-1]:.3E}',fontname=\"Charter\")\n",
    "#             ax.plot(val_values,color='r',label=f'Training {val_values[len(val_values)-1]:.3E}',fontname=\"Charter\")\n",
    "#             if metric_id == 'fbeta':\n",
    "#                 ax.set_title('F-1 evolution',fontname=\"Charter\",weight='bold')\n",
    "#             else:\n",
    "#                 ax.set_title(f'{metric_id[0].upper()+metric_id[1:]} evolution',fontname=\"Charter\",weight='bold')\n",
    "            \n",
    "#             ax.set_ylabel(f'{metric_id[0].upper()+metric_id[1:]}')\n",
    "\n",
    "#             # plt.annotate('%0.2E' % train_values[len(train_values)-1], xy=(new_test_df['epoch'][len(new_test_df['epoch'])-1], train_values[len(train_values)-1]),xytext=(-40,5),textcoords='offset pixels')\n",
    "\n",
    "#             # plt.annotate('%0.2E' % val_values[len(val_values)-1], xy=(new_test_df['epoch'][len(new_test_df['epoch'])-1], val_values[len(val_values)-1]),xytext=(-40,5),textcoords='offset pixels')\n",
    "            \n",
    "#             metric_id_list.append(metric_id)\n",
    "#         else:\n",
    "#             metric_id = None\n",
    "#             plt.close(fig)\n",
    "#     else:\n",
    "#         print('ERROR')\n",
    "#         plt.close(fig)\n",
    "\n",
    "\n",
    "#     if metric_id:\n",
    "#         ax.legend()\n",
    "#         plt.savefig(os.path.join(plots_path,metric_id+'_evolution.png'))\n",
    "\n",
    "#         plt.show()\n",
    "#     # plt.savefig(os.path.join(model_path,file_name.replace('.csv','.png')))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = combined_df.copy()\n",
    "\n",
    "# columns_to_drop = ['lr-Adam','step','test_IoU','test_acc','test_fbeta','test_jaccard','test_loss','train_acc','val_acc']\n",
    "\n",
    "# new_test_df = test_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# for column in new_test_df.columns[1:]:\n",
    "#     fig, ax = plt.subplots()\n",
    "#     ax.set_xlabel('# epochs')\n",
    "#     # ax.set_title(column)\n",
    "#     ax.set_title(f'{column} evolution')\n",
    "#     ax.set_ylabel(column)\n",
    "#     ax.plot(new_test_df['epoch'],new_test_df[column],color='k')\n",
    "\n",
    "\n",
    "#     if column[-4:]=='loss':\n",
    "#         plt.annotate('%0.2E' % new_test_df[column][len(new_test_df[column])-1], xy=(new_test_df['epoch'][len(new_test_df['epoch'])-1], new_test_df[column][len(new_test_df[column])-1]),xytext=(-50,10),textcoords='offset pixels')\n",
    "#     else:\n",
    "#         ax.set_ylim(0,1)\n",
    "#         plt.annotate('%0.3E' % new_test_df[column][len(new_test_df[column])-1], xy=(new_test_df['epoch'][len(new_test_df['epoch'])-1], new_test_df[column][len(new_test_df[column])-1]),xytext=(-40,6),textcoords='offset pixels')\n",
    "\n",
    "\n",
    "# # ax.yaxis.set_minor_locator(AutoMinorLocator(4))\n",
    "# # print(new_test_df[column][len(new_test_df[column])-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# %matplotlib inline\n",
    "# # %matplotlib widget\n",
    "# import os\n",
    "\n",
    "# import matplotlib as mpl\n",
    "# mpl.rc('font',family='Charter')\n",
    "\n",
    "# from matplotlib.ticker import AutoMinorLocator, MultipleLocator\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # model_path = os.path.join(os.getcwd(),'models','current_best_model')\n",
    "\n",
    "# metric_id_list = []\n",
    "\n",
    "# model_name = 'Unet'\n",
    "\n",
    "# ENCODER = 'mobilenet_v2'\n",
    "\n",
    "# file_name = os.path.join(os.getcwd(),'models',f'{model_name}_{ENCODER}_metrics_without_empty_masks',f'metrics_{model_name}_{ENCODER}.csv')\n",
    "# plots_path = os.path.join(os.path.dirname(file_name),'plots')\n",
    "# os.makedirs(plots_path,exist_ok=True)\n",
    "\n",
    "\n",
    "# csv_file = pd.read_csv(os.path.join(file_name))\n",
    "\n",
    "# columns_to_drop = ['lr-Adam','step','test_IoU','test_acc','test_fbeta','test_jaccard','test_loss','train_acc','val_acc']\n",
    "\n",
    "# new_test_df = csv_file.drop(columns=columns_to_drop)\n",
    "\n",
    "# # ax.yaxis.set_minor_locator(AutoMinorLocator(4))\n",
    "\n",
    "# for metric_name in new_test_df.columns[1:]:\n",
    "#     fig, ax = plt.subplots(figsize=(7,5))\n",
    "#     if metric_name[:5]=='train':\n",
    "#         ax.set_xlabel('# epochs')\n",
    "\n",
    "#         metric_id = metric_name.replace('train_','')    \n",
    "#         if metric_id not in metric_id_list:\n",
    "#             val_metric_name = metric_name.replace('train','val')\n",
    "#             train_values = new_test_df[metric_name].astype('float32')\n",
    "#             val_values = new_test_df[val_metric_name].astype('float32')\n",
    "#             ax.plot(train_values,color='b',label=f'Validation {train_values[len(train_values)-1]:.3E}')\n",
    "#             ax.plot(val_values,color='r',label=  f'Training    {val_values[len(val_values)-1]:.3E}')\n",
    "#             if metric_id == 'fbeta':\n",
    "#                 ax.set_title('F-1 evolution',fontname=\"Charter\")\n",
    "#             else:\n",
    "#                 ax.set_title(f'{metric_id[0].upper()+metric_id[1:]} evolution',fontname=\"Charter\")\n",
    "            \n",
    "#             ax.set_ylabel(f'{metric_id[0].upper()+metric_id[1:]}')\n",
    "            \n",
    "#             # plt.annotate('%0.2f' % train_values[len(train_values)-1], xy=(len(train_values)-1, train_values[len(train_values)-1]))\n",
    "#             # plt.annotate('%0.2E' % train_values[len(train_values)-1], xy=(new_test_df['epoch'][len(new_test_df['epoch'])-1], train_values[len(train_values)-1]),xytext=(-40,5),textcoords='offset pixels')\n",
    "\n",
    "#             # plt.annotate('%0.2E' % val_values[len(val_values)-1], xy=(new_test_df['epoch'][len(new_test_df['epoch'])-1], val_values[len(val_values)-1]),xytext=(-40,5),textcoords='offset pixels')\n",
    "#             metric_id_list.append(metric_id)\n",
    "#         else:\n",
    "#             metric_id = None\n",
    "#             plt.close(fig)\n",
    "#     elif metric_name[:3]=='val':\n",
    "#         ax.set_xlabel('# epochs')\n",
    "        \n",
    "#         metric_id = metric_name.replace('val_','')\n",
    "#         if metric_id not in metric_id_list:\n",
    "\n",
    "#             train_metric_name = metric_name.replace('val','train')\n",
    "#             train_values = new_test_df[metric_name].astype('float32')\n",
    "#             val_values = new_test_df[train_metric_name].astype('float32')\n",
    "\n",
    "#             ax.plot(train_values,color='b',label=f'Validation {train_values[len(train_values)-1]:.3E}',fontname=\"Charter\")\n",
    "#             ax.plot(val_values,color='r',label=f'Training {val_values[len(val_values)-1]:.3E}',fontname=\"Charter\")\n",
    "#             ax.set_title(f'{metric_id[0].upper()+metric_id[1:]} evolution',fontname=\"Charter\")\n",
    "#             ax.set_ylabel(f'{metric_id[0].upper()+metric_id[1:]}')\n",
    "\n",
    "#             # plt.annotate('%0.2E' % train_values[len(train_values)-1], xy=(new_test_df['epoch'][len(new_test_df['epoch'])-1], train_values[len(train_values)-1]),xytext=(-40,5),textcoords='offset pixels')\n",
    "\n",
    "#             # plt.annotate('%0.2E' % val_values[len(val_values)-1], xy=(new_test_df['epoch'][len(new_test_df['epoch'])-1], val_values[len(val_values)-1]),xytext=(-40,5),textcoords='offset pixels')\n",
    "            \n",
    "#             metric_id_list.append(metric_id)\n",
    "#         else:\n",
    "#             metric_id = None\n",
    "#             plt.close(fig)\n",
    "#     else:\n",
    "#         print('ERROR')\n",
    "#         plt.close(fig)\n",
    "\n",
    "\n",
    "#     if metric_id:\n",
    "#         ax.legend()\n",
    "#         plt.savefig(os.path.join(plots_path,metric_id+'_evolution.png'))\n",
    "\n",
    "#         plt.show()\n",
    "#     # plt.savefig(os.path.join(model_path,file_name.replace('.csv','.png')))\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
