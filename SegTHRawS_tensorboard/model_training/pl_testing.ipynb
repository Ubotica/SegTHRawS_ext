{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "import torchvision.transforms as T\n",
    "import albumentations.pytorch as pytorch\n",
    "import albumentations as albu\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchmetrics import Accuracy, JaccardIndex, FBetaScore\n",
    "from typing import Any, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from typing import Any\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "\n",
    "# ENCODER = 'se_resnext50_32x4d'\n",
    "ENCODER = 'mobilenet_v2'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "    \n",
    "CLASSES = ['Background','Thermal Event']\n",
    "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "n_cpu = os.cpu_count()\n",
    "\n",
    "model_name = 'Unet'\n",
    "\n",
    "\n",
    "# model = smp.DeepLabV3Plus(\n",
    "#     encoder_name=ENCODER, \n",
    "#     encoder_weights=ENCODER_WEIGHTS, \n",
    "#     classes=1, \n",
    "#     activation=ACTIVATION,\n",
    "# )\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    in_channels = 3,\n",
    "    classes=1, \n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "\n",
    "\n",
    "# from segmentation_models_pytorch.utils import losses, metrics\n",
    "\n",
    "from segmentation_models_pytorch.losses import FocalLoss, DiceLoss, JaccardLoss\n",
    "\n",
    "loss = FocalLoss(mode= 'binary')\n",
    "loss.__name__ = 'focal_loss'\n",
    "\n",
    "# # loss = DiceLoss(mode= 'binary')\n",
    "# # loss.__name__ = 'dice_loss'\n",
    "\n",
    "# # loss = JaccardLoss(mode= 'binary')\n",
    "# # loss.__name__ = 'jaccard_loss'\n",
    "\n",
    "# # loss = losses.DiceLoss()\n",
    "# # loss = losses.JaccardLoss()\n",
    "\n",
    "# metrics = [\n",
    "#     metrics.IoU(),\n",
    "# ]\n",
    "\n",
    "# optimizer = torch.optim.Adam([ \n",
    "#     dict(params=model.parameters(), lr=1e-3),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThermalDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 stage: str,\n",
    "                 masks_path: str,\n",
    "                 images_path: str,\n",
    "                 augmentation: Any,\n",
    "                 preprocessing: Any,\n",
    "                 test_size: float = 0.1,\n",
    "                 train_size: float = 0.8,\n",
    "                 val_size: float = 0.1,\n",
    "                 shuffle: bool = True,\n",
    "                 random_state: int = 42):\n",
    "\n",
    "        self.__attribute_checking(masks_path, images_path, test_size,\n",
    "                                  train_size, val_size,\n",
    "                                  stage, shuffle, random_state)\n",
    "\n",
    "        self.masks_path = masks_path\n",
    "        self.images_path = images_path\n",
    "\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "        self.test_size = test_size\n",
    "        self.train_size = train_size\n",
    "        self.val_size = val_size\n",
    "        self.stage = stage\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "        self.total_len = None\n",
    "        self._images, self._masks = self.__create_dataset()\n",
    "\n",
    "    @staticmethod\n",
    "    def __type_checking(masks_path: str, images_path: str,\n",
    "                        stage: str, shuffle: bool,\n",
    "                        test_size: float, train_size: float,\n",
    "                        val_size: float,  random_state: int) -> None:\n",
    "\n",
    "        assert isinstance(masks_path, str)\n",
    "        assert isinstance(images_path, str)\n",
    "        assert isinstance(test_size, float)\n",
    "        assert isinstance(train_size, float)\n",
    "        assert isinstance(val_size, float)\n",
    "        assert isinstance(stage, str)\n",
    "        assert isinstance(shuffle, bool)\n",
    "        assert isinstance(random_state, int)\n",
    "\n",
    "    @staticmethod\n",
    "    def __split_checking(train_size: float, test_size: float, val_size: float) -> None:\n",
    "        total_size = train_size + test_size + val_size\n",
    "        assert total_size == 1\n",
    "\n",
    "    @staticmethod\n",
    "    def __path_checking(masks_path: str, images_path: str) -> None:\n",
    "        assert os.path.isdir(images_path)\n",
    "        assert os.path.isdir(masks_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def __stage_checking(stage: str) -> None:\n",
    "        assert stage in [\"train\", \"test\", \"val\"]\n",
    "\n",
    "    @classmethod\n",
    "    def __attribute_checking(cls, masks_path: str,\n",
    "                             images_path: str,\n",
    "                             test_size: float,\n",
    "                             train_size: float,\n",
    "                             val_size: float,\n",
    "                             stage: str,\n",
    "                             shuffle: bool,\n",
    "                             random_state: int) -> None:\n",
    "\n",
    "        cls.__type_checking(masks_path=masks_path,\n",
    "                            images_path=images_path,\n",
    "                            train_size=test_size,\n",
    "                            test_size=test_size,\n",
    "                            val_size=val_size,\n",
    "                            stage=stage,\n",
    "                            shuffle=shuffle,\n",
    "                            random_state=random_state)\n",
    "\n",
    "        cls.__split_checking(train_size=train_size,\n",
    "                             test_size=test_size,\n",
    "                             val_size=val_size)\n",
    "\n",
    "        cls.__path_checking(masks_path=masks_path,\n",
    "                            images_path=images_path)\n",
    "\n",
    "        cls.__stage_checking(stage=stage)\n",
    "\n",
    "    def __create_dataset(self) -> dict:\n",
    "        dict_paths = {\n",
    "            \"image\": [],\n",
    "            \"mask\": []\n",
    "        }\n",
    "\n",
    "        images_path = self.__split_data(self.stage)\n",
    "\n",
    "        for image_name in os.listdir(images_path):\n",
    "            dict_paths[\"image\"].append(os.path.join(images_path,image_name))\n",
    "            dict_paths[\"mask\"].append(os.path.join(images_path,image_name.replace('_NIR_SWIR','_mask')))\n",
    "\n",
    "        dataframe = pd.DataFrame(\n",
    "            data=dict_paths,\n",
    "            index=np.arange(0, len(dict_paths[\"image\"]))\n",
    "        )\n",
    "        self.total_len = len(dataframe)\n",
    "        data_dict = {self.stage: (dataframe[\"image\"].values,dataframe[\"mask\"].values)}\n",
    "\n",
    "\n",
    "        return data_dict[self.stage]\n",
    "\n",
    "    def __split_data(self, stage: str) -> str:\n",
    "        return os.path.join(self.images_path,stage,'images')\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.total_len\n",
    "\n",
    "    def __getitem__(self, idx) -> tuple:\n",
    "\n",
    "        image = Image.open(self._images[idx])\n",
    "        mask = Image.open(self._masks[idx])\n",
    "        \n",
    "        image = np.array(image)\n",
    "\n",
    "        \n",
    "        ### FOR FOCAL LOSS\n",
    "        mask = mask.convert('L') # This ensures that the label only have 1 band, which is necessary for binary classification\n",
    "        mask = np.array(mask)[:,:,np.newaxis]\n",
    "        \n",
    "        mask = np.divide(mask,255).astype('float32')\n",
    "        \n",
    "        # # apply augmentation\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "        albu.VerticalFlip(p=0.5),\n",
    "\n",
    "        # albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n",
    "        # albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThermalDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, target_path: str,\n",
    "                 data_path: str,\n",
    "                 augmentation: Union[T.Compose, A.Compose],\n",
    "                 preprocessing: Any,\n",
    "                 train_size: float = 0.8,\n",
    "                 val_size: float = 0.1,\n",
    "                 test_size: float = 0.1,\n",
    "                 batch_size: int = 5,\n",
    "                 num_workers: int = os.cpu_count(),\n",
    "                 seed: int = 42):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.target_path = target_path\n",
    "        self.data_path = data_path\n",
    "        self.train_size = train_size\n",
    "        self.val_size = val_size\n",
    "        self.test_size = test_size\n",
    "        self.data_train = None\n",
    "        self.data_val = None\n",
    "        self.data_test = None\n",
    "        self.data_predict = None\n",
    "        self.mean = [0.485, 0.456, 0.406]\n",
    "        self.std = [0.229, 0.224, 0.225]\n",
    "        self.train_augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "\n",
    "\n",
    "    def setup(self, stage: str = None) -> None:\n",
    "        self.data_train = ThermalDataset(\n",
    "            target_path=self.target_path,\n",
    "            data_path=self.data_path,\n",
    "            augmentation=self.train_augmentation,\n",
    "            preprocessing=self.preprocessing,\n",
    "            stage=\"train\",\n",
    "            train_size=self.train_size,\n",
    "            test_size=self.test_size,\n",
    "            val_size=self.val_size,\n",
    "            shuffle=True,\n",
    "            random_state=42\n",
    "            )\n",
    "\n",
    "        self.data_val = ThermalDataset(\n",
    "            target_path=self.target_path,\n",
    "            data_path=self.data_path,\n",
    "            augmentation=self.eval_augmentation,\n",
    "            preprocessing=self.preprocessing,\n",
    "            stage=\"val\",\n",
    "            train_size=self.train_size,\n",
    "            test_size=self.test_size,\n",
    "            val_size=self.val_size,\n",
    "            shuffle=True,\n",
    "            random_state=42\n",
    "            )\n",
    "\n",
    "        self.data_test = ThermalDataset(\n",
    "            target_path=self.target_path,\n",
    "            data_path=self.data_path,\n",
    "            augmentation=self.eval_augmentation,\n",
    "            preprocessing=self.preprocessing,\n",
    "            stage=\"test\",\n",
    "            train_size=self.train_size,\n",
    "            test_size=self.test_size,\n",
    "            val_size=self.val_size,\n",
    "            shuffle=True,\n",
    "            random_state=42\n",
    "            )\n",
    "\n",
    "        self.data_predict = ThermalDataset(\n",
    "            target_path=self.target_path,\n",
    "            data_path=self.data_path,\n",
    "            augmentation=self.eval_augmentation,\n",
    "            preprocessing=self.preprocessing,\n",
    "            stage=\"test\",\n",
    "            train_size=self.train_size,\n",
    "            test_size=self.test_size,\n",
    "            val_size=self.val_size,\n",
    "            shuffle=True,\n",
    "            random_state=42\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            dataset=self.data_train,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            dataset=self.data_val,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            dataset=self.data_test,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            dataset=self.data_predict,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThermalModel(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 model: nn.Module,\n",
    "                 example_input_array: Union[list, tuple],\n",
    "                 optim_dict: dict = None,\n",
    "                 lr: float = None,\n",
    "                 num_classes: int = 23):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.example_input_array = torch.zeros(size=example_input_array)\n",
    "        self.num_classes = num_classes\n",
    "        self.model = model\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optim_dict = optim_dict\n",
    "        self._device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "\n",
    "        self.step_outputs = {\n",
    "            \"loss\": [],\n",
    "            \"accuracy\": [],\n",
    "            \"jaccard_index\": [],\n",
    "            \"fbeta_score\": []\n",
    "        }\n",
    "\n",
    "        self.metrics = {\n",
    "            \"accuracy\": Accuracy(task=\"binary\",\n",
    "                                 threshold=0.5,\n",
    "                                 num_classes=num_classes,\n",
    "                                 validate_args=True,\n",
    "                                 ignore_index=None,\n",
    "                                 average=\"micro\").to(self._device),\n",
    "\n",
    "            \"jaccard_index\": JaccardIndex(task=\"binary\",\n",
    "                                          threshold=0.5,\n",
    "                                          num_classes=num_classes,\n",
    "                                          validate_args=True,\n",
    "                                          ignore_index=None,\n",
    "                                          average=\"macro\").to(self._device),\n",
    "\n",
    "            \"fbeta_score\": FBetaScore(task=\"binary\",\n",
    "                                      beta=1.0,\n",
    "                                      threshold=0.5,\n",
    "                                      num_classes=num_classes,\n",
    "                                      average=\"micro\",\n",
    "                                      ignore_index=None,\n",
    "                                      validate_args=True).to(self._device)\n",
    "        }\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def shared_step(self, batch, stage: str) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        x, y = x.to(self._device),y.to(self._device)\n",
    "\n",
    "        assert x.ndim == 4\n",
    "        assert x.max() <= 3 and x.min() >= -3 \n",
    "        assert y.ndim == 4\n",
    "        assert y.max() <= 1 and y.min() >= 0\n",
    "\n",
    "        logits = self.forward()\n",
    "        activated = F.softmax(input=logits, dim=1)\n",
    "        predictions = torch.argmax(activated, dim=1)\n",
    "        loss = self.criterion(logits, y)\n",
    "\n",
    "        accuracy = self.metrics[\"accuracy\"](predictions, y)\n",
    "        jaccard_index = self.metrics[\"jaccard_index\"](predictions, y)\n",
    "        fbeta_score = self.metrics[\"fbeta_score\"](predictions, y)\n",
    "\n",
    "        self.step_outputs[\"loss\"].append(loss)\n",
    "        self.step_outputs[\"accuracy\"].append(accuracy)\n",
    "        self.step_outputs[\"jaccard_index\"].append(jaccard_index)\n",
    "        self.step_outputs[\"fbeta_score\"].append(fbeta_score)\n",
    "        return loss\n",
    "\n",
    "    def shared_epoch_end(self, stage: Any):\n",
    "        loss = torch.mean(torch.tensor([\n",
    "            loss for loss in self.step_outputs[\"loss\"]\n",
    "        ]))\n",
    "\n",
    "        accuracy = torch.mean(torch.tensor([\n",
    "            accuracy for accuracy in self.step_outputs[\"accuracy\"]\n",
    "        ]))\n",
    "\n",
    "        jaccard_index = torch.mean(torch.tensor([\n",
    "            jaccard_index for jaccard_index in self.step_outputs[\"jaccard_index\"]\n",
    "        ]))\n",
    "\n",
    "        fbeta_score = torch.mean(torch.tensor(\n",
    "            [fbeta_score for fbeta_score in self.step_outputs[\"fbeta_score\"]\n",
    "             ]))\n",
    "\n",
    "        for key in self.step_outputs.keys():\n",
    "            self.step_outputs[key].clear()\n",
    "\n",
    "        metrics = {\n",
    "            f\"{stage}_loss\": loss,\n",
    "            f\"{stage}_accuracy\": accuracy,\n",
    "            f\"{stage}_jaccard_index\": jaccard_index,\n",
    "            f\"{stage}_fbeta_score\": fbeta_score\n",
    "        }\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: Any):\n",
    "        return self.shared_step(batch=batch, stage=\"train\")\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        return self.shared_epoch_end(stage=\"train\")\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: Any):\n",
    "        return self.shared_step(batch=batch, stage=\"val\")\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        return self.shared_epoch_end(stage=\"val\")\n",
    "\n",
    "    def test_step(self, batch: Any, batch_idx: Any):\n",
    "        return self.shared_step(batch=batch, stage=\"test\")\n",
    "\n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        return self.shared_epoch_end(stage=\"test\")\n",
    "\n",
    "    def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0):\n",
    "        x, y = batch\n",
    "\n",
    "        assert x.ndim == 4\n",
    "        assert x.max() <= 3 and x.min() >= -3\n",
    "        assert y.ndim == 4\n",
    "        assert y.max() <= 22 and y.min() >= 0\n",
    "\n",
    "        logits = self.forward(x)\n",
    "        activated = F.softmax(input=logits, dim=1)\n",
    "        predictions = torch.argmax(activated, dim=1)\n",
    "        return predictions\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            params=self.parameters(),\n",
    "            lr=self.hparams.lr\n",
    "        )\n",
    "\n",
    "        scheduler_dict = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer=optimizer,\n",
    "                patience=5\n",
    "            ),\n",
    "            \"interval\": \"epoch\",\n",
    "            \"monitor\": \"val_loss\"\n",
    "        }\n",
    "\n",
    "        optimization_dictionary = {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_dict}\n",
    "        return self.optim_dict if self.optim_dict else optimization_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import segmentation_models_pytorch as smp\n",
    "import warnings\n",
    "from typing import Union, Any\n",
    "from colorama import Fore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(callbacks: list,\n",
    "         model: Union[list, tuple],\n",
    "         logger: TensorBoardLogger,\n",
    "         data_path: str,\n",
    "         target_path: str,\n",
    "         optim_dict: dict,\n",
    "         example_input_array: Union[list, tuple],\n",
    "         transforms_dict: dict\n",
    "         ) -> None:\n",
    "\n",
    "    # Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        fast_dev_run=False,\n",
    "        accelerator=\"auto\",\n",
    "        strategy=\"auto\",\n",
    "        devices=\"auto\",\n",
    "        num_nodes=1,\n",
    "        logger=logger,\n",
    "        callbacks=callbacks,\n",
    "        max_epochs=80,\n",
    "        min_epochs=35\n",
    "    )\n",
    "\n",
    "    # Transforms Module\n",
    "    # if not transforms_dict:\n",
    "    #     transforms = TransformPipelineModule(\n",
    "    #         height=704,\n",
    "    #         width=1056,\n",
    "    #         cut=0,\n",
    "    #         defocus=True,\n",
    "    #         pixel_dropout=False,\n",
    "    #         pieces_dropout=False,\n",
    "    #         horizontal_lines=False,\n",
    "    #         vertical_lines=False,\n",
    "    #         spatial=True,\n",
    "    #         rain=True,\n",
    "    #         sunny=True,\n",
    "    #         snow=False,\n",
    "    #         foggy=False\n",
    "    #     )\n",
    "    # else:\n",
    "    #     transforms = TransformPipelineModule(\n",
    "    #         **transforms_dict\n",
    "    #     )\n",
    "\n",
    "    # Datamodule\n",
    "    datamodule = ThermalDataModule(\n",
    "        target_path=target_path,\n",
    "        data_path=data_path,\n",
    "        augmentation=get_training_augmentation(),\n",
    "        preprocessing=get_preprocessing(preprocessing_fn),\n",
    "        train_size=0.80,\n",
    "        val_size=0.1,\n",
    "        test_size=0.1,\n",
    "        batch_size=2,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    # LightningModule\n",
    "    lightning_model = ThermalModel(\n",
    "        model=model,\n",
    "        optim_dict=optim_dict,\n",
    "        lr=3e-4,\n",
    "        example_input_array=example_input_array\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.fit(model=lightning_model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        dirpath=f\"models/{model_name}\",\n",
    "        filename=\"{epoch}_{val_loss:.2f}_{val_accuracy:.2f}\",\n",
    "        save_top_k=10,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\"\n",
    "    ),\n",
    "\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=2e-4,\n",
    "        patience=8,\n",
    "        verbose=False,\n",
    "        mode=\"min\"\n",
    "    ),\n",
    "\n",
    "    LearningRateMonitor(\n",
    "        logging_interval=\"step\"\n",
    "    )\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
