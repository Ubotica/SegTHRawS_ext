{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "import torchvision.transforms as T\n",
    "import albumentations.pytorch as pytorch\n",
    "import albumentations as albu\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchmetrics import Accuracy, JaccardIndex, FBetaScore\n",
    "from typing import Any, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import segmentation_models_pytorch as smp\n",
    "import warnings\n",
    "from typing import Union, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from typing import Any\n",
    "\n",
    "from PIL import Image\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from segmentation_models_pytorch.utils import metrics\n",
    "\n",
    "from segmentation_models_pytorch.losses import FocalLoss, DiceLoss, JaccardLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThermalDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 stage: str,\n",
    "                 images_path: str,\n",
    "                 augmentation: Any,\n",
    "                 preprocessing: Any,\n",
    "                 shuffle: bool = True,\n",
    "                 random_state: int = 42):\n",
    "\n",
    "        self.__attribute_checking(images_path,\n",
    "                                  stage, shuffle, random_state)\n",
    "\n",
    "        self.images_path = images_path\n",
    "\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "\n",
    "        self.stage = stage\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "        self.total_len = None\n",
    "        self._images, self._masks = self.__create_dataset()\n",
    "\n",
    "    @staticmethod\n",
    "    def __type_checking(images_path: str,\n",
    "                        stage: str, shuffle: bool,\n",
    "                        random_state: int) -> None:\n",
    "        \n",
    "        assert isinstance(images_path, str)\n",
    "        assert isinstance(stage, str)\n",
    "        assert isinstance(shuffle, bool)\n",
    "        assert isinstance(random_state, int)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def __path_checking(images_path: str) -> None:\n",
    "        assert os.path.isdir(images_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def __stage_checking(stage: str) -> None:\n",
    "        assert stage in [\"train\", \"test\", \"val\"]\n",
    "\n",
    "    @classmethod\n",
    "    def __attribute_checking(cls, images_path: str,\n",
    "                             stage: str,\n",
    "                             shuffle: bool,\n",
    "                             random_state: int) -> None:\n",
    "\n",
    "        cls.__type_checking(images_path=images_path,\n",
    "                            stage=stage,\n",
    "                            shuffle=shuffle,\n",
    "                            random_state=random_state)\n",
    "\n",
    "        cls.__path_checking(images_path=images_path)\n",
    "\n",
    "        cls.__stage_checking(stage=stage)\n",
    "\n",
    "    def __create_dataset(self) -> dict:\n",
    "        dict_paths = {\n",
    "            \"image\": [],\n",
    "            \"mask\": []\n",
    "        }\n",
    "\n",
    "        images_path = self.__split_data(self.stage)\n",
    "\n",
    "        for image_name in os.listdir(images_path):\n",
    "            dict_paths[\"image\"].append(os.path.join(images_path,image_name))\n",
    "            dict_paths[\"mask\"].append(os.path.join(os.path.dirname(images_path),'masks',image_name.replace('_NIR_SWIR','_mask')))\n",
    "\n",
    "        dataframe = pd.DataFrame(\n",
    "            data=dict_paths,\n",
    "            index=np.arange(0, len(dict_paths[\"image\"]))\n",
    "        )\n",
    "        self.total_len = len(dataframe)\n",
    "        data_dict = {self.stage: (dataframe[\"image\"].values,dataframe[\"mask\"].values)}\n",
    "\n",
    "        return data_dict[self.stage]\n",
    "\n",
    "    def __split_data(self, stage: str) -> str:\n",
    "        return os.path.join(self.images_path,stage,'images')\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.total_len\n",
    "\n",
    "    def __getitem__(self, idx) -> tuple:\n",
    "\n",
    "        image = Image.open(self._images[idx])\n",
    "        mask = Image.open(self._masks[idx])\n",
    "        \n",
    "        image = np.array(image)\n",
    "\n",
    "        ### FOR FOCAL LOSS\n",
    "        mask = mask.convert('L') # This ensures that the label only have 1 band, which is necessary for binary classification\n",
    "        mask = np.array(mask)[:,:,np.newaxis]\n",
    "        \n",
    "        mask = np.divide(mask,255).astype('float32') #Masks need to be 0-1 values\n",
    "        \n",
    "        # # apply augmentation\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "        albu.VerticalFlip(p=0.5),\n",
    "\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThermalDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,images_path: str,\n",
    "                 augmentation: Union[T.Compose, A.Compose],\n",
    "                 preprocessing: Any,\n",
    "                 batch_size: int = 5,\n",
    "                 num_workers: int = os.cpu_count(),\n",
    "                 seed: int = 42):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.images_path = images_path\n",
    "        self.data_train = None\n",
    "        self.data_val = None\n",
    "        self.data_test = None\n",
    "        self.data_predict = None\n",
    "        self.seed = seed\n",
    "\n",
    "        self.train_augmentation = augmentation\n",
    "        self.eval_augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "\n",
    "\n",
    "    def setup(self, stage: str = None) -> None:\n",
    "        self.data_train = ThermalDataset(\n",
    "            images_path=self.images_path,\n",
    "            augmentation=self.train_augmentation,\n",
    "            preprocessing=self.preprocessing,\n",
    "            stage=\"train\",\n",
    "            shuffle=True,\n",
    "            random_state=self.seed\n",
    "            )\n",
    "\n",
    "        self.data_val = ThermalDataset(\n",
    "            images_path=self.images_path,\n",
    "            augmentation=self.eval_augmentation,\n",
    "            preprocessing=self.preprocessing,\n",
    "            stage=\"val\",\n",
    "            shuffle=True,\n",
    "            random_state=self.seed\n",
    "            )\n",
    "\n",
    "        self.data_test = ThermalDataset(\n",
    "            images_path=self.images_path,\n",
    "            augmentation=self.eval_augmentation,\n",
    "            preprocessing=self.preprocessing,\n",
    "            stage=\"test\",\n",
    "            shuffle=True,\n",
    "            random_state=self.seed\n",
    "            )\n",
    "\n",
    "        self.data_predict = ThermalDataset(\n",
    "            images_path=self.images_path,\n",
    "            augmentation=self.eval_augmentation,\n",
    "            preprocessing=self.preprocessing,\n",
    "            stage=\"test\",\n",
    "            shuffle=True,\n",
    "            random_state=self.seed\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            dataset=self.data_train,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            dataset=self.data_val,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            dataset=self.data_test,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            dataset=self.data_predict,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThermalModel(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 model: nn.Module,\n",
    "                 loss_fn: Any,\n",
    "                 optim_dict: dict = None,\n",
    "                 lr: float = None,\n",
    "                 num_classes: int = 1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['model','loss_fn'])\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.model = model\n",
    "        # self.criterion = nn.CrossEntropyLoss()\n",
    "        self.criterion = loss_fn\n",
    "        self.optim_dict = optim_dict\n",
    "        self._device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "\n",
    "        self.step_outputs = {\n",
    "            \"loss\": [],\n",
    "            \"accuracy\": [],\n",
    "            \"jaccard_index\": [],\n",
    "            \"fbeta_score\": [],\n",
    "            \"IoU\": []\n",
    "        }\n",
    "\n",
    "        self.metrics = {\n",
    "            \"accuracy\": Accuracy(task=\"binary\",\n",
    "                                 threshold=0.5,\n",
    "                                 num_classes=num_classes,\n",
    "                                 validate_args=True,\n",
    "                                 ignore_index=None,\n",
    "                                 average=\"micro\").to(self._device),\n",
    "\n",
    "            \"jaccard_index\": JaccardIndex(task=\"binary\",\n",
    "                                          threshold=0.5,\n",
    "                                          num_classes=num_classes,\n",
    "                                          validate_args=True,\n",
    "                                          ignore_index=None,\n",
    "                                          average=\"macro\").to(self._device),\n",
    "\n",
    "            \"fbeta_score\": FBetaScore(task=\"binary\",\n",
    "                                      beta=1.0,\n",
    "                                      threshold=0.5,\n",
    "                                      num_classes=num_classes,\n",
    "                                      average=\"micro\",\n",
    "                                      ignore_index=None,\n",
    "                                      validate_args=True).to(self._device),\n",
    "\n",
    "            \"IoU\": metrics.IoU()\n",
    "        }\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def shared_step(self, batch, stage: str) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        x, y = x.to(self._device),y.to(self._device)\n",
    "\n",
    "        assert x.ndim == 4\n",
    "        assert x.max() <= 3 and x.min() >= -3 \n",
    "        assert y.ndim == 4\n",
    "        assert y.max() <= 1 and y.min() >= 0\n",
    "\n",
    "        logits = self.forward(x.to(torch.float32))\n",
    "        \n",
    "\n",
    "        # activated = F.softmax(input=logits, dim=1)\n",
    "        # predictions = torch.argmax(activated, dim=1)\n",
    "\n",
    "        predictions = torch.round(torch.sigmoid(logits))\n",
    "        # predictions = logits\n",
    "        \n",
    "        loss = self.criterion(logits, y)\n",
    "\n",
    "        accuracy = self.metrics[\"accuracy\"](predictions, y)\n",
    "        jaccard_index = self.metrics[\"jaccard_index\"](predictions, y)\n",
    "        fbeta_score = self.metrics[\"fbeta_score\"](predictions, y)\n",
    "        IoU_score = self.metrics[\"IoU\"](predictions, y)\n",
    "\n",
    "        self.step_outputs[\"loss\"].append(loss)\n",
    "        self.step_outputs[\"accuracy\"].append(accuracy)\n",
    "        self.step_outputs[\"jaccard_index\"].append(jaccard_index)\n",
    "        self.step_outputs[\"fbeta_score\"].append(fbeta_score)\n",
    "        self.step_outputs[\"IoU\"].append(IoU_score)\n",
    "\n",
    "        self.log(f'{stage}_loss'   , loss          , prog_bar=True , on_step=False , on_epoch=True)\n",
    "        # self.log(f'{stage}_acc'    , accuracy      , prog_bar=True , on_step=False , on_epoch=True)\n",
    "        # self.log(f'{stage}_jaccard', jaccard_index , prog_bar=True , on_step=False , on_epoch=True)\n",
    "        self.log(f'{stage}_fbeta'  , fbeta_score   , prog_bar=True , on_step=False , on_epoch=True)\n",
    "        self.log(f'{stage}_IoU'    , IoU_score     , prog_bar=True , on_step=False , on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    # def shared_epoch_end(self, stage: Any):\n",
    "    #     loss = torch.mean(torch.tensor([\n",
    "    #         loss for loss in self.step_outputs[\"loss\"]\n",
    "    #     ]))\n",
    "\n",
    "    #     accuracy = torch.mean(torch.tensor([\n",
    "    #         accuracy for accuracy in self.step_outputs[\"accuracy\"]\n",
    "    #     ]))\n",
    "\n",
    "    #     jaccard_index = torch.mean(torch.tensor([\n",
    "    #         jaccard_index for jaccard_index in self.step_outputs[\"jaccard_index\"]\n",
    "    #     ]))\n",
    "\n",
    "    #     fbeta_score = torch.mean(torch.tensor(\n",
    "    #         [fbeta_score for fbeta_score in self.step_outputs[\"fbeta_score\"]\n",
    "    #          ]))\n",
    "\n",
    "    #     for key in self.step_outputs.keys():\n",
    "    #         self.step_outputs[key].clear()\n",
    "\n",
    "    #     metrics = {\n",
    "    #         f\"{stage}_loss\": loss,\n",
    "    #         f\"{stage}_accuracy\": accuracy,\n",
    "    #         f\"{stage}_jaccard_index\": jaccard_index,\n",
    "    #         f\"{stage}_fbeta_score\": fbeta_score\n",
    "    #     }\n",
    "    #     self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: Any):\n",
    "        return self.shared_step(batch=batch, stage=\"train\")\n",
    "\n",
    "    # def on_train_epoch_end(self) -> None:\n",
    "    #     return self.shared_epoch_end(stage=\"train\")\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: Any):\n",
    "        return self.shared_step(batch=batch, stage=\"val\")\n",
    "\n",
    "    # def on_validation_epoch_end(self) -> None:\n",
    "    #     return self.shared_epoch_end(stage=\"val\")\n",
    "\n",
    "    def test_step(self, batch: Any, batch_idx: Any):\n",
    "        return self.shared_step(batch=batch, stage=\"test\")\n",
    "\n",
    "    # def on_test_epoch_end(self) -> None:\n",
    "    #     return self.shared_epoch_end(stage=\"test\")\n",
    "\n",
    "    def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0):\n",
    "        x, y = batch\n",
    "\n",
    "        assert x.ndim == 4\n",
    "        assert x.max() <= 3 and x.min() >= -3\n",
    "        assert y.ndim == 4\n",
    "        assert y.max() <= 1 and y.min() >= 0\n",
    "\n",
    "        logits = self.forward(x.to(torch.float32))\n",
    "        # predictions = logits\n",
    "        predictions = torch.round(torch.sigmoid(logits))\n",
    "\n",
    "        # activated = F.softmax(input=logits, dim=1)\n",
    "        # predictions = torch.argmax(activated, dim=1)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            params=self.parameters(),\n",
    "            lr=self.hparams.lr\n",
    "        )\n",
    "\n",
    "        scheduler_dict = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer=optimizer,\n",
    "                patience=5\n",
    "            ),\n",
    "            \"interval\": \"epoch\",\n",
    "            \"monitor\": \"val_loss\"\n",
    "        }\n",
    "\n",
    "        optimization_dictionary = {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_dict}\n",
    "        return self.optim_dict if self.optim_dict else optimization_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(callbacks: list,\n",
    "         model: Union[list, tuple],\n",
    "         loss_fn: Any,\n",
    "         augmentation: Any,\n",
    "         preprocessing: Any,\n",
    "         logger: TensorBoardLogger,\n",
    "         images_path: str,\n",
    "         optim_dict: dict,\n",
    "         ) -> None:\n",
    "\n",
    "    # Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        fast_dev_run=False,\n",
    "        accelerator=\"auto\",\n",
    "        strategy=\"auto\",\n",
    "        devices=\"auto\",\n",
    "        num_nodes=1,\n",
    "        logger=logger,\n",
    "        callbacks=callbacks,\n",
    "        max_epochs=200,\n",
    "        min_epochs=150\n",
    "    )\n",
    "\n",
    "    # Datamodule\n",
    "    datamodule = ThermalDataModule(\n",
    "        images_path=images_path,\n",
    "        augmentation=augmentation,\n",
    "        preprocessing=preprocessing,\n",
    "        batch_size=5,\n",
    "        num_workers=os.cpu_count()\n",
    "    )\n",
    "\n",
    "    # LightningModule\n",
    "    lightning_model = ThermalModel(\n",
    "        model=model,\n",
    "        loss_fn=loss_fn,\n",
    "        optim_dict=optim_dict,\n",
    "        lr=3e-4\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.fit(model=lightning_model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Constants\n",
    "SEED: int = 42\n",
    "ACTION: str = \"ignore\"\n",
    "DATA_PATH: str = os.path.join(os.getcwd(),'train_dataset')\n",
    "CHECKPOINT: Any = None\n",
    "    \n",
    "# Model Constants\n",
    "CLASSES = 1\n",
    "IN_CHANNELS = 3\n",
    "\n",
    "optim_dict = None\n",
    "\n",
    "# ENCODER = 'se_resnext50_32x4d'\n",
    "ENCODER = 'mobilenet_v2'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "    \n",
    "CLASSES = ['Background','Thermal Event']\n",
    "#### Removed the activation function for testing\n",
    "ACTIVATION = None\n",
    "# ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "n_cpu = os.cpu_count()\n",
    "\n",
    "# model_name = 'Unet'\n",
    "model_name = 'DeepLabV3Plus'\n",
    "\n",
    "\n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=1, \n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "\n",
    "# model = smp.Unet(\n",
    "#     encoder_name=ENCODER, \n",
    "#     encoder_weights=ENCODER_WEIGHTS, \n",
    "#     in_channels = 3,\n",
    "#     classes=1, \n",
    "#     activation=ACTIVATION,\n",
    "# )\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "\n",
    "\n",
    "\n",
    "loss = FocalLoss(mode= 'binary')\n",
    "loss.__name__ = 'focal_loss'\n",
    "\n",
    "# # loss = DiceLoss(mode= 'binary')\n",
    "# # loss.__name__ = 'dice_loss'\n",
    "\n",
    "# # loss = JaccardLoss(mode= 'binary')\n",
    "# # loss.__name__ = 'jaccard_loss'\n",
    "\n",
    "# # loss = losses.DiceLoss()\n",
    "# # loss = losses.JaccardLoss()\n",
    "\n",
    "# metrics = [\n",
    "#     metrics.IoU(),\n",
    "# ]\n",
    "\n",
    "# optimizer = torch.optim.Adam([ \n",
    "#     dict(params=model.parameters(), lr=1e-3),\n",
    "# ])\n",
    "\n",
    "augmentation=get_training_augmentation()\n",
    "preprocessing=get_preprocessing(preprocessing_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        dirpath=f\"models/{model_name}\",\n",
    "        filename=\"Unet_MobileNetV2_{epoch}_{val_loss:.2f}_{val_accuracy:.2f}\",\n",
    "        save_top_k=10,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\"\n",
    "    ),\n",
    "\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=2e-4,\n",
    "        patience=8,\n",
    "        verbose=False,\n",
    "        mode=\"min\"\n",
    "    ),\n",
    "\n",
    "    LearningRateMonitor(\n",
    "        logging_interval=\"step\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "logger = TensorBoardLogger(save_dir=\"./logs\", name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(\n",
    "    callbacks=callbacks,\n",
    "    model=model,\n",
    "    loss_fn=loss,\n",
    "    augmentation=augmentation,\n",
    "    preprocessing=preprocessing,\n",
    "    logger=logger,\n",
    "    images_path=DATA_PATH,\n",
    "    optim_dict=optim_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "# %tensorboard --logdir models/current_best_model/version_33\n",
    "\n",
    "# %tensorboard --logdir logs/DeepLabV3Plus/version_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# checkpoint_path = os.path.join(os.getcwd(),'models','current_best_model','Unet_MobileNetV2_epoch=146.ckpt')\n",
    "checkpoint_path = os.path.join(os.getcwd(),'models',model_name,'DeepLabv3PLus_MobileNetV2_epoch=144_val_loss=0.00_val_accuracy=0.00.ckpt')\n",
    "trained_model = ThermalModel.load_from_checkpoint(checkpoint_path=checkpoint_path,model=model,loss_fn=loss)\n",
    "trained_model.eval();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b775b9370b4de1809c02f2022a90a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_IoU             0.652067244052887\n",
      "       test_fbeta           0.5769277215003967\n",
      "        test_loss         0.00044822978088632226\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.00044822978088632226,\n",
       "  'test_fbeta': 0.5769277215003967,\n",
       "  'test_IoU': 0.652067244052887}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Perform the testing, c  NEED TO create a function\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    fast_dev_run=False,\n",
    "    accelerator=\"auto\",\n",
    "    strategy=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    num_nodes=1,\n",
    "    logger=logger,\n",
    "    callbacks=callbacks,\n",
    "    max_epochs=200,\n",
    "    min_epochs=150\n",
    ")\n",
    "\n",
    "# Datamodule\n",
    "datamodule = ThermalDataModule(\n",
    "    images_path=DATA_PATH,\n",
    "    augmentation=augmentation,\n",
    "    preprocessing=preprocessing,\n",
    "    batch_size=5,\n",
    "    num_workers=os.cpu_count()\n",
    ")\n",
    "\n",
    "loss_2 = FocalLoss(mode= 'binary')\n",
    "loss_2.__name__ = 'focal_loss'\n",
    "\n",
    "# LightningModule\n",
    "lightning_model = ThermalModel(\n",
    "    model=model,\n",
    "    loss_fn=loss_2,\n",
    "    optim_dict=optim_dict,\n",
    "    lr=3e-4\n",
    ")\n",
    "\n",
    "trainer.test(model=trained_model,datamodule=datamodule)\n",
    "# trainer.predict(model=trained_model,datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(trained_model,'trained_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "# %matplotlib widget\n",
    "import os\n",
    "\n",
    "from matplotlib.ticker import AutoMinorLocator, MultipleLocator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_path = os.path.join(os.getcwd(),'models','current_best_model')\n",
    "\n",
    "metric_id_list = []\n",
    "\n",
    "for file_name in os.listdir(model_path):\n",
    "    if file_name[-4:]=='.csv':\n",
    "        fig, ax = plt.subplots(figsize=(7,5))\n",
    "        \n",
    "        csv_file = pd.read_csv(os.path.join(model_path,file_name))\n",
    "        metric_name = file_name.replace('_evolution.csv','')\n",
    "        \n",
    "\n",
    "        ax.yaxis.set_minor_locator(AutoMinorLocator(4))\n",
    "\n",
    "        if metric_name[:5]=='train':\n",
    "            ax.set_xlabel('# epochs')\n",
    "\n",
    "            metric_id = metric_name.replace('train_','')    \n",
    "            if metric_id not in metric_id_list:\n",
    "                for inner_file_name in os.listdir(model_path):\n",
    "                    if inner_file_name[-4:]=='.csv' and inner_file_name.replace('_evolution.csv','')==f'val_{metric_id}':\n",
    "                        csv_file_inner = pd.read_csv(os.path.join(model_path,inner_file_name))\n",
    "                        \n",
    "                        csv_values = csv_file['Value']\n",
    "                        csv_inner_values = csv_file_inner['Value']\n",
    "                        \n",
    "                        ax.plot(csv_values,color='r',label='Training')\n",
    "                        ax.plot(csv_inner_values,color='b',label='Validation')\n",
    "                        ax.set_title(f'{metric_id} evolution')\n",
    "                        ax.set_ylabel(f'{metric_id}')\n",
    "                        plt.annotate('%0.2f' % csv_values[len(csv_values)-1], xy=(len(csv_values)-1, csv_values[len(csv_values)-1]))\n",
    "                        plt.annotate('%0.2f' % csv_inner_values[len(csv_inner_values)-1], xy=(len(csv_inner_values)-1, csv_inner_values[len(csv_inner_values)-1]))\n",
    "                        metric_id_list.append(metric_id)\n",
    "                        break\n",
    "            else:\n",
    "                metric_id = None\n",
    "                plt.close(fig)\n",
    "        elif metric_name[:3]=='val':\n",
    "            ax.set_xlabel('# epochs')\n",
    "            \n",
    "            metric_id = metric_name.replace('val_','')\n",
    "            if metric_id not in metric_id_list:\n",
    "                for inner_file_name in os.listdir(model_path):\n",
    "                    if inner_file_name[-4:]=='.csv' and inner_file_name.replace('_evolution.csv','')==f'train_{metric_id}':\n",
    "                        csv_file_inner = pd.read_csv(os.path.join(model_path,inner_file_name))\n",
    "                        \n",
    "                        csv_values = csv_file['Value']\n",
    "                        csv_inner_values = csv_file_inner['Value']\n",
    "\n",
    "                        ax.plot(csv_values,color='b',label='Validation')\n",
    "                        ax.plot(csv_inner_values,color='r',label='Training')\n",
    "                        ax.set_title(f'{metric_id} evolution')\n",
    "                        ax.set_ylabel(f'{metric_id}')\n",
    "\n",
    "                        plt.annotate('%0.2f' % csv_values[len(csv_values)-1], xy=(len(csv_values)-1, csv_values[len(csv_values)-1]))\n",
    "                        plt.annotate('%0.2f' % csv_inner_values[len(csv_inner_values)-1], xy=(len(csv_inner_values)-1, csv_inner_values[len(csv_inner_values)-1]))\n",
    "                        \n",
    "                        metric_id_list.append(metric_id)\n",
    "            else:\n",
    "                metric_id = None\n",
    "                plt.close(fig)\n",
    "\n",
    "        else:\n",
    "            csv_values = csv_file['Value']\n",
    "            ax.set_title(f'{metric_name} evolution')\n",
    "            ax.set_ylabel(f'{metric_name}')\n",
    "            ax.set_xlabel('Time')\n",
    "            ax.plot(csv_values,color='g',label='learning rate')\n",
    "            plt.annotate('%.0E' % csv_values[0], xy=(0, csv_values[0]))\n",
    "            plt.annotate('%.0E' % csv_values[len(csv_values)-1], xy=(len(csv_values)-1, csv_values[len(csv_values)-1]))\n",
    "            metric_id = 'lr'\n",
    "        # ax.plot(csv_file['Value'])\n",
    "        if metric_id:\n",
    "            ax.legend()\n",
    "            plt.savefig(os.path.join(model_path,metric_id+'_evolution.png'))\n",
    "        # break\n",
    "        # break\n",
    "        # plt.show()\n",
    "        # break\n",
    "        # plt.savefig(os.path.join(model_path,file_name.replace('.csv','.png')))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cristopher/Documents/SegTHRawS training/model_training/segthraws_env/lib/python3.11/site-packages/segmentation_models_pytorch/base/model.py:16: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if h % output_stride != 0 or w % output_stride != 0:\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, 3, 256, 256).cpu()\n",
    "model_onnx = trained_model.cpu()\n",
    "model_onnx.eval()\n",
    "torch_out = model_onnx(x)\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(model_onnx,               # model being run\n",
    "                  x,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"Unet_MobileNetV2_epoch146.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=11,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                                'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(\"Unet_MobileNetV2_epoch146.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Exported model has been tested with ONNXRuntime, and the result looks good!\n"
     ]
    }
   ],
   "source": [
    "# !pip install onnxruntime\n",
    "import onnxruntime\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\"Unet_MobileNetV2_epoch146.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "# compute ONNX Runtime output prediction\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# compare ONNX Runtime and PyTorch results\n",
    "print(np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05))\n",
    "\n",
    "print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
